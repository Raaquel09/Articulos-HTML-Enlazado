[
    {
        "title": "A Community Roadmap for Scientific Workflows Research and Development",
        "implementation_urls": [],
        "doi": "10.1109/works54523.2021.00016",
        "arxiv": "2110.02168",
        "abstract": "Abstract—The landscape of workflow systems for scientificapplications is notoriously convoluted with hundreds of seeminglyequivalent workflow systems, many isolated research claims, anda steep learning curve. To address some of these challenges andlay the groundwork for transforming workflows research anddevelopment, the WorkflowsRI and ExaWorks projects partneredto bring the international workflows community together. Thispaper reports on discussions and findings from two virtual“Workflows Community Summits” (January and April, 2021).The overarching goals of these workshops were to develop a viewof the state of the art, identify crucial research challenges in theworkflows community, articulate a vision for potential communityefforts, and discuss technical approaches for realizing this vision.To this end, participants identified six broad themes: FAIR com-putational workflows; AI workflows; exascale challenges; APIs,interoperability, reuse, and standards; training and education;and building a workflows community. We summarize discussionsand recommendations for each of these themes.Index Terms—Scientific workflows, community roadmap, datamanagement, AI workflows, exascale computing, interoperabilityI. INTRODUCTIONScientific workflow systems are used almost universallyacross scientific domains for solving complex and large-scale computing and data analysis problems, and have un-derpinned some of the most significant discoveries of thepast decades [1]. Many of these workflows have significantcomputational, storage, and communication demands, and thusmust execute on a wide range of large-scale platforms, fromlocal clusters over science or public clouds to upcomingexascale HPC platforms [2]. Managing these executions isoften a significant undertaking, requiring a sophisticated andversatile software infrastructure.Historically, many of these infrastructures for workflowexecution consisted of complex, integrated systems, developedin-house by workflow practitioners with strong dependencieson a range of legacy technologies—even including sets ofad-hoc scripts. Due to the increasing need to support work-flows, dedicated workflow systems were developed to provideabstractions for creating, executing, and adapting workflowsconveniently and efficiently while ensuring portability. Whilethese efforts are all worthwhile individually, there are nowhundreds of independent workflow systems [3]. These arecreated and used by thousands of researchers and developers,leading to a rapidly growing corpus of workflows researchpublications. The resulting workflow system technology land-scape is fragmented, which may present significant barriersfor future workflow users due to the tens of seemingly com-parable, yet usually mutually incompatible, systems that exist.In the current workflow research, there are conflicting theo-retical bases and abstractions for what constitutes a workflow",
        "publication_date": "2021-11-01",
        "authors": "Rafael Ferreira da Silva, Henri Casanova, Kyle Chard, İlkay Altıntaş, Rosa M. Badía, Bartosz Baliś, Tainã Coleman, Frederik Coppens, Frank Di Natale, Bjoern Enders, Thomas Fahringer, Rosa Filgueira, Grigori Fursin, Daniel Garijo, Carole Goble, Dorran Howell, Shantenu Jha, Daniel S. Katz, Daniel Laney, Ulf Leser, M. Malawski, Kshitij Mehta, Loïc Pottier, Jonathan Ozik, J. L. Peterson, Lavanya Ramakrishnan, Stian Soiland‐Reyes, Douglas Thain, Matthew Wolf",
        "file_name": "20250512000235.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/2110/PDFs/20250512000235.pdf",
        "pdf_link": "https://arxiv.org/pdf/2110.02168",
        "file_html": "html/20250512000235.html",
        "url": "https://arxiv.org/pdf/2110.02168",
        "archiveprefix": "arXiv",
        "eprint": "2110.02168",
        "year": "2021",
        "author": "Rafael Ferreira da Silva and Henri Casanova and Kyle Chard and Ilkay Altintas and Rosa M Badia and Bartosz Balis and Tainã Coleman and Frederik Coppens and Frank Di Natale and Bjoern Enders and Thomas Fahringer and Rosa Filgueira and Grigori Fursin and Daniel Garijo and Carole Goble and Dorran Howell and Shantenu Jha and Daniel S. Katz and Daniel Laney and Ulf Leser and Maciej Malawski and Kshitij Mehta and Loïc Pottier and Jonathan Ozik and J. Luc Peterson and Lavanya Ramakrishnan and Stian Soiland-Reyes and Douglas Thain and Matthew Wolf",
        "ENTRYTYPE": "misc",
        "ID": "dasilva2021community"
    },
    {
        "title": "LLMs for Ontology Engineering: A landscape of Tasks and Benchmarking challenges",
        "implementation_urls": [],
        "abstract": "AbstractLarge Language Models (LLMs) have emerged as a powerful technology for text generation tasks, showingpromise in supporting the Ontology Engineering (OE) process. In this paper, we review current research onapplying LLMs to OE tasks, aiming to identify commonalities and gaps in the state of the art. We categorize theseefforts using the Linked Open Terms (LOT) methodology, characterizing them by their input and expected output.From this analysis, we highlight key challenges when creating benchmarks to evaluate LLM performance in OEtasks.KeywordsLarge Language Models, Ontology Engineering, Benchmark, Challenges1. IntroductionOntologies are a key component of Knowledge Engineering for integrating, validating and reasoningwith data in Knowledge Graphs [1]. However, developing ontologies is a challenging and time consumingtask. According to existing methodologies for ontology development [2], Knowledge Engineers shouldfollow an iterative process to 1) distill the knowledge of the target domain by interviewing expertsand understand their data-driven requirements, 2) implement a shared conceptualization by assessingexisting standard ontologies described in the domain and validating it against the requirements, 3) makethe ontology available on the web in both human and machine-readable manner, and 4) assess andmaintain the ontology by addressing any new requirements that may arise from its use. While differenttools have been developed by the scientific community to assist in the Ontology Engineerning process(e.g., for formalizing tests to assess requirements [3], creating human-readable documentation [4],ontology assessment [5, 6], etc.) a significant manual effort is still required from knowledge engineersfor conceptualizing, reusing and validating existing ontologies.In recent years, Large Language Models (LLMs) [7, 8, 9] have emerged as a disruptive AI technologyfor text generation tasks. On the one hand, LLMs have revolutionized the state of the art by providingimpressive results in challenging AI tasks such as code generation [10], question answering [11] ortext summarization [7], and becoming easy to adapt as chat bots such as ChatGPT.1 On the other hand,LLMs have limited reasoning skills [12], hallucination problems (i.e., producing inaccurate answers andinformation) [13], lack transparency when providing results [14] and present bias problems [15].A number of works have started using LLMs for aiding developers in ontology engineering tasks(e.g., proposing competency questions [16], learning ontologies from text [17], aligning concepts toexisting taxonomies [18], etc.). However, the tasks addressed in existing works are usually defined inan heterogeneous manner, with different scope, inputs and expected outputs. In this paper we providean overview of existing Ontology Engineering tasks addressed in the state of the art and map themISWC 2024 Special Session on Harmonising Generative AI and Semantic Web Technologies, November 13, 2024, Baltimore, MarylandEnvelope-Open daniel.garijo@upm.es (D. Garijo); m.poveda@upm.es (M. Poveda-Villalón); elvira.amador@upm.es(E. Amador-Domínguez); ziyuan.wang@upm.es (Z. Wang); r.garcia@upm.es (R. García-Castro); oscar.corcho@upm.es(O. Corcho)Orcid 0000-0003-0454-7145 (D. Garijo); 0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0001-6838-1266 (E. Amador-Domínguez);0009-0000-6228-4713 (Z. Wang); 0000-0002-0421-452X (R. García-Castro); 0000-0002-9260-0753 (O. Corcho)© 2025 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).1https://chat.openai.com/CEURWorkshopProceedingsceur-ws.orgISSN 1613-0073mailto:daniel.garijo@upm.esmailto:m.poveda@upm.esmailto:elvira.amador@upm.esmailto:ziyuan.wang@upm.es",
        "file_name": "20250511232538.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/364/PDFs/20250511232538.pdf",
        "pdf_link": "https://ceur-ws.org/Vol-3953/364.pdf",
        "file_html": "html/20250511232538.html",
        "year": "2024",
        "url": "https://ceur-ws.org/Vol-3953/364.pdf",
        "booktitle": "Proceedings of the Special Session on Harmonising Generative AI and Semantic Web Technologies (HGAIS 2024) co-located with the 23rd International Semantic Web Conference (ISWC 2024)",
        "author": "Garijo, Daniel and Poveda-Villal{\\'o}n, Mar{\\'i}a and Amador-Dominguez, Elvira and Wang, Ziyuan and Garc{\\'i}a-Castro, Raul and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo_llm"
    },
    {
        "title": "Towards Continuous Scientific Data Analysis and Hypothesis Evolution.",
        "implementation_urls": [
            {
                "identifier": "https://github.com/JohnLangford/vowpal_wabbit",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/aaai2017/PDFs/20250512001506.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available from https://github.com/JohnLangford/vowpal_wabbit, 2011."
                    }
                ]
            },
            {
                "identifier": "http://doi.org/10.5281/zenodo.190374",
                "type": "zenodo",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            },
            {
                "identifier": "http://doi.org/10.5281/zenodo.180716",
                "type": "zenodo",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1609/aaai.v31i1.11157",
        "abstract": "Abstract Scientific data is continuously generated throughout the world.  However, analyses of these data are typically per-formed exactly once and on a small fragment of recently generated data. Ideally, data analysis would be a continuous process that uses all the data available at the time, and would be automatically re-run and updated when new data appears. We present a framework for automated discovery from data repositories that tests user-provided hypotheses using expert-grade data analysis strategies, and reassesses hypotheses when more data becomes available. Novel con-tributions of this approach include a framework to trigger new analyses appropriate for the available data through lines of inquiry that support progressive hypothesis evolution, and a representation of hypothesis revisions with prove-nance records that can be used to inspect the results. We implemented our approach in the DISK framework, and evaluated it using two scenarios from cancer multi-omics: 1) data for new patients becomes available over time, 2) new types of data for the same patients are released. We show that in all scenarios DISK updates the confidence on the original hypotheses as it automatically analyzes new data.   Introduction   In many areas of science, sensors and instruments are con-tinuously collecting data.  Yet most research projects ana-lyze data at a particular point in time, and once articles are published they are rarely revisited to account for new data.  In some cases, this makes sense since more data may only be tangentially related, and thus may not be relevant to include in a joint analysis. However, in many cases the availability of additional data may significantly affect prior results, by confirming with additional evidence or invali-dating them. In addition, the new data may enable new types of analyses, leading to important revisions of prior findings or to entirely new findings.                                                    Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.   Our goal is to automatically and continuously analyze scientific data as it becomes available, so scientists can be alerted if their prior studies are affected or if new results are gleamed.  In prior work, we developed an approach to represent hypotheses and link them to relevant data to be analyzed, test them through lines of inquiry that capture expert-grade data analysis strategies, and aggregate the analysis results through meta-reasoning to combine the evidence gathered and generate revised hypotheses and confidence values [Gil et al 2016].  We implemented our approach in the DISK (Automated DIscovery of Scientific ",
        "publication_date": "2017-02-12",
        "authors": "Yolanda Gil, Daniel Garijo, Varun Ratnakar, Rajiv Mayani, Ravali Adusumilli, Hunter Boyce, Arunima Srivastava, Parag Mallick",
        "file_name": "20250512001506.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/aaai2017/PDFs/20250512001506.pdf",
        "pdf_link": "https://dgarijo.com/papers/aaai2017.pdf",
        "file_html": "html/20250512001506.html",
        "url": "https://dgarijo.com/papers/aaai2017.pdf",
        "pages": "4406--4414",
        "booktitle": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
        "year": "2017",
        "author": "Gil, Yolanda and Garijo, Daniel and Ratnakar, Varun and Mayani, Rajiv and Adusumilli, Ravali and Boyce, Hunter and Srivastava, Arunima and Mallick, Parag",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2017towards2"
    },
    {
        "title": "Automated Hypothesis Testing with Large Scientific Data Repositories",
        "implementation_urls": [],
        "abstract": "Abstract The automation of important aspects of scientific data analysis would significantly accelerate the pace of science and innovation. Although there has been a lot of work done towards that automation, the hypothesize-test-evaluate discovery cycle is still largely carried out by hand by researchers.  This introduces a significant human bottleneck, which leads to inefficiencies, potential errors, and incomplete explorations of the hypothesis and data analysis space. We introduce a novel approach to automate the hypothesize-test-evaluate discovery cycle with an intelligent system that a scientist can task to test hypotheses of interest against a data repository. Our approach captures three types of data analytics knowledge: 1) common data analytic methods represented as semantic workflows; 2) meta-analysis methods that aggregate those results, represented as meta-workflows; and 3) data analysis strategies that specify for a type of hypothesis what data and methods to use, represented as lines of inquiry.  Given a hypothesis specified by a scientist, appropriate lines of inquiry are triggered, which lead to retrieving relevant datasets, running relevant workflows on that data, and finally running meta-workflows on workflow results.  The scientist is then presented with a level of confidence on the initial hypothesis, a revised hypothesis, and possibly with new hypotheses. We have implemented this approach in the DISK system, and applied it to multi-omics data analysis. 1.  Introduction The rate of data collection has vastly surpassed our ability to analyze it.  In science, massive amounts of data are already available in repositories, waiting to be analyzed [Tomczak et al 2015, Rudnick et al. 2016].  As these repositories are constantly growing, an analysis performed today may give different results when performed in the future. Data analytics expertise is not easily disseminated, and institutions have more data than experts to analyze it.  For example, in a recent survey of reviewers of Science magazine (which could be considered to be at the top of their field) a majority of respondents said that their lab did not have the necessary expertise to analyze Y. GIL, D. GARIJO, V. RATNAKAR, R. MAYANI, R. ADUSUMILLI, H. BOYCE, P. MALLICK  2 the data they already have [Science 2011].  The situation is likely worse for the vast majority of scientists, and in less privileged institutions and sectors.  Indeed, data analytic processes are currently carried out by hand by investigators, introducing a significant human bottleneck that can lead to erroneous and incomplete explorations, and hampers reproducibility [Begley and Ellis 2012].  The automation of important aspects of scientific discovery would significantly accelerate the pace of science and innovation. Although scientific discovery may involve complex representational and paradigm changes [Kuhn 1962], AI researchers have automated important aspects of discovery such as experiment design and testing [Kulkarni and Simon 1988; King et al 2009] and induction of laws from given datasets [Langley et al 1987; Valdes-Perez 1997; Todorovski et al 2000; Schmidt and Lipson 2009]. Once a representation is chosen, discovery processes often involve searching through the space of possible hypotheses and models.   Our goal is to develop an intelligent system able to conduct hypothesis-driven data analysis of science data repositories. In many science domains, comprehensive data repositories are being developed with large amounts of diverse data.  Given the necessary knowledge and methods, an intelligent system could autonomously analyze the data in a systematic, comprehensive, and efficient manner [Buchanan and Waltz 2009; Gil et al 2014]. Automating data analyses would also enforce consistency, as they would follow processes recognized by experts in the field of study.  In addition, automation would facilitate inspectability and reproducibility of results. In this paper we introduce a novel approach to automate the hypothesize-test-evaluate discovery cycle by capturing data analytics expertise and applying it to automatically test given hypotheses against existing data repositories.  Our approach captures three types of data analytics ",
        "file_name": "20250512001428.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/acs2016/PDFs/20250512001428.pdf",
        "pdf_link": "https://dgarijo.com/papers/acs2016.pdf",
        "file_html": "html/20250512001428.html",
        "funding": "DARPA W911NF-15-1-0555, NIH 1U01CA196387",
        "url": "https://dgarijo.com/papers/acs2016.pdf",
        "booktitle": "Proceedings of the Fourth Annual Conference on Advances in Cognitive Systems (ACS)",
        "year": "2016",
        "author": "Gil, Yolanda and Garijo, Daniel and Ratnakar, Varun and Mayani, Rajiv and Adusumilli, Ravali and Boyce, Hunter and Mallick, Parag",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2016automated"
    },
    {
        "title": "PaCTS 1.0: A Crowdsourced Reporting Standard for Paleoclimate Data",
        "implementation_urls": [],
        "abstract": "Abstract The progress of science is tied to the standardization of measurements, instruments, and data.This is especially true in the Big Data age, where analyzing large data volumes critically hinges on the databeing standardized. Accordingly, the lack of community‐sanctioned data standards in paleoclimatologyhas largely precluded the benefits of Big Data advances in the field. Building upon recent efforts tostandardize the format and terminology of paleoclimate data, this article describes the PaleoclimateCommunity reporTing Standard (PaCTS), a crowdsourced reporting standard for such data. PaCTS captureswhich information should be included when reporting paleoclimate data, with the goal of maximizing thereuse value of paleoclimate data sets, particularly for synthesis work and comparison to climate modelsimulations. Initiated by the LinkedEarth project, the process to elicit a reporting standard involved aninternational workshop in 2016, various forms of digital community engagement over the next few years,and grassroots working groups. Participants in this process identified important properties acrosspaleoclimate archives, in addition to the reporting of uncertainties and chronologies; they also identifiedarchive‐specific properties and distinguished reporting standards for new versus legacy data sets. This workshows that at least 135 respondents overwhelmingly support a drastic increase in the amount of metadataaccompanying paleoclimate data sets. Since such goals are at odds with present practices, we discuss atransparent path toward implementing or revising these recommendations in the near future, using bothbottom‐up and top‐down approaches.Plain Language Summary Standardizing the way data are described and shared is key toaccelerating the progress of science. Building on recent advances in paleoceanography andpaleoclimatology, we present the first community‐led reporting standard for such datasets. The PaleoclimateCommunity reporTing Standard (PaCTS) provides guidelines as to which information should be includedwhen reporting data from various paleoclimate archives, as well as themes common to many fields, likeuncertainty and other site‐specific information. The ultimate goal of this effort is to (1) make these datasetsmore re‐usable over the long term, and (2) provide a roadmap for implementing and revising the standard,as the field of paleoclimatology and its practitioners both evolve. The requirements are driven by thediffering needs of data producers and the data consumers, who often have different goals in mind. Thus,agreeing on and writing up these requirements involves building consensus among the community to decideon their present and future goals.10.1029/2019PA003632Paleoceanography and PaleoclimatologyKHIDER ET AL.Oxford Brookes University, Oxford, UK, University of Copenhagen, Copenhagen, Denmark, Max Planck Institute forChemistry, Mainz, Germany, Lamont‐8 Doherty Earth Observatory, Columbia University, Palisades, NW, USA157141. IntroductionPaleoclimatology is a highly integrative discipline, often requiring the comparison of multiple data setsand model simulations to reach fundamental insights about the climate system. Currently, suchsyntheses are hampered by the time and effort required to transform the data into a usable formatfor each application. This task, called data wrangling, is estimated to consume up to 80% of researchertime in some scientific fields (Dasu & Johnson, 2003), an estimate commensurate with the experienceof many paleoclimatologists, particularly at the early‐career stage. Wrangling involves not onlyidentifying missing values or outliers in the time series but also searching multiple databases forthe scattered records, contacting the original investigators for the missing data and metadata, andorganizing the data into a machine‐readable format. Further, this wrangling requires an understandingof each data set's originating field and its unspoken practices and so cannot be easily automated or out-sourced to unskilled labor or software. There is therefore an acute need for standardizing paleoclimatedata sets.Indeed, standardization accelerates scientific progress, particularly in the era of Big Data, where data shouldbe Findable, Accessible, Interoperable, and Reusable (FAIR; Wilkinson et al., 2016). Standardization iscritical to many scientific endeavors: efficiently querying databases, analyzing the data and visualizing",
        "file_name": "20250512003002.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/agu_2019/PDFs/20250512003002.pdf",
        "pdf_link": "https://dgarijo.com/papers/agu_2019.pdf",
        "file_html": "html/20250512003002.html",
        "eprint": "https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019PA003632",
        "keywords": "standards, FAIR, paleoclimate, paleoceanography, data, best practices",
        "url": "https://dgarijo.com/papers/agu_2019.pdf",
        "doi": "10.1029/2019PA003632",
        "pages": "1570--1596",
        "number": "10",
        "volume": "34",
        "journal": "Paleoceanography and Paleoclimatology",
        "year": "2019",
        "author": "Khider, D. and Emile-Geay, J. and McKay, N. P. and Gil, Y. and Garijo, D. and Ratnakar, V. and Alonso-Garcia, M. and Bertrand, S. and Bothe, O. and Brewer, P. and Bunn, A. and Chevalier, M. and Comas-Bru, L. and Csank, A. and Dassié, E. and DeLong, K. and Felis, T. and Francus, P. and Frappier, A. and Gray, W. and Goring, S. and Jonkers, L. and Kahle, M. and Kaufman, D. and Kehrwald, N. M. and Martrat, B. and McGregor, H. and Richey, J. and Schmittner, A. and Scroxton, N. and Sutherland, E. and Thirumalai, K. and Allen, K. and Arnaud, F. and Axford, Y. and Barrows, T. and Bazin, L. and Pilaar Birch, S. E. and Bradley, E. and Bregy, J. and Capron, E. and Cartapanis, O. and Chiang, H.-W. and Cobb, K. M. and Debret, M. and Dommain, R. and Du, J. and Dyez, K. and Emerick, S. and Erb, M. P. and Falster, G. and Finsinger, W. and Fortier, D. and Gauthier, Nicolas and George, S. and Grimm, E. and Hertzberg, J. and Hibbert, F. and Hillman, A. and Hobbs, W. and Huber, M. and Hughes, A. L. C. and Jaccard, S. and Ruan, J. and Kienast, M. and Konecky, B. and Le Roux, G. and Lyubchich, V. and Novello, V. F. and Olaka, L. and Partin, J. W. and Pearce, C. and Phipps, S. J. and Pignol, C. and Piotrowska, N. and Poli, M.-S. and Prokopenko, A. and Schwanck, F. and Stepanek, C. and Swann, G. E. A. and Telford, R. and Thomas, E. and Thomas, Z. and Truebe, S. and von Gunten, L. and Waite, A. and Weitzel, N. and Wilhelm, B. and Williams, J. and Williams, J. J. and Winstrup, M. and Zhao, N. and Zhou, Y.",
        "ENTRYTYPE": "article",
        "ID": "doi:10.1029/2019PA003632"
    },
    {
        "title": "AI buzzwords explained: scientific workflows",
        "implementation_urls": [],
        "abstract": "[29] D. Garijo and Y. Gil, “A New Approach for Publishing Workflows: Abstractions, Standards, and Linked Data,” in Proceedings of the 6th workshop on Workflows in support of large-scale science, Seattle, 2011, pp. 47–56. [30] K. Belhajjame et al., “Using a suite of ontologies for preserving workflow-centric Research Objects,” Web Semant. Sci. Serv. Agents World Wide Web, 2015. [31] T. Lebo et al., “The PROV ontology, W3C Recommendation,” WWW Consortium, Apr. 2013. [32] P. Missier, S. Dey, K. Belhajjame, V. Cuevas-Vicenttín, and B. Ludäscher, “D-PROV: Extending the PROV Provenance Model with Workflow Structure,” in Proceedings of the 5th USENIX Workshop on the Theory and Practice of Provenance, Lombard, Illinois, 2013, p. 9:1–9:7. [33] D. D. Roure, C. A. Goble, and R. Stevens, “The design and realisation of the myExperiment Virtual Research Environment for social sharing of workflows,” Future Gener. Comp Syst, vol. 25, no. 5, pp. 561–567, 2009. [34] P. Mates, E. Santos, J. Freire, and C. T. Silva, “CrowdLabs: Social Analysis and Visualization for the Sciences,” in 23rd International Conference on Scientific and Statistical Database Management (SSDBM), 2011, pp. 555–564. [35] K. Belhajjame et al., “A workflow PROV-corpus based on Taverna and Wings,” in Proceedings of the Joint EDBT/ICDT 2013 Workshops, Genoa, Italy, 2013, pp. 331–332. [36] F. Chirigati, D. Shasha, and J. Freire, “ReproZip: Using Provenance to Support Computational Reproducibility,” in Proceedings of the 5th USENIX Workshop on the Theory and Practice of Provenance, Lombard, Illinois, 2013, p. 1:1–1:4. [37] I. Santana-Pérez and M. Pérez-Hernández, “Towards Reproducibility in Scientific Workflows: An Infrastructure-Based Approach,” Sci. Program., vol. 2015, p. 11, 2015. [38] Qasha, Wawaa, Cala, Jacek, and Watson, Paul, “A Framework for Scientific Workflow Reproducibility in the Cloud,” in IEEE 12th International Conference on eScience. [39] Y. Gil et al., “Examining the Challenges of Scientific Workflows,” Computer, vol. 40, no. 12, pp. 24–32, Dec. 2007.  ",
        "file_name": "20250512001721.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/aibuzzword2017/PDFs/20250512001721.pdf",
        "pdf_link": "https://dgarijo.com/papers/aibuzzword2017.pdf",
        "file_html": "html/20250512001721.html",
        "url": "https://dgarijo.com/papers/aibuzzword2017.pdf",
        "doi": "10.1145/3054837.3054839",
        "pages": "4--8",
        "number": "1",
        "volume": "3",
        "publisher": "ACM",
        "journal": "AI Matters",
        "year": "2017",
        "author": "Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "garijo2017ai"
    },
    {
        "title": "P4ML: A Phased Performance-Based Pipeline Planner for Automated Machine Learning",
        "implementation_urls": [],
        "abstract": "AbstractWhile many problems could benefit from recent advances in machine learning, significanttime and expertise are required to design customized solutions to each problem. Priorattempts to automate machine learning have focused on generating multi-step solutionscomposed of primitive steps for feature engineering and modeling, but using already cleanand featurized data and carefully curated primitives. However, cleaning and featurizationare often the most time-consuming steps in a data science pipeline. We present a novelapproach that works with naturally occurring data of any size and type, and with diversethird-party data processing and modeling primitives that can lead to better quality so-lutions. The key idea is to generate multi-step pipelines (or workflows) by factoring thesearch for solutions into phases that apply a different expert-like strategy designed to im-prove performance. This approach is implemented in the P4ML system, and demonstratessuperior performance over other systems on a variety of raw datasets.Keywords: Automating machine learning, planning, pipelines, workflows, AutoML1. IntroductionMachine learning applications require significant expertise, tuning and effort. Research onautomating machine learning (AutoML) focuses on developing approaches to automaticallygenerate models for a given dataset, including any necessary featurization and data prepa-ration steps. Early work in this area explored the use of artificial intelligence planning togenerate multi-step pipelines (i.e., workflows) composed of data pre-processing and modelingsteps (St. Amant and Cohen, 1998; Hauder et al., 2011). Given the ubiquity of data and thegreat interest in exploiting it, AutoML has been receiving increased attention. Auto-sklearnuses Bayesian optimization methods, and placed first in the ChaLearn AutoML challenge(Feurer et al., 2015a). Fusi et al. (2017) augmented that approach with probabilistic ma-trix factorization. A very different approach was used in TPOT, which relies on geneticalgorithms to explore combinations of steps that lead to better performance (Olson et al.,2016b). However, there are still many open research topics in AutoML. Existing approachesfocus on feature engineering and modeling, and assume that the data is already clean andthat numerical features have already been generated. They use a carefully selected andwell-curated set of pre-processing and modeling steps (or primitives) in order to make thesearch manageable. They also focus on classification tasks. Our goal is to design AutoMLapproaches that can accommodate any type of naturally occurring data, any collection ofc© 2018 Y. Gil et al.P4ML: A Phased Performance-Based Pipeline Planner for Automated Machine Learningprimitives, and any size of data. This paper presents a novel approach to AutoML thatsupports these goals and that has three key contributions:1. Exploits expert strategies to structure the search for solutions in a machine learningproblem into meaningful phases,2. Uses knowledge about both datasets and primitives in order to design end-to-endpipelines that include data cleaning and featurization steps,3. Explores the search space efficiently and returns the best solution found within a giventime limit.The paper begins articulating our goals and requirements, followed by an overview of ourapproach. We then describe the implementation of our approach in P4ML, a phasedperformance-based pipeline planner for automating machine learning. P4ML was popu-lated with dozens of diverse third-party primitives, and the evaluations so far demonstratesuperior performance on a variety of naturally occurring datasets.2. Goals and RequirementsOur goal is to automate machine learning with approaches that will handle naturally oc-curring datasets, which leads us to several important requirements not addressed in prior",
        "file_name": "20250512000715.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/AutoML2018/PDFs/20250512000715.pdf",
        "pdf_link": "https://dgarijo.com/papers/AutoML2018.pdf",
        "file_html": "html/20250512000715.html",
        "url": "https://dgarijo.com/papers/AutoML2018.pdf",
        "booktitle": "Proceedings of Machine Learning Research, ICML 2018 AutoML Workshop",
        "year": "2018",
        "author": "Yolanda Gil and Ke-Thia Yao and Varun Ratnakar and Daniel Garijo and Greg Ver Steeg and Pedro Szekely and Rob Brekelmans and Mayank Kejriwal and Fanghao Luo and I-Hui Huang",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2018p4ml"
    },
    {
        "title": "A study of the quality of Wikidata",
        "implementation_urls": [],
        "abstract": "Wikidata has been increasingly adopted by many communities for a wide variety of applications, which demand high-quality knowledge to deliver successful results. In this paper, we develop a framework to detect and analyze low-quality statements in Wikidata by shedding light on the current practices exercised by the community. We explore three indicators of data quality in Wikidata, based on: (1) community consensus on the currently recorded knowledge, assuming that statements that have been removed and not added back are implicitly agreed to be of low quality; (2) statements that have been deprecated; and (3) constraint violations in the data. We combine these indicators to detect low-quality statements, revealing challenges with duplicate entities, missing triples, violated type rules, and taxonomic distinctions. Our findings complement ongoing efforts by the Wikidata community to improve data quality, aiming to make it easier for users and editors to find and correct mistakes.",
        "publication_date": "2009-01-01",
        "authors": "Elsevier Sdol",
        "file_name": "20250511235602.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/a_study_wikidata_2022/PDFs/20250511235602.pdf",
        "pdf_link": "https://dgarijo.com/papers/a_study_wikidata_2022.pdf",
        "file_html": "html/20250511235602.html",
        "keywords": "Wikidata, Data quality, Knowledge graphs, Constraints, Crowdsourcing",
        "author": "Kartik Shenoy and Filip Ilievski and Daniel Garijo and Daniel Schwabe and Pedro Szekely",
        "url": "https://dgarijo.com/papers/a_study_wikidata_2022.pdf",
        "doi": "10.1016/j.websem.2021.100679",
        "issn": "1570-8268",
        "year": "2022",
        "pages": "100679",
        "volume": "72",
        "journal": "Journal of Web Semantics",
        "ENTRYTYPE": "article",
        "ID": "SHENOY2022100679"
    },
    {
        "title": "Best Practices for Implementing FAIR Vocabularies and Ontologies on the Web",
        "implementation_urls": [],
        "doi": "10.3233/SSW200034",
        "arxiv": "2003.13084",
        "abstract": "Abstract. With the adoption of Semantic Web technologies, an increas-ing number of vocabularies and ontologies have been developed in differ-ent domains, ranging from Biology to Agronomy or Geosciences. How-ever, many of these ontologies are still difficult to find, access and un-derstand by researchers due to a lack of documentation, URI resolvingissues, versioning problems, etc. In this chapter we describe guidelinesand best practices for creating accessible, understandable and reusableontologies on the Web, using standard practices and pointing to exist-ing tools and frameworks developed by the Semantic Web community.We illustrate our guidelines with concrete examples, in order to helpresearchers implement these practices in their future vocabularies.Keywords: Ontology metadata · Ontology publication · Ontology ac-cess · FAIR principles · Linked Data principles.1 IntroductionIn the last decade, a series of initiatives for open data, transparency and openscience have led to the development of a myriad of datasets and linked Knowl-edge Graphs on the Web.3 Ontologies and vocabularies have been developedaccordingly to represent the contents of these datasets and Knowledge Graphsand help in their integration and linking. However, while significant effort hasbeen spent on making data Findable, Accessible, Interoperable and Reusable(FAIR) [18], ontologies and vocabularies are often difficult to access, understandand reuse. This may be due to several reasons, including a lack of definitionsof ontology classes and properties; deprecated or unavailable imported ontolo-gies, non-resolvable ontology URIs, lack of examples and diagrams in the docu-mentation, or having scientific publications describing an ontology without anyreference to its implementation.The scientific community has started to acknowledge the need for ontologiesto be properly documented, versioned, published and maintained following the3 https://lod-cloud.net/https://lod-cloud.net/2 Garijo and Poveda-VillalónLinked Data Principles [6] and adapting the FAIR principles for data [8]. Butthese recommendations do not include guidelines on how to implement them for atarget vocabulary. In this chapter we address this issue by describing how to makean ontology or vocabulary comply with the FAIR principles, including examplessummarizing best practices from the community and our own experience; andpointing to popular existing tools and frameworks.Our guidelines are aimed at ontology engineers, and therefore the chapter isstructured following an ontology development process: Section 2 describes designdecisions to be considered when creating an ontology URI (naming conventions,versioning, permanent URIs); Section 3 describes how to create a documentationthat is easy to reuse and understand by others (with minimum metadata anddiagrams); Section 4 illustrates how to make an ontology accessible and findableon the Web; Section 5 points to existing end-to-end frameworks that support theontology publication process; and Section 6 concludes our chapter. We considerthe design and development of an ontology out of the scope of this chapter, asit has been covered by existing methodologies (e.g., LOT4 or NeOn [14]).2 Accessible Ontology URI DesignOntologies are digital artifacts, and therefore they should follow the Linked DataPrinciples,5 and use a URI namespace under control of its authors. The rationale",
        "publication_date": "2020-11-12",
        "authors": "Daniel Garijo, María Poveda‐Villalón",
        "file_name": "20250512003308.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/best_practices2020/PDFs/20250512003308.pdf",
        "pdf_link": "https://dgarijo.com/papers/best_practices2020.pdf",
        "file_html": "html/20250512003308.html",
        "editor": "Giuseppe Cota, Marilena Daquino and Gian Luca Pozzato",
        "url": "https://dgarijo.com/papers/best_practices2020.pdf",
        "address": "Netherlands",
        "publisher": "{IOS} Press",
        "booktitle": "Applications and Practices in Ontology Design, Extraction, and Reasoning",
        "year": "2020",
        "author": "Daniel Garijo and Maria Poveda-Villalón",
        "ENTRYTYPE": "incollection",
        "ID": "garijo2020"
    },
    {
        "title": "EAAI-22 Blue Sky Ideas in Artificial Intelligence Education from the AAAI/ACM SIGAI New and Future AI Educator Program",
        "implementation_urls": [],
        "abstract": "The 12th Symposium on Educational Advances in Artificial Intelligence (EAAI-22, cochaired by Michael Guerzhoy and Marion Neumann) continued the AAAI/ACM SIGAI New and Future AI Educator Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia). As part of the program, awardees were asked to address one of the following \"blue sky\" questions:•How could/should AI courses incorporate AI Ethics into the curriculum?•How could we teach AI topics at an early undergraduate or a secondary school level?•AI has the potential for broad impact to numerous disciplines. How could we make AI education more interdisciplinary, specifically to benefit non-engineering fields?•How should standard AI courses evolve?•How could we leverage AI education to promote diversity in the field?This paper is a collection of their responses, intended to help motivate discussion around these issues in AI education.",
        "file_name": "20250511235407.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/blue_sky_cr/PDFs/20250511235407.pdf",
        "pdf_link": "https://dgarijo.com/papers/blue_sky_cr.pdf",
        "file_html": "html/20250511235407.html",
        "numpages": "6",
        "pages": "16–21",
        "month": "nov",
        "journal": "AI Matters",
        "doi": "10.1145/3557785.3557789",
        "url": "https://dgarijo.com/papers/blue_sky_cr.pdf",
        "number": "2",
        "volume": "8",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "issue_date": "June 2022",
        "year": "2022",
        "author": "Guerzhoy, Michael and Neumann, Marion and Johnson, Emmanuel and Johnson, David and Chai, Henry and Garijo, Daniel and Lyu, Zhuoyue and MacLellan, Christopher J.",
        "ENTRYTYPE": "article",
        "ID": "10.1145/3557785.3557789"
    },
    {
        "title": "{KGTK}: A Toolkit for Large Knowledge Graph Manipulation and Analysis",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/kgtk",
                "type": "git",
                "paper_frequency": 5,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "README_TEXT",
                        "location_type": "ARXIV",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "RELATED_PAPERS",
                        "location_type": "ARXIV",
                        "source": "SOMEF"
                    },
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/Camera_ready_ISWC__KGTK/PDFs/20250512002243.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Resource type: Software License: MIT DOI: https://doi.org/10.5281/zenodo.3828068 Repository: https://github.com/usc-isi-i2/kgtk/Keywords: knowledge graph · knowledge graph embedding · knowledge graph filtering · knowledge graph manipulation 1 Introduction Knowledge graphs (KGs) have become the preferred technology for representing, sharing and using knowledge in applications."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62466-8_18",
        "arxiv": "2006.00088",
        "abstract": "Abstract. Knowledge graphs (KGs) have become the preferred tech-nology for representing, sharing and adding knowledge to modern AIapplications. While KGs have become a mainstream technology, theRDF/SPARQL-centric toolset for operating with them at scale is hetero-geneous, difficult to integrate and only covers a subset of the operationsthat are commonly needed in data science applications. In this paper wepresent KGTK, a data science-centric toolkit designed to represent, cre-ate, transform, enhance and analyze KGs. KGTK represents graphs intables and leverages popular libraries developed for data science applica-tions, enabling a wide audience of developers to easily construct knowl-edge graph pipelines for their applications. We illustrate the frameworkwith real-world scenarios where we have used KGTK to integrate andmanipulate large KGs, such as Wikidata, DBpedia and ConceptNet.Resource type: SoftwareLicense: MITDOI: https://doi.org/10.5281/zenodo.3828068Repository: https://github.com/usc-isi-i2/kgtk/Keywords: knowledge graph · knowledge graph embedding · knowledgegraph filtering · knowledge graph manipulation1 IntroductionKnowledge graphs (KGs) have become the preferred technology for representing,sharing and using knowledge in applications. A typical use case is building a newknowledge graph for a domain or application by extracting subsets of several ex-isting knowledge graphs, combining these subsets in application-specific ways,augmenting them with information from structured or unstructured sources, andcomputing analytics or inferred representations to support downstream applica-tions. For example, during the COVID-19 pandemic, several efforts focused on2 Filip Ilievski, Daniel Garijo et. al.building KGs about scholarly articles related to the pandemic starting from theCORD-19 dataset provided by the Allen Institute for AI [25].1 Enhancing thesedata with with KGs such as DBpedia [1] and Wikidata [24] to incorporate gene,chemical, disease and taxonomic information, and computing network analyticson the resulting graphs, requires the ability to operate these these KGs at scale.Many tools exist to query, transform and analyze KGs. Notable examples in-clude graph databases, such as RDF triple stores and Neo4J;2 tools for operatingon RDF such as graphy3 and RDFlib4, entity linking tools such as WAT [18] orBLINK [26], entity resolution tools such as MinHash-LSH [14] or MFIBlocks [12],libraries to compute graph embeddings such as PyTorch-BigGraph [13] and li-braries for graph analytics, such as graph-tool5 and NetworkX.6There are three main challenges when using these tools together. First, toolsmay be challenging to set up with large KGs (e.g., the Wikidata RDF dump takesover a week to load into a triple store) and often need custom configurationsthat require significant expertise. Second, interoperating between tools requiresdeveloping data transformation scripts, as some of them may not support thesame input/output representation. Third, composing two or more tools together(e.g., to filter, search, and analyze a KG) includes writing the intermediate resultsto disk, which is time and memory consuming for large KGs.In this paper, we introduce the Knowledge Graph Toolkit (KGTK), a frame-work for manipulating, validating, and analyzing large-scale KGs. Our work isinspired by Scikit-learn [17] and SpaCy,7 two popular toolkits for machine learn-",
        "publication_date": "2020-01-01",
        "authors": "Filip Ilievski, Daniel Garijo, Hans Chalupsky, Naren Teja Divvala, Yixiang Yao, Craig Milo Rogers, Rongpeng Li, Jun Liu, Amandeep Singh, Daniel Schwabe, Pedro Szekely",
        "file_name": "20250512002243.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/Camera_ready_ISWC__KGTK/PDFs/20250512002243.pdf",
        "pdf_link": "https://dgarijo.com/papers/Camera_ready_ISWC__KGTK.pdf",
        "file_html": "html/20250512002243.html",
        "organization": "Springer, Cham",
        "url": "https://dgarijo.com/papers/Camera_ready_ISWC__KGTK.pdf",
        "year": "2020",
        "pages": "278--293",
        "booktitle": "International Semantic Web Conference",
        "author": "Ilievski, Filip and Garijo, Daniel and Chalupsky, Hans and Teja Divvala, Naren and Yao, Yixiang and Rogers, Craig and Li, Ronpeng  and Liu,  and  Singh, Amandeep and Schwabe, Daniel and Szekely, Pedro",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2020KGTK"
    },
    {
        "title": "NiW: Converting Notebooks into Workflows to Capture Dataflow and Provenance",
        "implementation_urls": [],
        "abstract": "ABSTRACT Interactive notebooks are increasingly popular among scientists to expose computational methods and share their results.  However, it is often challenging to track their dataflow, and therefore the provenance of their results. This paper presents an approach to convert notebooks into scientific workflows that capture explicitly the dataflow across software components and facilitate tracking provenance of new results. In our approach, users should first write notebooks according to a set of  guidelines that we have designed, and then use an automated tool to generate workflow descriptions from the modified notebooks. Our approach is implemented in NiW (Notebooks into Workflows), and we demonstrate its use by generating workflows with third-party notebooks.  The resulting workflow descriptions have explicit dataflow, which facilitates tracking provenance of new results, comparison of workflows, and sub-workflow mining.  Our guidelines can also be used to improve understandability of notebooks by making the dataflow more explicit. CCS CONCEPTS • Information systems → Artificial intelligence; Knowledge representation and reasoning KEYWORDS Scientific Workflows; Workflow Design; Electronic Notebooks. 1 INTRODUCTION Interactive notebooks have become very popular in science to capture computational experiments [14]. These notebooks include code, visualizations, and explanations, and can be easily shared and re-run. As scientists carry out their research, they may need to compare the results and methods of different experiments.  This involves comparing final results, comparing intermediate results, comparing steps of the method, and comparing parameter values.  Since notebooks contain raw code, it can be hard to understand how new results are generated, as well as to compare notebooks.  In contrast, workflows offer modular components to run code, and have an explicit dataflow. This can facilitate provenance capture, as well as automated mining of reusable workflow fragments [4].  Workflows also facilitates understanding and performing comparisons, particularly for non-programmers [6]. This paper presents an approach for converting notebooks into workflow descriptions by mapping various aspects of notebook cells into workflow components and dataflow. Our approach is implemented in NiW, a prototype tool to convert Jupyter Notebooks 1  into WINGS workflows [7]. Based on the assumptions of our approach, we propose a set of guidelines for designing notebooks that facilitate the conversion and can be used by notebook developers to improve the understandability of their notebooks.  2 DATAFLOW AND PROVENANCE IN WORKFLOWS AND NOTEBOOKS ",
        "publication_date": "2017-01-01",
        "authors": "Lucas Carvalho, Regina Wang, Yolanda Gil, Daniel Garijo",
        "file_name": "20250512000742.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/carvalho_niw/PDFs/20250512000742.pdf",
        "pdf_link": "https://dgarijo.com/papers/carvalho_niw.pdf",
        "file_html": "html/20250512000742.html",
        "funding": "USNSF ICER-1440323, SPRF 2017/03570-3",
        "url": "https://dgarijo.com/papers/carvalho_niw.pdf",
        "address": "Austin, Texas",
        "booktitle": "Proceedings of the Workshop on Capturing Scientific Knowledge (SciKnow), held in conjunction with the ACM International Conference on Knowledge Capture (K-CAP)",
        "year": "2017",
        "author": "Lucas A. M. C. Carvalho and Regina Wang and Yolanda Gil and Daniel Garijo",
        "ENTRYTYPE": "inproceedings",
        "ID": "carvalho-etal-sciknow2017b"
    },
    {
        "title": "Requirements for Supporting the Iterative Exploration of Scientific Workflow Variants",
        "implementation_urls": [],
        "abstract": "ABSTRACT Workflow systems support scientists in capturing computational experiments and managing their execution.  However, such systems are not designed to help scientists create and track the many related workflows that they build as variants, trying different software implementations and distinct ways to process data and deciding what to do next by looking at previous workflow results.  An initial workflow will be changed to create many new variants thereof that differ from each other in one or more steps. Our goal is to support scientists in the iterative design of computational experiments by assisting them in the creation and management of workflow variants.  In this paper, we present several use cases for creating workflow variants in hydrology, from which we specify requirements for workflow variants.  We also discuss major research directions to address these requirements. CCS CONCEPTS • Information systems → Artificial intelligence; Knowledge representation and reasoning KEYWORDS Scientific workflows, workflow variants, computational experiments 1 INTRODUCTION Scientific workflow systems play a major role in supporting scientists to design, document and execute their computational experiments, automatically tracking provenance during the workflow execution [11; 1]. Scientists follow an iterative exploratory cycle where they often create an initial workflow, and then explore variations of it using different data, replacing some of the software steps, or adding new steps. Sometimes workflows have to be modified because of changes in data (e.g. when datasets are updated with new formats) or software (e.g., software is no longer available, a newer version is better).  In current workflow systems, scientists manage this exploratory process manually. Updating a workflow is a complex and time-consuming task that may involve several steps, and may require tracking down information about different versions of the software used in the workflow.  This paper presents use cases and their requirements to support scientists in the process of exploring different variations of an original workflow, and introduces research directions to address these requirements. These scenarios are based on discussion with domain scientists, particularly in hydrology and bioinformatics.  2 WORKFLOW VARIANTS  Computational workflows describe the computational steps and the dataflow among them to perform complex multi-step analyses. The steps are implemented by software components (or workflow components) that process data. A software component has a well-defined interface consisting of input and output files as well as parameter constant values. The dataflow between components is captured as connections among their respective interfaces. A ",
        "file_name": "20250512000756.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/carvalho_sciknow17/PDFs/20250512000756.pdf",
        "pdf_link": "https://dgarijo.com/papers/carvalho_sciknow17.pdf",
        "file_html": "html/20250512000756.html",
        "funding": "USNSF ICER-1440323, SPRF 2017/03570-3",
        "url": "https://dgarijo.com/papers/carvalho_sciknow17.pdf",
        "address": "Austin, Texas",
        "booktitle": "Proceedings of the Workshop on Capturing Scientific Knowledge (SciKnow), held in conjunction with the ACM International Conference on Knowledge Capture (K-CAP)",
        "year": "2017",
        "author": "Lucas A. M. C. Carvalho and Bakinam T. Essawy and Daniel Garijo and Claudia Bauzer Medeiros and Yolanda Gil",
        "ENTRYTYPE": "inproceedings",
        "ID": "carvalho-etal-sciknow2017a"
    },
    {
        "title": "{Engineering Academic Software (Dagstuhl Perspectives Workshop 16252)}",
        "implementation_urls": [
            {
                "identifier": "https://github.com/danielskatz/sustaining-research-projects",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/dagstuhl_2017/PDFs/20250512001734.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "URL: https://github.com/danielskatz/sustaining-research-projects."
                    }
                ]
            }
        ],
        "doi": "10.4230/DagMan.6.1.1",
        "abstract": "AbstractSoftware is often a critical component of scientific research. It can be a component of theacademic research methods used to produce research results, or it may itself be an academicresearch result. Software, however, has rarely been considered to be a citable artifact in its ownright. With the advent of open-source software, artifact evaluation committees of conferences,and journals that include source code and running systems as part of the published artifacts, weforesee that software will increasingly be recognized as part of the academic process. The qualityand sustainability of this software must be accounted for, both a priori and a posteriori.The Dagstuhl Perspectives Workshop on “Engineering Academic Software” has examined thestrengths, weaknesses, risks, and opportunities of academic software engineering. A key outcomeof the workshop is this Dagstuhl Manifesto, serving as a roadmap towards future professionalsoftware engineering for software-based research instruments and other software produced andused in an academic context. The manifesto is expressed in terms of a series of actionable“pledges” that users and developers of academic research software can take as concrete stepstowards improving the environment in which that software is produced.Perspectives Workshop June 19–24, 2016 – http://www.dagstuhl.de/162521998 ACM Subject Classification D.2.9 [Software Engineering] ManagementKeywords and phrases Academic software, Research software, Software citation, Software sus-tainabilityDigital Object Identifier 10.4230/DagMan.6.1.1∗ Main contact: Jurgen J. Vinju, Jurgen.Vinju@cwi.nl.Except where otherwise noted, content of this manifesto is licensedunder a Creative Commons BY 3.0 Unported licenseEngineering Academic Software, Dagstuhl Manifestos, Vol. 6, Issue 1, pp. 1–20Authors: A. Allen et al.Dagstuhl ManifestosSchloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germanyhttp://www.dagstuhl.de/16252http://dx.doi.org/10.4230/DagMan.6.1.1http://creativecommons.org/licenses/by/3.0/http://creativecommons.org/licenses/by/3.0/http://www.dagstuhl.de/dagman/http://www.dagstuhl.de2 Engineering Academic SoftwareExecutive SummaryAlthough the role of software is becoming increasingly important in diverse fields of research,it is commonly not given due recognition. Software is often not cited, or even consideredto be citable. Developers of research software are not given due credit. In cases wheresoftware embodies a core intellectual contribution of research, its creators may not be invitedas co-authors on papers disseminating that research. In cases where software that enabledresearch can be cited, it may not be. This Dagstuhl Perspectives Workshop explored thecurrent state of engineering of academic research software, identified common problems withits development, recognition, and sustainability, proposed a set of concrete actions to improvethe state of academic research software, expressed as a number of personal pledges, and putforward numerous future research directions to better understand and support academicsoftware.The personal pledges expressed in this Dagstuhl Manifesto1 address three general con-cerns: (i) ensuring that research software is properly cited; (ii) promoting the careers ofresearch software engineers who develop academic software; and (iii) ensuring the qualityand sustainability of software during and following its development:",
        "publication_date": "2017-01-01",
        "authors": "Alice Allen, Cecilia Aragón, Christoph Becker, Jeffrey C. Carver, Andrei Chiş, Benoît Combemale, Mike Croucher, Kevin Crowston, Daniel Garijo, Ashish Gehani, Carole Goble, Robert Haines, Robert M. A. Hirschfeld, James Howison, Kathryn D. Huff, Caroline Jay, Daniel S. Katz, Claude Kirchner, Katie Kuksenok, Ralf Lämmel, Oscar Nierstrasz, Matt Turk, Rob V. van Nieuwpoort, Matthew Vaughn, Jurgen Vinju",
        "file_name": "20250512001734.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/dagstuhl_2017/PDFs/20250512001734.pdf",
        "pdf_link": "https://dgarijo.com/papers/dagstuhl_2017.pdf",
        "file_html": "html/20250512001734.html",
        "annote": "Keywords: Academic software, Research software, Software citation, Software sustainability",
        "urn": "urn:nbn:de:0030-drops-71468",
        "editor": "Alice Allen et al.",
        "url": "https://dgarijo.com/papers/dagstuhl_2017.pdf",
        "issn": "2193-2433",
        "pages": "1--20",
        "number": "1",
        "volume": "6",
        "address": "Dagstuhl, Germany",
        "publisher": "Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik",
        "journal": "Dagstuhl Manifestos",
        "year": "2017",
        "author": "Alice Allen and Cecilia Aragon and Christoph Becker and Jeffrey Carver and Andrei Chis and Benoit Combemale and Mike Croucher and Kevin Crowston and Daniel Garijo and Ashish Gehani and Carole Goble and Robert Haines and Robert Hirschfeld and James Howison and Kathryn Huff and Caroline Jay and Daniel S. Katz and Claude Kirchner and Katie Kuksenok and Ralf L{\\\"a}mmel and Oscar Nierstrasz and Matt Turk and Rob van Nieuwpoort and Matthew Vaughn and Jurgen J. Vinju",
        "ENTRYTYPE": "article",
        "ID": "allen_et_al:DM:2017:7146"
    },
    {
        "title": "Extending DCAM for metadata provenance",
        "implementation_urls": [],
        "abstract": "AbstractThe  Metadata  Provenance  Task  Group  aims  to  define  a  data  model  that  allows  for  makingassertions  about  description  sets.  Creating  a  shared  model  of  the  data  elements  required  todescribe an aggregation of metadata statements allows to collectively import,  access, use andpublish facts about the quality, rights,  timeliness, data source type, trust situation, etc.  of thedescribed statements. In this paper we outline the preliminary model created by the task group,together with first examples that demonstrate how the model is to be used.Keywords: metadata; provenance; DCAM; model1.  IntroductionThe rise of the Web of Data during the last few years has increased the amount of informationavailable for users in a wide range of domains: digital libraries, scientific workflows, or socialnetworks among others. In order to provide high quality content to users, content providers havestarted to pay more attention to the provenance of their content: where does it come from, whocreated it, or how was it modified by other sources to produce its current version?1.1.  MotivationMany vocabularies and specifications have been developed for representing provenance, but nocommonly accepted standard or official recommendation has emerged yet. The W3C ProvenanceIncubator Group1 was launched in September 2009 with the objective of creating a roadmap and astate  of  the  art  report  of  the  current  approaches,  taking  the  first  steps  towards  a  domain-independent standard. The group also analyzed the “gaps” when trying to provide provenancesolutions in different domains (news aggregation2, scientific workflows3 and business contracts4)but  didn’t  analyze  deeply  the  topic  of  the  provenance  of  metadata  itself,  as  well  as  therepresentation of provenance information as metadata together with the described resources.The latter is already practiced in some areas like digital libraries or scientific workflows, butthese approaches are independent of each other and thus use different models and vocabularies.Therefore,  our  motivation  for  a  Dublin  Core  application  profile  for  metadata  provenance  istwofold: Firstly, we want to represent existing metadata provenance information in a simple andunified way that is well suited for an application of Dublin Core. Secondly, we want to enable theprovision of provenance information for Dublin Core metadata in a Dublin Core compatible way.1.2.  Related WorkAn early initiative to define a vocabulary and usage guidelines for the provenance of metadatawas the ACore (Iannella & Campbell, 1999) and, based on it, the proposal (Hansen & Andresen,1  http://www.w3.org/2005/Incubator/prov/wiki/Main_Page2  http://www.w3.org/2005/Incubator/prov/wiki/Analysis_of_News_Aggregator_Scenario3  http://www.w3.org/2005/Incubator/prov/wiki/Analysis_of_Disease_Outbreak_Scenario4  http://www.w3.org/2005/Incubator/prov/wiki/Analysis_of_Business_Contract_Scenario12Proc. Int’l Conf. on Dublin Core and Metadata Applications 20112001) for the DCMI Administrative Metadata Working Group5. The working group finished in2003 and presented the Administrative Components (AC), addressing metadata for the entirerecord, for update and change, and for batch interchange of records (Hansen & Andresen, 2003).Both  initiatives  focused  more  on  the  definition  of  specific  vocabularies  to  describe  theprovenance of metadata. There was not yet a concise model to relate the provenance informationwith the metadata.Later  initiatives  have  focused  more  on  domain-independent  provenance  representation.Vocabularies like OPM (Moreau et. al., 2010), Provenir (Sahoo et. al., 2010) and, more domain-specific,  the  Provenance  Vocabulary  (Hartig,  2009)  allow  for  representing  various  levels  ofprovenance as a hierarchy, but they are agnostic about the resource they are providing provenanceabout. So in the context of metadata, they leave the implementer alone to decide how to identifymetadata as a resource.",
        "publication_date": "2011-09-21",
        "authors": "Kai Eckert, Daniel Garijo, Michael Panzer",
        "file_name": "20250512000908.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/dc2011/PDFs/20250512000908.pdf",
        "pdf_link": "https://dgarijo.com/papers/dc2011.pdf",
        "file_html": "html/20250512000908.html",
        "url": "https://dgarijo.com/papers/dc2011.pdf",
        "pages": "12--25",
        "booktitle": "International Conference on Dublin Core and Metadata Applications",
        "year": "2011",
        "author": "Eckert, Kai and Garijo, Daniel and Panzer, Michael",
        "ENTRYTYPE": "inproceedings",
        "ID": "eckert2011extending"
    },
    {
        "title": "Coming to terms with FAIR ontologies: A position paper",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-61244-3_18",
        "abstract": "Abstract. Ontologies are widely used nowadays for many different purposes andinmany different contexts.When used for research, ontologies should be treated asother research artefacts, such as data, software, methods, etc.; following the sameprinciples used tomake themfindable, accessible, interoperable and reusable (FAIR)to others. However, in comparison to the number of guides, indicators and recom-mendations available for making research data FAIR, not much attention has beenpaid so far on how to publish ontologies following the FAIR principles. This po-sition paper reviews the technical and social needs required to define a roadmapfor generating and publishing FAIR ontologies on the Web. We analyze four ini-tiatives for ontology publication, aligning them in a common framework for com-parison. The paper concludes by opening a discussion about existing, ongoingand required initiatives and instruments to facilitate FAIR ontology sharing onthe Web.Keywords: FAIR principles · Ontologies · Semantics.1 IntroductionSince its inception in 2016, the FAIR (Findable, Accessible, Interoperable, Reusable)data principles [35] have gained an increasing importance in the context of researchdata management, and are being adopted by a large number of private and public or-ganisations worldwide, including initiatives such as the European Open Science Cloud3(EOSC) or the Research Data alliance4 (RDA).Ontologies play a relevant role in some of the FAIR data principles, especially inrelation to providing support for data “interoperability“ and “reusability“. The need forontologies (also called vocabularies) is pointed out in the following principles: data andmetadata should (I2)5 use vocabularies that follow FAIR principles, (I1) use a formal,accessible, shared, and broadly applicable language for knowledge representation; (I3)3 https://www.eosc-portal.eu/4 https://www.rd-alliance.org/5 We point in parentheses to the principles numeration used in the original FAIR paper [35]2 Poveda-Villalón et al.include qualified references to other (meta)data, and (R1.3) meet domain-relevant com-munity standards. And ontologies are also relevant in terms of “findability”, (F2) re-quiring to describe data with rich metadata, and “accessibility”, (A1) metadata shouldbe retrievable from a unique identifier.The research community has already acknowledged the need for ontologies to fol-low the FAIR principles [7]. First, there is a clear movement towards expanding theapplication of the FAIR principles beyond research data, as described in the ongoingEOSC Interoperability Framework [8]. Since ontologies are often the result of researchactivities or fundamental components in many areas of research, the FAIR principlesshould be applied to them, independently of whether they are used to describe data ormetadata. Second, ontologies are already identified as a relevant artefact in the prin-ciples (even though the term vocabulary is used more generally and there is a generalpreference to talk about semantic artefacts, including thesauri, glossaries, shared UMLmodels, etc.). Therefore, we consider that it is critical for the community to discuss andanalyse how the FAIR principles should be applied to these artefacts.However, we do not start from scratch when it comes to making ontologies availableto others. Before the appearance and general acceptance of FAIR principles in research,many approaches had already focused on how to publish ontologies on the Web follow-ing Linked Data principles, ensuring the existence of permanent identifiers and makingthem available through standardised protocols like HTTP ([4, 18, 21]). Other approachesfocused on making ontologies findable by creating metadata schemas and ontologies to",
        "publication_date": "2020-01-01",
        "authors": "María Poveda‐Villalón, Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": "20250512002601.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/EKAW2020_Coming_to_Terms_with_FAIR_Ontologies/PDFs/20250512002601.pdf",
        "pdf_link": "https://dgarijo.com/papers/EKAW2020_Coming_to_Terms_with_FAIR_Ontologies.pdf",
        "file_html": "html/20250512002601.html",
        "url": "https://dgarijo.com/papers/EKAW2020_Coming_to_Terms_with_FAIR_Ontologies.pdf",
        "booktitle": "Proceedings of the 22nd International Conference on Knowledge Engineering and Knowledge Management (EKAW 2020)",
        "pages": "255-270",
        "month": "September",
        "year": "2020",
        "author": "Poveda-Villalón, María and Espinoza-Arias, Paola and Garijo, Daniel and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2020EKAW"
    },
    {
        "title": "Extending Ontology Engineering Practices to Facilitate Application Development",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/oatapi",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/EKAW2022_espinoza/PDFs/20250511235420.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "9 An API specification template we have developed is available in https://github.com/oeg-upm/oatapi/tree/main/Additional%20Resources 10 Espinoza-Arias et al."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-17105-5_2",
        "abstract": "Abstract. Ontologies define data organization and meaning in Knowl-edge Graphs (KGs). However, ontologies have generally not been takeninto account when designing and generating Application ProgrammingInterfaces (APIs) to allow developers to consume KG data in a developer-friendly way. To fill this gap, this work proposes a method for API gener-ation based on the artefacts generated during the ontology developmentprocess. This method is described as part of a new phase, called ontologyexploitation, that may be included in the last stages of the traditionalontology development methodologies. Moreover, to support some of thetasks of the proposed method, we developed OATAPI, a tool that gen-erates APIs from two ontology artefacts: the competency questions andthe ontology serialization. The conclusions of this work reflect that thelimitations found in the state-of-the-art have been addressed both atthe methodological and tooling levels for the generation of APIs basedon ontology artefacts. Finally, the lines of future work present severalchallenges that need to be addressed so that the potential of KGs andontologies can be more easily exploited by application developers.Keywords: Ontology Engineering · Application Development · Appli-cation Programming Interface · Ontology Artefacts.1 IntroductionOver recent years, Knowledge Graphs (KGs) have been generated and adoptedby many organizations to integrate data, facilitate interoperability, and generatenew insights and recommendations. KGs are commonly structured according toontologies, which allow data to be unambiguously defined with a shared andagreed meaning, as well as to infer new knowledge. However, despite their adop-tion, KGs are still challenging to consume by application developers.On the one hand, developers face a production-consumption challenge: thereis a gap between the ontology engineers who design an ontology and may in-tervene in KG creation and the application developers who want to consumeits contents [7]. Ontologies may be complex, and the resources generated duringtheir development (use cases, requirements, etc.) are often not made available totheir users (e.g. application developers). As a result, developers usually need to2 Espinoza-Arias et al.duplicate some of the effort already done by ontology engineers when they wereunderstanding the domain, interacting with domain experts, taking modeling de-cisions, etc. On the other hand, application developers face technical challenges:many of them are not familiar with Semantic Web standards such as OWL andSPARQL, and hence those KGs that are exclusively based on Semantic Webtechnologies remain hardly accessible to them [18]. Developers (and in particu-lar application developers) are mostly used to data representation formats likeJSON and Application Programming Interfaces (APIs) for accessing data.In order to address both production-consumption and technical challenges,multiple approaches have been proposed by the Semantic Web community, rang-ing from Semantic RESTful APIs [15] which are compatible with Semantic Weband REST; to tools to create Web APIs on top of SPARQL endpoints [3, 12, 1,4]. Outside the Semantic Web community, approaches like GraphQL1 are gain-ing traction among developers due to their flexibility to query and retrieve datafrom public endpoints. However, generating APIs based on ontology artefactshas received less attention so far. These artefacts are any intermediate or finalresources generated during the ontology development process (e.g. competency",
        "publication_date": "2022-01-01",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": "20250511235420.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/EKAW2022_espinoza/PDFs/20250511235420.pdf",
        "pdf_link": "https://dgarijo.com/papers/EKAW2022_espinoza.pdf",
        "file_html": "html/20250511235420.html",
        "organization": "Springer",
        "url": "https://dgarijo.com/papers/EKAW2022_espinoza.pdf",
        "year": "2022",
        "pages": "19--35",
        "booktitle": "Proceedings of the 23rd International Conference Knowledge on Engineering and Knowledge Management (EKAW 2022)",
        "author": "Espinoza-Arias, Paola and Garijo, Daniel and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "espinoza2022extending"
    },
    {
        "title": "Good practice versus reality: A landscape analysis of Research Software metadata adoption in European Open Science Clusters",
        "implementation_urls": [],
        "abstract": "Abstract—Research Software has become a key asset to sup-port the results described in academic publications, enablingeffective data analysis and reproducibility. In order to ensureadherence of Research Software to the Findable, Accessible,Interoperable, and Reusable (FAIR) principles, the scientificcommunity has proposed metadata guidelines and best practices.However, it is unclear how these practices have been adopted sofar. This paper examines how different scientific communitiesdescribe Research Software with metadata to support FAIR,how do they adopt existing good practices regarding citation,documentation or versioning, and what is the current adoptionof archival services for long-term preservation. We carry outour analysis in the software registries of five science clusters(in domains ranging from Physics to Environmental Sciences),together with a multi-domain collaborative software registry.Our results highlight the main gaps in metadata adoption inthe different communities, opening an opportunity for futurecontributions to aid researchers in adopting good FAIR and OpenScience practices.Index Terms—Research Software, Metadata, FAIR software,FAIR principles, Guidelines.I. INTRODUCTIONResearch Software, i.e., the code files, scripts, tools, orworkflows involved in or produced throughout the researchlifecycle [1] plays a pivotal role in modern scientific re-search, to support the research outputs described in scientificpublications. Understanding the role of Research Softwareis essential not only for supporting research efficiency butalso for ensuring reproducibility. In this regard, the scientificcommunity has promoted initiatives towards adopting OpenScience best practices, such as the Findable, Accessible,Interoperable and Reusable (FAIR) principles for data [2],which have also been extended for Research Software [3].The adoption of FAIR is expected to ease the reuse of dataand software while allowing automated systems to retrieve andinteroperate with (meta)data. In fact, compliance with FAIRis now part of the agenda of international organizations suchas the European Commission, through the European OpenScience Cloud initiative [4], or NASA [5].11https://science.nasa.gov/open-science/In order to ease the adoption of FAIR and Open Science,several efforts have developed guidelines, lessons, and goodpractices for researchers to improve the metadata available intheir code repositories (e.g., the Research Software MetaDataGuidelines (RSMD) [6], Software Carpentry2, Open SourceSecurity Foundation3, etc.). However, it is unclear how thesepractices are adopted by different scientific communities,making it challenging to assess their impact.In this paper, we address this issue by assessing the adoptionof Research Software metadata best practices (inspired by",
        "file_name": "20250511232510.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/El_Hounsri_MSR_2025_landscape_analysis_CR/PDFs/20250511232510.pdf",
        "pdf_link": "https://dgarijo.com/papers/El_Hounsri_MSR_2025_landscape_analysis_CR.pdf",
        "file_html": "html/20250511232510.html",
        "url": "https://dgarijo.com/papers/El_Hounsri_MSR_2025_landscape_analysis_CR.pdf",
        "series": "MSR '25",
        "publisher": "Association for Computing Machinery",
        "booktitle": "To appear in Proceedings of the Mining Software Repositories Conference, 2025)",
        "year": "2025",
        "author": "El Hounsri, Anas and Garijo, Daniel",
        "ENTRYTYPE": "inproceedings",
        "ID": "el_hounsri_25"
    },
    {
        "title": "Towards Automated Hypothesis Testing in Neuroscience",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-33752-0_18",
        "abstract": "Abstract. Scientific data generation in the world is continuous. However,scientific studies once published do not take advantage of new data. Inorder to leverage this incoming flow of data, we present Neuro-DISK,an end-to-end framework to continuously process neuroscience data andupdate the assessment of a given hypothesis as new data become available.Our scope is within the ENIGMA consortium, a large internationalcollaboration for neuro-imaging and genetics whose goal is to understandbrain structure and function. Neuro-DISK includes an ontology andframework to organize datasets, cohorts, researchers, tools, working groupsand organizations participating in multi-site studies, such as those ofENIGMA, and an automated discovery framework to continuously testhypotheses through the execution of scientific workflows. We illustratethe usefulness of our approach with an implemented example.Keywords: Hypothesis Evaluation, Scientific Workflow, Ontology, AutomatedDiscovery, Neuroscience1 IntroductionScientific discoveries are based on hypothesis testing and rigorous data analysis.Such analyses are often time consuming and include steps that are difficultto interpret from scientific publications, and therefore, hard to systemicallyreproduce. Often, the designed hypothesis is tested only once against the acquireddata sample and later archived. Interestingly, in empirical sciences such as thebiological sciences, it is not uncommon for a hypothesis to yield contradictoryresults when evaluated on different data samples. In our data-driven world, datathat may be potentially relevant for testing a hypothesis is being continuously? co-first author2 D. Garijo & S. Fakhraei et al.generated but is often not studied to its full potential for hypothesis re-evaluationin combination with other related data. The lack of an integrated system toconstantly monitor the hypothesis of interest and update the underlying analysiswhen new data become available, is one of the challenges for automatic hypothesisre-evaluation. Having a framework that can keep such hypotheses alive requiressystematically capturing the knowledge about the data and analytics involved inthe hypothesis testing, which is often heterogeneous and compartmentalized.In this paper, we propose a solution to address the above challenges in theneurosciences based on our previous work for Automated DIscovery of ScientificKnowledge (DISK) [1]. We have extended DISK to explore brain-aging relatedhypothesis and data by generalizing the ability for the system to connect toexternal knowledge bases, including projects available within the EnhancingNeuro Imaging Genetics through Meta-Analysis (ENIGMA)4 consortium [2], aneuroscience collaboration where projects span many contributors from differentinstitutions around the world. In our proposed solution we address challengesof data, analytics, and hypothesis complexity. The data shared through imaginginitiatives such as the ENIGMA consortium includes multiple levels of hetero-geneity, and are regularly expanding in volume. The analytics related to suchdata requires the use of dozens of interconnected tools, each of which may requiresubstantial domain knowledge. The underlying hypotheses may depend on arange of possible multi-modal technical, neurological, clinical, demographic, andgenetic data which could be collected across multiple datasets.2 Related WorkTwo closely related research areas in machine learning are online algorithms [3]",
        "publication_date": "2019-01-01",
        "authors": "Daniel Garijo, Shobeir Fakhraei, Varun Ratnakar, Qifan Yang, Hanna Endrias, Yibo Ma, Regina Wang, Michael M. Bornstein, Joanna K. Bright, Yolanda Gil, Neda Jahanshad",
        "file_name": "20250512002948.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/Enigma_Disk_2019/PDFs/20250512002948.pdf",
        "pdf_link": "https://dgarijo.com/papers/Enigma_Disk_2019.pdf",
        "file_html": "html/20250512002948.html",
        "editor": "Gadepally, Vijay and Mattson, Timothy and Stonebraker, Michael and Wang, Fusheng and Luo, Gang and Laing, Yanhui and Dubovitskaya, Alevtina",
        "url": "https://dgarijo.com/papers/Enigma_Disk_2019.pdf",
        "isbn": "978-3-030-33752-0",
        "pages": "249--257",
        "address": "Cham",
        "publisher": "Springer International Publishing",
        "booktitle": "Heterogeneous Data Management, Polystores, and Analytics for Healthcare",
        "year": "2019",
        "author": "Garijo, Daniel and Fakhraei, Shobeir and Ratnakar, Varun and Yang, Qifan and Endrias, Hanna and Ma, Yibo and Wang, Regina and Bornstein, Michael and Bright, Joanna and Gil, Yolanda and Jahanshad, Neda",
        "ENTRYTYPE": "inproceedings",
        "ID": "ref10.1007/978-3-030-33752-0_18"
    },
    {
        "title": "ENIGMA and global neuroscience: A decade of large-scale studies of the brain in health and disease across more than 40 countries",
        "implementation_urls": [],
        "doi": "10.1038/s41398-020-0705-1",
        "abstract": "AbstractThis review summarizes the last decade of work by the ENIGMA (Enhancing NeuroImaging Genetics through MetaAnalysis) Consortium, a global alliance of over 1400 scientists across 43 countries, studying the human brain in healthand disease. Building on large-scale genetic studies that discovered the first robustly replicated genetic loci associatedwith brain metrics, ENIGMA has diversified into over 50 working groups (WGs), pooling worldwide data and expertiseto answer fundamental questions in neuroscience, psychiatry, neurology, and genetics. Most ENIGMA WGs focus onspecific psychiatric and neurological conditions, other WGs study normal variation due to sex and gender differences,or development and aging; still other WGs develop methodological pipelines and tools to facilitate harmonizedanalyses of “big data” (i.e., genetic and epigenetic data, multimodal MRI, and electroencephalography data). Theseinternational efforts have yielded the largest neuroimaging studies to date in schizophrenia, bipolar disorder, majordepressive disorder, post-traumatic stress disorder, substance use disorders, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, autism spectrum disorders, epilepsy, and 22q11.2 deletion syndrome. More recentENIGMA WGs have formed to study anxiety disorders, suicidal thoughts and behavior, sleep and insomnia, eatingdisorders, irritability, brain injury, antisocial personality and conduct disorder, and dissociative identity disorder. Here,we summarize the first decade of ENIGMA’s activities and ongoing projects, and describe the successes and challengesencountered along the way. We highlight the advantages of collaborative large-scale coordinated data analyses fortesting reproducibility and robustness of findings, offering the opportunity to identify brain systems involved in clinicalsyndromes across diverse samples and associated genetic, environmental, demographic, cognitive, and psychosocialfactors.IntroductionThe ENIGMA (Enhancing NeuroImaging Geneticsthrough Meta Analysis) Consortium is a collaboration ofmore than 1400 scientists from 43 countries studying thehuman brain. ENIGMA started 10 years ago, in 2009, withthe initial aim of performing a large-scale neuroimaginggenetic study, and has since diversified into 50 workinggroups (WGs), pooling worldwide data, resources andexpertise to answer fundamental questions in neu-roscience, psychiatry, neurology, and genetics (Fig. 1shows a world map of participating sites, broken down byworking group). Thirty of the ENIGMA WGs focus onspecific psychiatric and neurologic conditions. Four studyQ1different aspects of development and aging. Others studykey transdiagnostic constructs, such as irritability, and theimportance of Q2evolutionarily interesting genomic regionsin shaping human brain structure and function. Central tothe success of these Q3WGs are the efforts of dedicatedmethods development groups within ENIGMA. There arecurrently 12 WGs that develop and disseminate multi-scale and ‘big data’ analysis pipelines to facilitate harmo-nized analyses using genetic and epigenetic data,multimodal (anatomical, diffusion, functional) magneticCorrespondence: Paul M. Thompson (pthomp@usc.edu)Full list of author information is available at the end of the article.© The Author(s) 2020OpenAccessThis article is licensedunder aCreativeCommonsAttribution 4.0 International License,whichpermits use, sharing, adaptation, distribution and reproductionin any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate ifchangesweremade. The images or other third partymaterial in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to thematerial. Ifmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtainpermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.12",
        "publication_date": "2020-03-20",
        "authors": "Paul M. Thompson, Neda Jahanshad, Christopher R. K. Ching, Lauren E. Salminen, Sophia I. Thomopoulos, Joanna K. Bright, Bernhard T. Baune, Sara Bertolín, Janita Bralten, Willem B. Bruin, Robin Bülow, Jian Chen, Yann Chye, Udo Dannlowski, Carolien G. F. de Kovel, Gary Donohoe, Lisa T. Eyler, Stephen V. Faraone, Pauline Favre, Courtney A. Filippi, Thomas Frodl, Daniel Garijo, Yolanda Gil, Hans J. Grabe, Katrina L. Grasby, Tomáš Hájek, Laura K. M. Han, Sean N. Hatton, Kevin Hilbert, Tiffany C. Ho, Laurena Holleran, Georg Homuth, Norbert Hosten, Josselin Houenou, Iliyan Ivanov, Tianye Jia, Sinéad Kelly, Marieke Klein, Jun Soo Kwon, Max A. Laansma, Jeanne Leerssen, Ulrike Lueken, Abraham Nunes, Joseph O' Neill, Nils Opel, Fabrizio Piras, Federica Piras, Merel C. Postema, Elena Pozzi, Natalia Shatokhina, Carles Soriano‐Mas, Gianfranco Spalletta, Daqiang Sun, Alexander Teumer, Amanda K. Tilot, Leonardo Tozzi, Celia van der Merwe, Eus J.W. Van Someren, Guido van Wingen, Henry Völzke, Esther Walton, Lei Wang, Anderson M. Winkler, Katharina Wittfeld, Margaret J. Wright, Je‐Yeon Yun, Guohao Zhang, Yanli Zhang‐James, Bhim M. Adhikari, Ingrid Agartz, Moji Aghajani, André Alemán, Robert R. Althoff, André Altmann, Ole A. Andreassen, David Baron, Brenda Bartnik‐Olson, Janna Marie Bas‐Hoogendam, Arielle Baskin–Sommers, Carrie E. Bearden, Laura A. Berner, Premika S.W. Boedhoe, Rachel M. Brouwer, Jan K. Buitelaar, Karen Caeyenberghs, Charlotte A. M. Cecil, Ronald A. Cohen, James H. Cole, Patricia Conrod, Stéphane A. De Brito, Sonja M. C. de Zwarte, Emily L. Dennis, Sylvane Desrivières, Danai Dima, Stefan Ehrlich, Carrie Esopenko, Graeme Fairchild, Simon E. Fisher, Jean‐Paul Fouché, Clyde Francks",
        "file_name": "20250512003347.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/enigma_global/PDFs/20250512003347.pdf",
        "pdf_link": "https://dgarijo.com/papers/enigma_global.pdf",
        "file_html": "html/20250512003347.html",
        "url": "https://dgarijo.com/papers/enigma_global.pdf",
        "issn": "2158-3188",
        "pages": "100",
        "number": "1",
        "volume": "10",
        "journal": "Translational Psychiatry",
        "day": "20",
        "month": "March",
        "year": "2020",
        "author": "Thompson, Paul M. and Jahanshad, Neda and Ching, Christopher R. K. and Salminen, Lauren E. and Thomopoulos, Sophia I. and Bright, Joanna and Baune, Bernhard T. and Bertol{\\'i}n, Sara and Bralten, Janita and Bruin, Willem B. and B{\\\"u}low, Robin and Chen, Jian and Chye, Yann and Dannlowski, Udo and de Kovel, Carolien G. F. and Donohoe, Gary and Eyler, Lisa T. and Faraone, Stephen V. and Favre, Pauline and Filippi, Courtney A. and Frodl, Thomas and Garijo, Daniel and Gil, Yolanda and Grabe, Hans J. and Grasby, Katrina L. and Hajek, Tomas and Han, Laura K. M. and Hatton, Sean N. and Hilbert, Kevin and Ho, Tiffany C. and Holleran, Laurena and Homuth, Georg and Hosten, Norbert and Houenou, Josselin and Ivanov, Iliyan and Jia, Tianye and Kelly, Sinead and Klein, Marieke and Kwon, Jun Soo and Laansma, Max A. and Leerssen, Jeanne and Lueken, Ulrike and Nunes, Abraham and Neill, Joseph O' and Opel, Nils and Piras, Fabrizio and Piras, Federica and Postema, Merel C. and Pozzi, Elena and Shatokhina, Natalia and Soriano-Mas, Carles and Spalletta, Gianfranco and Sun, Daqiang and Teumer, Alexander and Tilot, Amanda K. and Tozzi, Leonardo and van der Merwe, Celia and Van Someren, Eus J. W. and van Wingen, Guido A. and V{\\\"o}lzke, Henry and Walton, Esther and Wang, Lei and Winkler, Anderson M. and Wittfeld, Katharina and Wright, Margaret J. and Yun, Je-Yeon and Zhang, Guohao and Zhang-James, Yanli and Adhikari, Bhim M. and Agartz, Ingrid and Aghajani, Moji and Aleman, Andr{\\'e} and Althoff, Robert R. and Altmann, Andre and Andreassen, Ole A. and Baron, David A. and Bartnik-Olson, Brenda L. and Marie Bas-Hoogendam, Janna and Baskin-Sommers, Arielle R. and Bearden, Carrie E. and Berner, Laura A. and Boedhoe, Premika S. W. and Brouwer, Rachel M. and Buitelaar, Jan K. and Caeyenberghs, Karen and Cecil, Charlotte A. M. and Cohen, Ronald A. and Cole, James H. and Conrod, Patricia J. and De Brito, Stephane A. and de Zwarte, Sonja M. C. and Dennis, Emily L. and Desrivieres, Sylvane and Dima, Danai and Ehrlich, Stefan and Esopenko, Carrie and Fairchild, Graeme and Fisher, Simon E. and Fouche, Jean-Paul and Francks, Clyde and Frangou, Sophia and Franke, Barbara and Garavan, Hugh P. and Glahn, David C. and Groenewold, Nynke A. and Gurholt, Tiril P. and Gutman, Boris A. and Hahn, Tim and Harding, Ian H. and Hernaus, Dennis and Hibar, Derrek P. and Hillary, Frank G. and Hoogman, Martine and Hulshoff Pol, Hilleke E. and Jalbrzikowski, Maria and Karkashadze, George A. and Klapwijk, Eduard T. and Knickmeyer, Rebecca C. and Kochunov, Peter and Koerte, Inga K. and Kong, Xiang-Zhen and Liew, Sook-Lei and Lin, Alexander P. and Logue, Mark W. and Luders, Eileen and Macciardi, Fabio and Mackey, Scott and Mayer, Andrew R. and McDonald, Carrie R. and McMahon, Agnes B. and Medland, Sarah E. and Modinos, Gemma and Morey, Rajendra A. and Mueller, Sven C. and Mukherjee, Pratik and Namazova-Baranova, Leyla and Nir, Talia M. and Olsen, Alexander and Paschou, Peristera and Pine, Daniel S. and Pizzagalli, Fabrizio and Renter{\\'i}a, Miguel E. and Rohrer, Jonathan D. and S{\\\"a}mann, Philipp G. and Schmaal, Lianne and Schumann, Gunter and Shiroishi, Mark S. and Sisodiya, Sanjay M. and Smit, Dirk J. A. and S{\\o}nderby, Ida E. and Stein, Dan J. and Stein, Jason L. and Tahmasian, Masoud and Tate, David F. and Turner, Jessica A. and van den Heuvel, Odile A. and van der Wee, Nic J. A. and van der Werf, Ysbrand D. and van Erp, Theo G. M. and van Haren, Neeltje E. M. and van Rooij, Daan and van Velzen, Laura S. and Veer, Ilya M. and Veltman, Dick J. and Villalon-Reina, Julio E. and Walter, Henrik and Whelan, Christopher D. and Wilde, Elisabeth A. and Zarei, Mojtaba and Zelman, Vladimir and the ENIGMA Consortium",
        "ENTRYTYPE": "article",
        "ID": "Thompson2020"
    },
    {
        "title": "FAIR Principles for Research Software (FAIR4RS Principles)",
        "implementation_urls": [],
        "abstract": "AbstractResearch software is a fundamental and vital part of research worldwide, yet there remainsignificant challenges to software productivity, quality, reproducibility, and sustainability.Improving the practice of scholarship is a common goal of the open science, open sourcesoftware and FAIR (Findable, Accessible, Interoperable and Reusable) communities, butimproving the sharing of research software has not yet been a strong focus of the latter.To improve the FAIRness of research software, the FAIR for Research Software (FAIR4RS)Working Group has sought to understand how to apply the FAIR Guiding Principles for scientificdata management and stewardship to research software, bringing together existing and newcommunity efforts. Many of the FAIR Guiding Principles can be directly applied to researchsoftware by treating software and data as similar digital research objects. However, specificcharacteristics of software — such as its executability, composite nature, and continuousevolution and versioning — make it necessary to revise and extend the principles.This document presents the first version of the FAIR Principles for Research Software(FAIR4RS Principles). It is an outcome of the FAIR for Research Software Working Group(FAIR4RS WG).The FAIR for Research Software Working Group is jointly convened as an RDA Working Group,FORCE11 Working Group, and Research Software Alliance (ReSA) Task Force.Date Version Number Description Editor(s)9/6/2021 0.3 Draft for formalcommunity reviewNeil Chue Hong7/6/2021 0.2.1 Amended abstractand text of F1, F1.1,F1.2, F4 and R1 forreview by draftinggroupNeil Chue Hong1/6/2021 0.2 Second draft forreview by FAIR4RSSteering CommitteeNeil Chue Hong17/5/2021 0.1 First draft for reviewby FAIR4RS WGNeil Chue Hong,Michelle Barker2Table of ContentsIntroduction 4Aims 5Previous work 6Development of the FAIR4RS Principles 7FAIR Principles for Research Software 9Findable 9Accessible 11Interoperable 12Reusable 13Challenges to implementation 15The path to adoption 16Acknowledgements 17",
        "file_name": "20250511235536.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/FAIR4RS_2022/PDFs/20250511235536.pdf",
        "pdf_link": "https://dgarijo.com/papers/FAIR4RS_2022.pdf",
        "file_html": "html/20250511235536.html",
        "language": "English",
        "url": "https://dgarijo.com/papers/FAIR4RS_2022.pdf",
        "doi": "10.15497/RDA00065",
        "day": "16",
        "month": "March",
        "year": "2022",
        "author": "Hong, {Neil P. Chue} and Katz, {Daniel S.} and Michelle Barker and Anna-Lena Lamprecht and Carlos Martinez and Psomopoulos, {Fotis E.} and Jen Harrow and Castro, {Leyla Jael} and Morane Gruenpeter and Martinez, {Paula Andrea} and Tom Honeyman and Alexander Struck and Allen Lee and Axel Loewe and Werkhoven, {Ben van} and Daniel Garijo and Esther Plomp and Francoise Genova and Hugh Shanahan and Maggie Hellstr{\\\"o}m and Malin Sandstr{\\\"o}m and Manodeep Sinha and Mateusz Kuzak and Patricia Herterich and Sharif Islam and Susanna-Assunta Sansone and Tom Pollard and Atmojo, {Udayanto Dwi} and Alan Williams and Andreas Czerniak and Anna Niehues and Fouilloux, {Anne Claire} and Bala Desinghu and Carole Goble and C{\\'e}line Richard and Charles Gray and Chris Erdmann and Daniel N{\\\"u}st and Daniele Tartarini and Elena Ranguelova and Hartwig Anzt and Ilian Todorov and James McNally and Jessica Burnett and Juli{\\'a}n Garrido-S{\\'a}nchez and Khalid Belhajjame and Laurents Sesink and Lorraine Hwang and Tovani-Palone, {Marcos Roberto} and Wilkinson, {Mark D.} and Mathieu Servillat and Matthias Liffers and Merc Fox and Nadica Miljkovi{\\'c} and Nick Lynch and Lavanchy, {Paula Martinez} and Sandra Gesing and Sarah Stevens and Cuesta, {Sergio Martinez} and Silvio Peroni and Stian Soiland-Reyes and Tom Bakker and Tovo Rabemanantsoa and Vanessa Sochat and Yo Yehudi and FAIR4RS WG",
        "ENTRYTYPE": "book",
        "ID": "751dfce356e5441f8e3f8c1401e0a1e0"
    },
    {
        "title": "FAIR Computational Workflows",
        "implementation_urls": [],
        "doi": "10.1162/dint_a_00033",
        "abstract": "ABSTRACTComputational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the FAIR data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that FAIR principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.†\t Corresponding author: Carole Goble (E-mail: carole.goble@manchester.ac.uk, ORCID: 0000-0003-1219-2137).© 2019 Chinese Academy of Sciences Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) licensehttp://dx.doi.org/10.1162/dint_a_00051http://dx.doi.org/10.1162/dint_a_00030https://w3id.org/fair/principles/terms/F1https://w3id.org/fair/principles/terms/F2https://w3id.org/fair/principles/terms/F3https://w3id.org/fair/principles/terms/F4https://w3id.org/fair/principles/terms/A1https://w3id.org/fair/principles/terms/A1.1https://w3id.org/fair/principles/terms/A1.2https://w3id.org/fair/principles/terms/A2https://w3id.org/fair/principles/terms/I1https://w3id.org/fair/principles/terms/I2https://w3id.org/fair/principles/terms/I3https://w3id.org/fair/principles/terms/R1https://w3id.org/fair/principles/terms/R1.1https://w3id.org/fair/principles/terms/R1.2https://w3id.org/fair/principles/terms/R1.3http://crossmark.crossref.org/dialog/?doi=10.1162/dint_a_00033&domain=pdf&date_stamp=2020-01-31Data Intelligence\t 109FAIR Computational Workflows1.  INTRODUCTIONIn data intensive science, e-infrastructures and software tool-chains are heavily used to help scientists manage, analyze, and share increasing volumes of complex data [1]. Data processing tasks like data cleansing, normalisation and knowledge extraction need to be automated stepwise in order to foster performance, standardisation and re-usability. Increasingly complex data computations and parameter-driven simulations need reliable e-infrastructures and consistent reporting to enable systematic comparisons of alternative setups [2, 3]. As a response to these needs, the practice of performing computational processes using workflows has taken hold in different domains such as the life sciences [4, 5, 6], biodiversity [7], astronomy [8], geosciences [9], and social sciences [10]. Workflows also support the adoption of novel computational approaches, notably machine learning methods [11], due to the ease with which single components in a processing pipeline can be exchanged or updated.Generally speaking, a workflow is a precise description of a procedure – a multi-step process to coordinate multiple tasks and their data dependencies. In computational workflows each task represents the execution of a computational process, such as: running a code, the invocation of a service, the calling of a command line tool, access to a database, submission of a job to a compute cloud, or the execution of data processing script or workflow. Figure 1 gives an example of a real workflow for variant detection in genomics, represented using the Common Workflow Language open standard [12].Computational workflows promise support for automation that scale across computational infrastructures and large datasets while shielding users from underlying execution complexities such as inter-resource ",
        "publication_date": "2019-11-01",
        "authors": "Carole Goble, Sarah Cohen‐Boulakia, Stian Soiland‐Reyes, Daniel Garijo, Yolanda Gil, Michael R. Crusoe, Kristian Peters, Daniel Schober",
        "file_name": "20250512003018.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/fair_wfs2019/PDFs/20250512003018.pdf",
        "pdf_link": "https://dgarijo.com/papers/fair_wfs2019.pdf",
        "file_html": "html/20250512003018.html",
        "eprint": "https://doi.org/10.1162/dint_a_00033",
        "url": "https://dgarijo.com/papers/fair_wfs2019.pdf",
        "pages": "108--121",
        "number": "1-2",
        "volume": "2",
        "journal": "Data Intelligence",
        "year": "2020",
        "author": "Goble, Carole and Cohen-Boulakia, Sarah and Soiland-Reyes, Stian and Garijo, Daniel and Gil, Yolanda and Crusoe, Michael R. and Peters, Kristian and Schober, Daniel",
        "ENTRYTYPE": "article",
        "ID": "goble_2020"
    },
    {
        "title": "Farm Explorer: A Tool for Calculating Transparent Greenhouse Gas Emissions",
        "implementation_urls": [],
        "abstract": "AbstractThis demo provides an overview of Farm Explorer , a tool designed to calculate greenhouse emissionson farms in a transparent manner. Farm Explorer uses semantic descriptions of emission calculationformulas by leveraging knowledge graphs containing static and dynamic information about the farmoperation and emission conversion factors. To enhance the transparency of emissions calculations, thetool records the provenance of the calculation process using standards like W3C PROV and W3C SOSA.Demo: https://w3id.org/tec-toolkit/ISWC-2024-demoSource: https://github.com/eats-project/farm-explorerKeywordsprovenance, carbon footprint, transparency, knowledge graph1. IntroductionThe agrifood systems, responsible for about a third of global anthropogenic greenhouse gasemissions [1], heavily rely on commercial carbon calculator tools [2]. Such tools aggregateresults calculated using various emission methodologies, data sources (e.g., GFLI for feeds [3])and carbon standards (e.g., GHG Protocol [4]) applied to specific aspects of agrifood activities(e.g., primary production on farms). Businesses provide data inputs that often need to belaboriously extracted from heterogeneous sources (e.g., sensors, manual records, machinerylogs, etc.). For example, to estimate emissions on a farm, a calculator may consider the electricityrequired to operate heavy machinery, use of fertilisers, amount of manure produced, andother logistics. Here, a typical emission calculation would estimate the amount of emissions-Posters, Demos, and Industry Tracks at ISWC 2024, November 13–15, 2024, Baltimore, USA∗Corresponding author.Envelope-Open milan.markovic@abdn.ac.uk (M. Markovic); stefano.germano@cs.ox.ac.uk (S. Germano); daniel.garijo@upm.es(D. Garijo); p.edwards@abdn.ac.uk (P. Edwards); a.li.21@abdn.ac.uk (A. Li); tewodrosalemu.ayall@abdn.ac.uk(T. A. Ayall); rachael.ramsey@sac.co.uk (R. Ramsey); georgios.leontidis@abdn.ac.uk (G. Leontidis)Orcid 0000-0002-5477-287X (M. Markovic); 0000-0001-6993-0618 (S. Germano); 0000-0003-0454-7145 (D. Garijo);0000-0002-0877-7063 (P. Edwards); 0009-0004-1103-8300 (A. Li); 0000-0001-9413-6459 (T. A. Ayall);0000-0001-9984-7241 (R. Ramsey); 0000-0001-6671-5568 (G. Leontidis)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).https://w3id.org/tec-toolkit/ISWC-2024-demohttps://github.com/eats-project/farm-explorermailto:milan.markovic@abdn.ac.ukmailto:stefano.germano@cs.ox.ac.ukmailto:daniel.garijo@upm.esmailto:p.edwards@abdn.ac.ukmailto:a.li.21@abdn.ac.ukmailto:tewodrosalemu.ayall@abdn.ac.ukmailto:rachael.ramsey@sac.co.ukmailto:georgios.leontidis@abdn.ac.ukhttps://orcid.org/0000-0002-5477-287Xhttps://orcid.org/0000-0001-6993-0618https://orcid.org/0000-0003-0454-7145https://orcid.org/0000-0002-0877-7063https://orcid.org/0009-0004-1103-8300https://orcid.org/0000-0001-9413-6459https://orcid.org/0000-0001-9984-7241https://orcid.org/0000-0001-6671-5568https://creativecommons.org/licenses/by/4.0generating resources (i.e., activity data) and multiply them by their corresponding emissionconversion factor. We argue that semantic web technologies are ideally positioned to provide",
        "file_name": "20250511232552.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/farm_explorer/PDFs/20250511232552.pdf",
        "pdf_link": "https://dgarijo.com/papers/farm_explorer.pdf",
        "file_html": "html/20250511232552.html",
        "year": "2024",
        "url": "https://dgarijo.com/papers/farm_explorer.pdf",
        "booktitle": "To appear in The 23rd International Semantic web Conference: ISWC 2024 Posters and Demos",
        "author": "Markovic, Milan and Germano, Stefano and Garijo, Daniel and Edwards, Pete and Li, Andy and Ayall, Tewodros Alemu and Ramsey, Rachael and Leontidis, Georgios",
        "ENTRYTYPE": "inproceedings",
        "ID": "markovic2024farm"
    },
    {
        "title": "Common motifs in scientific workflows: An empirical analysis",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2013.09.018",
        "abstract": "• We define a catalog of domain independent abstractions for workflows.• We discuss the distribution of the abstractions across different workflow systems.• Different workflow systems share a common core of workflow abstractions.• Data preparation is an obstacle for workflow understandability.a r t i c l e i n f oArticle history:Received 1 February 2013Received in revised form2 August 2013Accepted 5 September 2013Available online xxxxKeywords:Scientific workflowsWorkflow motifWorkflow patternTavernaWingsGalaxyVistrailsa b s t r a c tWorkflow technology continues to play an important role as a means for specifying and enactingcomputational experiments inmodern science. Reusing and re-purposingworkflows allow scientists to donew experiments faster, since the workflows capture useful expertise from others. As workflow librariesgrow, scientists face the challenge of finding workflows appropriate for their task, understanding whateachworkflow does, and reusing relevant portions of a givenworkflow.We believe that workflowswouldbe easier to understand and reuse if high-level views (abstractions) of their activities were available inworkflow libraries. As a first step towards obtaining these abstractions, we report in this paper on theresults of a manual analysis performed over a set of real-world scientific workflows from Taverna, Wings,Galaxy and Vistrails. Our analysis has resulted in a set of scientific workflowmotifs that outline (i) the kindsof data-intensive activities that are observed in workflows (Data-Operation motifs), and (ii) the differentmanners in which activities are implemented within workflows (Workflow-Oriented motifs). These motifsare helpful to identify the functionality of the steps in a given workflow, to develop best practices forworkflow design, and to develop approaches for automated generation of workflow abstractions.© 2013 Elsevier B.V. All rights reserved.1. IntroductionA scientific workflow is a template defining the set of tasksneeded to carry out a computational experiment [1]. Scientificworkflows have been increasingly used in the last decade as an in-strument for data intensive science. Workflows serve a dual func-tion: first, as detailed documentation of the scientific method usedfor an experiment (i.e. the input sources and processing steps takenfor the derivation of a certain data item), and second, as re-usable,∗ Corresponding author. Tel.: +34 667949892.E-mail addresses: dgarijo@fi.upm.es (D. Garijo), alperp@cs.manchester.ac.uk(P. Alper), khalidb@cs.manchester.ac.uk (K. Belhajjame), ocorcho@fi.upm.es(O. Corcho), gil@isi.edu (Y. Gil), carole.goble@cs.manchester.ac.uk (C. Goble).1 The first and second authors have contributed equally to the work presented inthis paper.executable artifacts for data-intensive analysis. Scientific work-flows are composed of a variety of data manipulation activities",
        "publication_date": "1988-01-01",
        "authors": "P. Érdi",
        "file_name": "20250512000935.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/fgcs-paper-motifs/PDFs/20250512000935.pdf",
        "pdf_link": "https://dgarijo.com/papers/fgcs-paper-motifs.pdf",
        "file_html": "html/20250512000935.html",
        "funding": "USAFOSR FA9550-11-1-0104",
        "url": "https://dgarijo.com/papers/fgcs-paper-motifs.pdf",
        "pages": "338--351",
        "volume": "36",
        "publisher": "North-Holland",
        "journal": "Future Generation Computer Systems",
        "year": "2014",
        "author": "Garijo, Daniel and Alper, Pinar and Belhajjame, Khalid and Corcho, Oscar and Gil, Yolanda and Goble, Carole",
        "ENTRYTYPE": "article",
        "ID": "garijo2014common"
    },
    {
        "title": "Abstract, link, publish, exploit: An end to end framework for workflow sharing",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/WingsProvenanceExport",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/fgcs2017/PDFs/20250512001547.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The public endpoint is available online,20 along with a set of sample queries to retrieve basic data from workflows and demonstrate its main functionality.21 The last step of methodology, the exploitation phase, is described in Section 5."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.future.2017.01.008",
        "publication_date": "1988-01-01",
        "authors": "P. Érdi",
        "file_name": "20250512001547.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/fgcs2017/PDFs/20250512001547.pdf",
        "pdf_link": "https://dgarijo.com/papers/fgcs2017.pdf",
        "file_html": "html/20250512001547.html",
        "funding": "DARPA  W911NF-15-1-0555, USNSF ICER-1440323",
        "url": "https://dgarijo.com/papers/fgcs2017.pdf",
        "publisher": "North-Holland",
        "journal": "Future Generation Computer Systems",
        "year": "2017",
        "author": "Garijo, Daniel and Gil, Yolanda and Corcho, Oscar",
        "ENTRYTYPE": "article",
        "ID": "garijo2017abstract"
    },
    {
        "title": "Ontology Engineering and the FAIR principles: A Gap Analysis toward a FAIR-by-design methodology",
        "implementation_urls": [],
        "abstract": "AbstractOntologies and vocabularies play a key role when standardising, organizing and integrating data fromheterogeneous data sources into Knowledge Graphs. In order to develop ontologies, different engineeringmethodologies have been proposed throughout the years, whose application resulted in thousands ofsemantic artefacts (taxonomies, vocabularies and ontologies) in a wide range of domains. But how toensure that ontologies follow the Findable, Accessible, Interoperable and Reusable principles (FAIR) fromtheir inception? In this paper, we review: (i) existing guidelines to help make ontologies FAIR and (ii)published FAIRness assessment methodologies and map them to the ontology development lifecycleactivities. Our analysis outlines the current gaps, where no guidelines exist for ontologies to becomeFAIR-by-design.KeywordsOntology Engineering, FAIR principles, FAIRness assessment, FAIR-by-design, Semantic Artefacts,Ontologies, Vocabularies1. IntroductionOntologies and vocabularies play a key role in data integration by defining the structure, guidingthe construction, and validating Knowledge Graphs. Ontologies are widely used in multipledomains, ranging from Biomedicine [1] and Astrophysics [2] to Smart Cities [3] or Web contentannotation [4].Proceedings of the Joint Ontology Workshops (JOWO) - Episode X: The Tukker Zomer of Ontology, and satellite eventsco-located with the 14th International Conference on Formal Ontology in Information Systems (FOIS 2024), July 15-19,2024, Enschede, The Netherlands.*Corresponding author.†These authors contributed equally.$ m.poveda@upm.es (M. Poveda-Villalón); daniel.garijo@upm.es (D. Garijo);alejandra.gonzalez-beltran@stfc.ac.uk (A. N. Gonzalez-Beltran); clement.jonquet@inrae.fr (C. Jonquet);ylefranc@esciencefactory.com (Y. L. Franc)� https://agbeltran.github.io/ (A. N. Gonzalez-Beltran)� 0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0003-0454-7145 (D. Garijo); 0000-0003-3499-8262(A. N. Gonzalez-Beltran); 0000-0002-2404-1582 (C. Jonquet); 0000-0003-4631-418X (Y. L. Franc)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingsceur-ws.orgISSN 1613-0073mailto:m.poveda@upm.esmailto:daniel.garijo@upm.esmailto:alejandra.gonzalez-beltran@stfc.ac.ukmailto:clement.jonquet@inrae.frmailto:ylefranc@esciencefactory.comhttps://agbeltran.github.io/https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0003-0454-7145https://orcid.org/0000-0003-3499-8262https://orcid.org/0000-0002-2404-1582https://orcid.org/0000-0003-4631-418Xhttps://creativecommons.org/licenses/by/4.0A number of ontology engineering methodologies have been proposed by researchers throughthe years in order to build ontologies [5, 6, 7, 8, 9, 10, 11, 12] and finally [13] that we, in part,",
        "publication_date": "2024-07-15",
        "authors": "María Poveda‐Villalón, Daniel Garijo, Alejandra González-Beltrán, Clément Jonquet, Yann Le Franc",
        "file_name": "20250511232523.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/foam-4/PDFs/20250511232523.pdf",
        "pdf_link": "https://ceur-ws.org/Vol-3882/foam-4.pdf",
        "file_html": "html/20250511232523.html",
        "url": "https://ceur-ws.org/Vol-3882/foam-4.pdf",
        "volume": "3882",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "FAIR principles for Ontologies and Metadata in Knowledge Management (FOAM 2024)",
        "year": "2024",
        "author": "Poveda-Villalon, Mar{\\'\\i}a and Garijo, Daniel and Gonzalez-Beltran, Alejandra and Jonquet, Clement and Le Franc, Yann",
        "ENTRYTYPE": "inproceedings",
        "ID": "poveda2024ontology"
    },
    {
        "title": "Fragflow automated fragment detection in scientific workflows",
        "implementation_urls": [],
        "doi": "10.1109/eScience.2014.32",
        "abstract": "Abstract—Scientific workflows provide the means to define, execute and reproduce computational experiments.  However, reusing existing workflows still poses challenges for workflow designers. Workflows are often too large and too specific to reuse in their entirety, so reuse is more likely to happen for fragments of workflows. These fragments may be identified manually by users as sub-workflows, or detected automatically.  In this paper we present the FragFlow approach, which detects workflow fragments automatically by analyzing existing workflow corpora with graph mining algorithms.  FragFlow detects the most common workflow fragments, links them to the original workflows and visualizes them. We evaluate our approach by comparing FragFlow results against user-defined sub-workflows from three different corpora of the LONI Pipeline system.  Based on this evaluation, we discuss how automated workflow fragment detection could facilitate workflow reuse. Keywords—scientific workflow; workflow fragment; workflow reuse; LONI pipeline.  I.  INTRODUCTION  Scientific workflows are templates that define the set of steps and data dependencies needed to execute computational experiments. They are usually created for executing research methods and sharing them with a community of users [21] [8], even in cases where the community is small (e.g., a research lab). Scientific workflows can ease data management among different steps [9] [15], facilitate collaborative development through a common interface and simplify access to computational resources (e.g., clusters, grids, etc.).  Different scientific workflows often share part of their functionality (common preprocessing steps, data manipulation for a particular visualization, reformatting, etc). A prior study analyzing workflows from different workflow systems and domains showed that almost 20% percent of the analyzed workflows were composed of other workflows [6]. Some systems even allow users to define hierarchical workflows to exploit these commonalities and split workflows into smaller reusable parts [15] [21]. However, there is not much support for the automatic detection of sub-workflows at the moment. In our work, we aim to automatically discover and expose common workflow fragments from a given corpus of workflows. Capturing common workflow fragments may bring in several benefits to workflows users and designers: they simplify the visualization of the workflow (simpler visualizations lead to better organization and comprehension), they make the workflow easier to understand, they modularize the functionality (by separating the different reusable parts of the workflow) and they save time in workflow design (designers are more likely to reuse fragments rather than complete workflows, which are more specific). Our approach combines exact and inexact graph mining ",
        "publication_date": "2014-10-01",
        "authors": "Daniel Garijo, Óscar Corcho, Yolanda Gil, Boris A. Gutman, Ivo D. Dinov, Paul M. Thompson, Arthur W. Toga",
        "file_name": "20250512001219.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/fragflow-escience14/PDFs/20250512001219.pdf",
        "pdf_link": "https://dgarijo.com/papers/fragflow-escience14.pdf",
        "file_html": "html/20250512001219.html",
        "funding": "USNSF IIS-1344272, NIH R01AG040060",
        "organization": "IEEE",
        "url": "https://dgarijo.com/papers/fragflow-escience14.pdf",
        "pages": "281--289",
        "volume": "1",
        "booktitle": "e-Science (e-Science), 2014 IEEE 10th International Conference on",
        "year": "2014",
        "author": "Garijo, Daniel and Corcho, Oscar and Gil, Yolanda and Gutman, Boris A and Dinov, Ivo D and Thompson, Paul and Toga, Arthur W",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2014fragflow"
    },
    {
        "title": "LDP4ROs: Managing Research Objects with the W3C Linked Data Platform",
        "implementation_urls": [],
        "abstract": "ABSTRACTIn this demo we present LDP4ROs, a prototype implemen-tation that allows creating, browsing and updating ResearchObjects (ROs) and their contents using typical HTTP oper-ations. This is achieved by aligning the RO model with theW3C Linked Data Platform (LDP).KeywordsLinked Data Platform, Research Object, LDP, RO1. INTRODUCTIONIt is widely recognised that raw PDF files are usually insuf-ficient to allow researchers to check, reuse and reproduce themethods exposed in scientific publications1. In this context,the Research Object model (RO) [1] has been proposed as apossible means to aggregate, link and describe the resources(e.g., code, scripts, datasets and references to them) associ-ated to scientific publications, what may help (although notnecessarily solve completely) these tasks.Different applications have been developed to create, man-age and access ROs2 [4]. However, they rely on custom ser-vices and APIs, making it difficult to access, edit or createsome of their resources and contents using standard HTTPoperations, affecting interoperability among them.In this demo we present LDP4ROs3, a prototype imple-mentation of our alternative approach to create, access andupdate ROs and their associated resources, which considersthem as Web objects. For this purpose we align the ROmodel to the W3C Linked Data Platform (LDP) [5]. Withour approach, one can create, edit or access resources asLinked Data using standard HTTP operations (e.g., GETplus content negotiation) instead of using SPARQL or cus-tom REST APIs.1https://www.force11.org/beyondthepdf22http://www.researchobject.org/specifications/3https://github.com/oeg-upm/LDP4ROPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee.SAVE-SD 2015 WWW, Florence, ItalyCopyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.Section 2 describes the alignment between LDP and theRO model, Section 3 describes our working prototype andSection 4 discusses future work.2. ALIGNING LDP WITH THE RO MODELThis section describes the mapping between ROs to LDPconcepts. First we describe the main features of these mod-els (Sections 2.1 and 2.2) and then we illustrate the resultsof the mapping with a simple example (Section 2.3).",
        "file_name": "20250512001315.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/garijo-savesd2015/PDFs/20250512001315.pdf",
        "pdf_link": "https://dgarijo.com/papers/garijo-savesd2015.pdf",
        "file_html": "html/20250512001315.html",
        "organization": "ACM",
        "url": "https://dgarijo.com/papers/garijo-savesd2015.pdf",
        "doi": "10.1145/2740908.2742016",
        "pages": "1057--1058",
        "booktitle": "Proceedings of the 24th International Conference on World Wide Web",
        "year": "2015",
        "author": "Garijo, Daniel and Mihindukulasooriya, Nandana and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2015ldp4ros"
    },
    {
        "title": "The DISK Hypothesis Ontology: Capturing Hypothesis Evolution for Automated Discovery",
        "implementation_urls": [],
        "abstract": "ABSTRACT Automated discovery systems can formulate and revise hypotheses by gathering and analyzing data. In order to generate new hypotheses and provide explanations of their new findings, these systems need a language to represent hypotheses, their revisions, and their provenance. This paper describes the DISK hypothesis ontology which fulfills these requirements.  The paper then presents a survey of existing models for representing hypotheses along with their features and tradeoffs. We compare these hypothesis models in the context of automated discovery and hypothesis evolution.  CCS CONCEPTS • Information systems → Artificial intelligence; Knowledge representation and reasoning KEYWORDS Hypothesis representation, hypothesis evolution, nanopublications, micropublications, automated discovery, ontologies. 1 INTRODUCTION Formal representations of scientific hypotheses would be useful in many contexts. For instance, in order to keep up with the latest updates on a research area, scientists need to quickly understand the contributions of an article and how it was derived from others. However, the vast amount of new scientific publications makes this task increasingly complex. If scientists represented hypotheses formally in publications, related literature could be easily searched for hypotheses of interest. Alternatively, machine reading systems could also extract hypotheses from text in articles, and generate these formal representations.  Formal representations of hypotheses may also be used to improve reproducibility. Community initiatives on reproducibility promote registering hypotheses and methods before conducting the research [Munafo et al 2017]. Hypotheses are stated in textual form, which can express arbitrarily complex statements about hypotheses. However, text can be imprecise and ambiguous. Creating machine readable representations of research hypotheses would facilitate the organization and management of the literature. To date there is not a standard way of capturing the contents and context of a hypothesis to understand its evolution.  Another important use of formal hypothesis representations is to enable automated discovery systems to do hypothesis testing and revision. Autonomous discovery systems generate hypotheses autonomously based on analysis of relevant data [Pankratius et al 2016; King 2017; Gil et al 2017].  In this paper, we focus on hypothesis representations to capture hypothesis evolution in automated discovery systems. We discuss the requirements that we have found throughout work on the DISK discovery system [Gil et al 2017]. We propose an ontology for hypothesis representation, and compare it to existing models for representing hypotheses.  ",
        "publication_date": "2017-01-01",
        "authors": "Daniel Garijo, Yolanda Gil, Varun Ratnakar",
        "file_name": "20250512000729.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/garijo_sciknow17/PDFs/20250512000729.pdf",
        "pdf_link": "https://dgarijo.com/papers/garijo_sciknow17.pdf",
        "file_html": "html/20250512000729.html",
        "funding": "DARPA W911NF-15-1-0555, NIH 1R01GM117097",
        "url": "https://dgarijo.com/papers/garijo_sciknow17.pdf",
        "address": "Austin, Texas",
        "booktitle": "Proceedings of the Workshop on Capturing Scientific Knowledge (SciKnow), held in conjunction with the ACM International Conference on Knowledge Capture (K-CAP)",
        "year": "2017",
        "author": "Daniel Garijo and Yolanda Gil and Varun Ratnakar",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo-etal-sciknow2017"
    },
    {
        "title": "The genetic architecture of the human cerebral cortex",
        "implementation_urls": [],
        "doi": "10.1126/science.aay6690",
        "abstract": "considering variables collected from the age of seven onwards (and potentially abstracted from obstetric notes) there are data available for more than the 14,541 pregnancies mentioned above. The number of new pregnancies not in the initial sample (known as Phase I enrolment) that are currently represented on the built files and reflecting enrolment status at the age of 18 is 706 (452 and 254 recruited during Phases II and III respectively), resulting in an additional 713 children being enrolled. The phases of enrolment are described in more detail in the cohort profile paper (see footnote 4 below). The total sample size for analyses using any data collected after the age of seven is therefore 15,247 pregnancies, resulting in 15,458 fetuses. Of this total sample of 15,458 fetuses, 14,775 were live births and 14,701 were alive at 1 year of age. A 10% sample of the ALSPAC cohort, known as the Children in Focus (CiF) group, attended clinics at the University of Bristol at various time intervals between 4 to 61 months of age. The CiF group were chosen at random from the last 6 months of ALSPAC births (1432 families attended at least one clinic). Excluded were those mothers who had moved out of the area or were lost to follow-up, and those partaking in another study of infant development in Avon. The data used in the present study were collected from 391 males and further description of this subset and the variables used in this study are provided in Supplementary Tables 2-4. The study website contains details of all the data that is available through a fully searchable data dictionary (http://www.bris.ac.uk/alspac/researchers/data-access/data-dictionary/). Further information can be found in the following papers: Boyd A, Golding J, Macleod J, Lawlor DA, Fraser A, Henderson J, Molloy L, Ness A, Ring S, Davey Smith G. Cohort Profile: The ‘Children of the 90s’; the index offspring of The Avon Longitudinal Study of Parents and Children (ALSPAC). International Journal of Epidemiology 2013; 42: 111-127; Fraser A, Macdonald-Wallis C, Tilling K, Boyd A, Golding J, Davey Smith G, Henderson J, Macleod J, Molloy L, Ness A, Ring S, Nelson SM, Lawlor DA. Cohort Profile: The Avon Longitudinal Study of Parents and Children: ALSPAC mothers cohort. International Journal of Epidemiology 2013; 42:97-110;   References 87 McClard, C. K. et al. POU6f1 Mediates Neuropeptide-Dependent Plasticity in the Adult Brain. The Journal of neuroscience : the official journal of the Society for Neuroscience 38, 1443-1461, doi:10.1523/jneurosci.1641-17.2017 (2018). 88 Dobson-Stone, C. et al. GSK3B and MAPT polymorphisms are associated with grey matter and intracranial volume in healthy individuals. PloS one 8, e71750, doi:10.1371/journal.pone.0071750 (2013). 89 Arbogast, T. et al. Mouse models of 17q21.31 microdeletion and microduplication syndromes highlight the importance of Kansl1 for cognition. PLoS genetics 13, e1006886, doi:10.1371/journal.pgen.1006886 (2017). .CC-BY-NC-ND 4.0 International licensenot peer-reviewed) is the author/funder. It is made available under aThe copyright holder for this preprint (which was. http://dx.doi.org/10.1101/399402doi: bioRxiv preprint first posted online Sep. 3, 2018; http://dx.doi.org/10.1101/399402http://creativecommons.org/licenses/by-nc-nd/4.0/34  90 Shi, C., Viccaro, K., Lee, H. G. & Shah, K. Cdk5-Foxo3 axis: initially neuroprotective, eventually neurodegenerative in Alzheimer's disease models. Journal of cell science 129, 1815-1830, doi:10.1242/jcs.185009 (2016). 91 Gilmore, E. C. & Walsh, C. A. Genetic causes of microcephaly and lessons for neuronal development. Wiley interdisciplinary reviews. Developmental biology 2, 461-478, doi:10.1002/wdev.89 (2013). 92 Pankratova, S. et al. The S100A4 Protein Signals through the ErbB4 Receptor to Promote Neuronal Survival. Theranostics 8, 3977-3990, doi:10.7150/thno.22274 (2018). 93 Lee, H. F., Chi, C. S., Tsai, C. R., Chen, H. C. & Lee, I. C. Prenatal brain disruption in isolated sulfite oxidase deficiency. Orphanet journal of rare diseases 12, 115, doi:10.1186/s13023-017-0668-3 ",
        "publication_date": "2020-03-19",
        "authors": "Katrina L. Grasby, Neda Jahanshad, Jodie N. Painter, Lucía Colodro‐Conde, Janita Bralten, Derrek P. Hibar, Penelope A. Lind, Fabrizio Pizzagalli, Christopher R. K. Ching, Mary McMahon, Natalia Shatokhina, Leo Zsembik, Sophia I. Thomopoulos, Alyssa H. Zhu, Lachlan T. Strike, Ingrid Agartz, Saud Alhusaini, Marcio Almeida, Dag Alnæs, Inge K. Amlien, Micael Andersson, Tyler Ard, Nicola J. Armstrong, Allison E. Ashley‐Koch, Joshua Atkins, Manon Bernard, Rachel M. Brouwer, Elizabeth E.L. Buimer, Robin Bülow, Christian Bürger, Dara M. Cannon, M. Mallar Chakravarty, Qiang Chen, J. Cheung, Baptiste Couvy‐Duchesne, Anders M. Dale, Shareefa Dalvie, Tânia Kawasaki de Araujo, Greig I. de Zubicaray, Sonja M. C. de Zwarte, Anouk den Braber, Nhat Trung Doan, Katharina Dohm, Stefan Ehrlich, Hannah-Ruth Engelbrecht, Susanne Erk, Chun Fan, Iryna O. Fedko, Sonya Foley, Judith M. Ford, Masaki Fukunaga, Melanie E. Garrett﻿, Tian Ge, Sudheer Giddaluru, Aaron L. Goldman, Melissa J. Green, Nynke A. Groenewold, Dominik Grotegerd, Tiril P. Gurholt, Boris A. Gutman, Narelle K. Hansell, Mathew A. Harris, Marc Harrison, Courtney C. Haswell, Michael A. Hauser, Stefan Herms, Dirk J. Heslenfeld, New Fei Ho, David Hoehn, Per Hoffmann, Laurena Holleran, Martine Hoogman, Jouke‐Jan Hottenga, Masashi Ikeda, Deborah Janowitz, Iris E. Jansen, Tianye Jia, Christiane Jockwitz, Ryota Kanai, Sherif Karama, Dalia Kasperavičiūtė, Tobias Kaufmann, Sinéad Kelly, Masataka Kikuchi, Marieke Klein, Michael Knapp, Annchen R. Knodt, Bernd Krämer, Max Lam, T. Lancaster, Phil H. Lee, Tristram A. Lett, Lindsay B. Lewis, Íscia Lopes‐Cendes, Michelle Luciano, Fabìo Macciardi, André F. Marquand, Samuel R. Mathias, Tracy R. Melzer, Yuri Milaneschi",
        "file_name": "20250512003137.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/genetic_architecture_of_the_human_cerebral_cor/PDFs/20250512003137.pdf",
        "pdf_link": "https://dgarijo.com/papers/genetic_architecture_of_the_human_cerebral_cor.pdf",
        "file_html": "html/20250512003137.html",
        "url": "https://dgarijo.com/papers/genetic_architecture_of_the_human_cerebral_cor.pdf",
        "number": "6484",
        "volume": "367",
        "publisher": "American Association for the Advancement of Science",
        "journal": "Science",
        "year": "2020",
        "author": "Grasby, Katrina L and Jahanshad, Neda and Painter, Jodie N and Colodro-Conde, Luc{\\'\\i}a and Bralten, Janita and Hibar, Derrek P and Lind, Penelope A and Pizzagalli, Fabrizio and Ching, Christopher RK and McMahon, Mary Agnes B and others",
        "ENTRYTYPE": "article",
        "ID": "grasby2020genetic"
    },
    {
        "title": "Challenges for provenance analytics over geospatial data",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-16462-5_28",
        "abstract": "Abstract. The growing availability of geospatial data online, theincreased use of crowdsourced maps and the advent of geospatial mash-ups have led to systems that deliver data to users after integration frommany sources. In such systems, understanding the provenance of geospa-tial data is crucial for assessing the quality of the data and decidingon whether to rely on the data for decision making. To be able to useand analyze provenance in geospatial integration systems in a princi-pled manner, we identify different levels of provenance in the geospatialdomain, provide a set of provenance questions from the point of viewof end users, and relate our geospatial provenance model to the W3CPROV recommendation.1 IntroductionThe Open Geospatial Consortium and the World Wide Web Consortium areworking jointly towards standards for linking and integrating geospatial data [1].As geospatial data is often used in decision making (e.g., navigation), the accu-racy of integrated data is important. While we specifically cover provenancefor geospatial information, some of these challenges are present in many otherdomains as well. The area of geospatial data integration is a prime scenariofor provenance management, as the involved data and systems are complex andexhibit many challenging characteristics:– External sources: when integrating two geospatial datasets, an algorithm mightconsult other sources.– Human-in-the-loop processes: in some cases, the integration might involvemanual intervention, to check particular values by seeking additional confir-mation or even perhaps with eyes on target.– Crowdsourcing: datasets may have been collected from many small contribu-tions, which should attacj provenance too.c© Springer International Publishing Switzerland 2015B. Ludäscher and B. Plale (Eds.): IPAW 2014, LNCS 8628, pp. 261–263, 2015.DOI: 10.1007/978-3-319-16462-5 28262 D. Garijo et al.– Granularity: geospatial information may be represented at different levels ofgranularity in space; a geographical feature can be a point in space (e.g., aroad intersection), a one-dimensional segment (e.g., a bridge that connectstwo points) or a two-dimensional region (e.g., a parking lot).– Computation: spatial reasoning may be needed to compute relationshipsbetween features; the integration system may have to integrate computedrelations from different sources.– Versioning: maps are updated as the original data sources are updated. Theobjects in a map themselves can have multiple revisions.We present an initial study on the requirements and challenges of trackinggeospatial provenance, based on discussions with researchers and practitionersat several meetings and workshops on geospatial data.2 Geospatial Provenance ModelBefore we explain how to apply the W3C PROV standard model [2] to thegeospatial domain, we present a classification of provenance levels on geospatialdata:– Dataset-level provenance: provenance assertions about a map as a single entity.The map contains objects, and these objects contain properties and values,but provenance is associated with the map as a whole.",
        "publication_date": "2015-01-01",
        "authors": "Daniel Garijo, Yolanda Gil, Andreas Harth",
        "file_name": "20250512001302.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/geo/PDFs/20250512001302.pdf",
        "pdf_link": "https://dgarijo.com/papers/geo.pdf",
        "file_html": "html/20250512001302.html",
        "organization": "Springer, Cham",
        "url": "https://dgarijo.com/papers/geo.pdf",
        "pages": "261--263",
        "booktitle": "International Provenance and Annotation Workshop",
        "year": "2014",
        "author": "Garijo, Daniel and Gil, Yolanda and Harth, Andreas",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2014challenges"
    },
    {
        "title": "OntoSoft: Capturing scientific software metadata",
        "implementation_urls": [],
        "abstract": "ABSTRACT This paper presents OntoSoft, an ontology to describe metadata for scientific software. The ontology is designed considering how scientists would approach the reuse and sharing of software.  This includes supporting a scientist to: 1) identify software, 2) understand and assess software, 3) execute software, 4) get support for the software, 5) do research with the software, and 6) update the software.  The ontology is available in OWL and contains more than fifty terms. We are using OntoSoft to structure a software registry for geosciences, and to develop user interfaces to capture its metadata. Author Keywords Ontologies, software reuse, knowledge capture. ACM Classification Keywords H.5.2 [Information Interfaces and Presentation]: User Interfaces - Interaction styles.  INTRODUCTION Scientific software captures important knowledge and should be shared and reused.  Although there are many popular code repositories used by scientists there is still a significant amount of software that is never shared.  The reasons for not sharing scientific software include the desire to not expose less-than-ideal code, lack of incentives and credit for software, and interest in software commercialization among others [Howison and Herbsleb 2011].  While the loss of “dark data” in science is well recognized [Heidorn 2008], there is an analogous problem in the pervasive loss of “dark software”. Our interest is in supporting software sharing across the geosciences. With the exception of big model packages typically shared in modeling frameworks, geosciences software is rarely shared.  In addition, it is scattered in different sites and not easy to find or reuse. Our goal is to improve software sharing by developing a software registry framework that includes metadata useful for discovery and reuse among scientists who are not software developers.   This paper introduces the OntoSoft software registry and its ontology for describing scientific software metadata. The ontology contains basic metadata properties to describe how to identify software, understand what it does and its utility for research, execute it, get support if questions arise, do research with it, and contribute to its development.  These are all topics of interest to scientists, and the ontology revolves around those categories as a way to frame the requests for metadata in a practical light to incentivize scientists to provide it.   RELATED WORK The Core Software Ontology (CSO) and the Core Ontology of Software Components (COSC) and [Oberle et al 2006] ",
        "publication_date": "2015-12-01",
        "authors": "Youn-Hee Gil",
        "file_name": "20250512001328.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/gil-etal-kcap2015/PDFs/20250512001328.pdf",
        "pdf_link": "https://dgarijo.com/papers/gil-etal-kcap2015.pdf",
        "file_html": "html/20250512001328.html",
        "funding": "USNSF ICER-1440323",
        "organization": "ACM",
        "url": "https://dgarijo.com/papers/gil-etal-kcap2015.pdf",
        "doi": "10.1145/2815833.2816955",
        "pages": "32",
        "booktitle": "Proceedings of the 8th International Conference on Knowledge Capture",
        "year": "2015",
        "author": "Gil, Yolanda and Ratnakar, Varun and Garijo, Daniel",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2015ontosoft"
    },
    {
        "title": "Towards Capturing Scientific Reasoning to Automate Data Analysis",
        "implementation_urls": [],
        "abstract": "Abstract This paper describes an initial cognitive framework that captures the reasoning involved in scientific data analyses, drawing from close collaborations with scientists in different domains over many years.  The framework aims to automate data analysis for science.  In doing so, existing large repositories of data could be continuously and systematically analyzed by machines, updating findings and potentially making new discoveries as new data becomes available.  The framework consists of a cycle with six phases: formulating an investigation, initiating the investigation, getting data, analyzing data, aggregating results, and integrating findings.  The paper also describes our implementation of this framework and illustrates it with examples from different science domains.  Keywords: scientific discovery, automated data analysis, cognitive scientists, AI scientists Introduction Over the last decades, many scholars have shed light on the diverse and rich processes involved in scientific reasoning, from discovering laws [Simon 1977], to understanding causal mechanisms [Craver and Darden 2013; Pearl 2018], to collaboration [Trickett et al 2015], to producing paradigm shifts [Kuhn 1962].  The development of cognitive models that reflect how scientists think is indeed a daunting task.  Our goals are much narrower, focusing very specifically on capturing the scientific reasoning that we observed through many years of working with scientists in data analysis as we represented their tasks, implemented their computational methods, and supported their collaborative work.   Our focus is on scientific research that revolves around data analysis, in particular observational science where data is abundant.  There are many other types of scientific research that are not directly linked to the analysis of data (though eventually they can be).  Some are designed to gain some understanding on how to tackle an open problem, perhaps by assembling information about the state-of-the-art in relevant publications or by coming up with new ways to frame a problem that can lead to new research avenues.  Other investigations are designed to be exploratory in nature in terms of trying out possible directions through informed guesses to gather more information about the problem.  These eventually lead to data analysis which is the current focus of our work. There is prior work on developing frameworks for scientific data analysis.  Others have focused on automating the extraction of findings from the literature [Tshitoyan et al 2019], the exploration of complex search spaces [Senior et al 2020], the formulation of hypotheses [Callahan et al 2011], or the design of laboratory experiments [Groth and Cox ",
        "file_name": "20250511235509.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/gil_et_al_cogsci_2022/PDFs/20250511235509.pdf",
        "pdf_link": "https://dgarijo.com/papers/gil_et_al_cogsci_2022.pdf",
        "file_html": "html/20250511235509.html",
        "url": "https://dgarijo.com/papers/gil_et_al_cogsci_2022.pdf",
        "year": "2022",
        "booktitle": "Proceedings of the Annual Meeting of the Cognitive Science Society",
        "author": "Gil, Yolanda and Khider, Deborah and Osorio, Maximiliano and Ratnakar, Varun and Vargas, Hernan and Garijo, Daniel and Pierce, Suzanne",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2022towards"
    },
    {
        "title": "Improving Publication and Reproducibility of Computational Experiments through Workflow Abstractions",
        "implementation_urls": [],
        "abstract": "Experiments through Workflow Abstractions  Yolanda Gil Information Sciences Institute University of Southern California USA gil@isi.edu Daniel Garijo Information Sciences Institute University of Southern California USA dgarijo@isi.edu Margaret Knoblock Information Sciences Institute University of Southern California USA mrk022@bucknell.edu Alyssa Deng Information Sciences Institute University of Southern California USA shipingd@andrew.cmu.edu  Ravali Adusumilli School of Medicine Stanford University USA ravali@stanford.edu Varun Ratnakar Information Sciences Institute University of Southern California USA varunr@isi.edu    Parag Mallick School of Medicine Stanford University USA paragm@stanford.edu     ABSTRACT The current practice of publishing articles solely containing textual descriptions of methods is error prone and incomplete. Even when a reproducible workflow or notebook is linked to an article, the text of the article is not well integrated with those computational components, and the workflow and notebook are focused mostly on implementation details that are disconnected ",
        "publication_date": "2017-01-01",
        "authors": "Yolanda Gil, Daniel Garijo, Margaret Knoblock, Alyssa Deng, Ravali Adusumilli, Varun Ratnakar, Parag Mallick",
        "file_name": "20250512000810.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/gil_sciknow17/PDFs/20250512000810.pdf",
        "pdf_link": "https://dgarijo.com/papers/gil_sciknow17.pdf",
        "file_html": "html/20250512000810.html",
        "funding": "DARPA W911NF-15-1-0555, NIH 1U01CA196387",
        "url": "https://dgarijo.com/papers/gil_sciknow17.pdf",
        "address": "Austin, Texas",
        "booktitle": "Proceedings of the Workshop on Capturing Scientific Knowledge (SciKnow), held in conjunction with the ACM International Conference on Knowledge Capture (K-CAP)",
        "year": "2017",
        "author": "Yolanda Gil and Daniel Garijo and Margaret Knoblock and Alyssa Deng and Ravali Adusumilli and Varun Ratnakar and Parag Mallick",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil-etal-sciknow2017"
    },
    {
        "title": "MINT: Model Integration Through Knowledge-Powered Data and Process Composition",
        "implementation_urls": [],
        "abstract": "Abstract: Major societal and environmental challenges require forecasting how natural processes and human activities affect one another. Model integration across natural and social science disciplines to study these problems requires resolving semantic, spatio-temporal, and execution mismatches, which are largely done by hand today and may take more than two years of human effort.  We are developing the Model INTegration (MINT) framework that incorporates extensive knowledge about models and data, with several innovative components: 1) New principle-based ontology generation tools for modeling variables, used to describe models and data; 2) A novel workflow system that selects relevant models from a curated registry and uses abductive reasoning to hypothesize new models and data transformation steps; 3) A new data discovery and integration framework that finds and categorizes new sources of data, learns to extract information from both online sources and remote sensing data, and transforms the data into the format required by the models; 4) New knowledge-guided machine learning algorithms for model parameterization to improve accuracy and estimate uncertainty; 5) A novel framework for multi-modal scalable workflow execution.  We are beginning to annotate models and datasets using standard ontologies, and to compose and execute workflows of models that span climate, hydrology, agriculture, and economics. We are building on many previously existing tools, including CSDMS, BMI, GSN, WINGS, Pegasus, Karma, and GOPHER.  Rapid model integration would enable efficient and comprehensive coupled human and natural system modeling.  Keywords: Model integration, semantic workflows, ontologies, reasoning, automated planning, machine learning, model metadata, data catalogs, model catalogs.     1 INTRODUCTION  Major societal and environmental challenges require forecasting how natural processes and human activities affect one another. This requires integrating highly heterogeneous models from separate disciplines, including geosciences, agriculture, economics, and social sciences. Model integration requires resolving semantic, spatio-temporal, and execution mismatches, which are largely done by hand today. It is also challenging to locate appropriate models and to find data at the resolution needed for each scenario and region. In addition, models are often designed or calibrated to approximate the real phenomena under study, which can lead to poor performance. Composing models to enable end-to-end simulations and executing them with large-scale data requires coordinating many requirements. Y. Gil et al. / MINT: Model Integration Through Knowledge-Rich Data and Process Composition  Unfortunately, model integration is extremely time consuming and integrating two or three models from different disciplines can take months or years. There are tools such as repositories to find models (Peckham et al 2013), software for regridding data to address mismatches of time and space scales, data representation standards, and model coupling for execution interleaving (BMI 2018).  However, these tools address only slices of the process and are not well integrated, so model integration is largely done by hand. (Laniak et al 2013) call for the “development of standards for publishing data and models in forms suitable for automated discovery, access, and integration.”  The paper describes initial work to develop an end-to-end approach that uses artificial intelligence to assist modelers by automating and improving important aspects of model integration. Building on our extensive prior work in artificial intelligence and modeling, we are developing the MINT (Model INTegration) framework that incorporates three key innovations: 1) Semantic technologies to address model and data discovery and to bridge the heterogeneity of model requirements and assumptions; 2) Automated planning to resolve data mismatches by including data transformations; and 3) Machine learning to improve model parameterization through the extraction of more accurate data from remote sensing and other sources and search optimization.  ",
        "publication_date": "2018-01-01",
        "authors": "Yolanda Gil, Kelly M. Cobourn, Ewa Deelman, Christopher Duffy, Rafael Ferreira da Silva, Armen R. Kemanian, Craig A. Knoblock, Vipin Kumar, S. D. Peckham, Lucas Carvalho, Yao-Yi Chang, Daniel Garijo, Ankush Khandelwal, Deborah Khider, Minh Pahm, Jay Pujara, Varun Ratnakar, Maria Stoica, Bình Dương Vũ",
        "file_name": "20250512000646.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/iemss2018-MINT/PDFs/20250512000646.pdf",
        "pdf_link": "https://dgarijo.com/papers/iemss2018-MINT.pdf",
        "file_html": "html/20250512000646.html",
        "url": "https://dgarijo.com/papers/iemss2018-MINT.pdf",
        "booktitle": "Proceedings of the 9th International Congress on Environmental Modelling and Software",
        "year": "2018",
        "author": "Yolanda  Gil and Kelly Cobourn and Ewa Deelman and Chris Duffy and Rafael Ferreira da Silva and Armen Kemanian and Craig Knoblock and Vipin Kumar and Scott Peckham and Lucas Carvalho and Yao-Yi Chiang and Daniel Garijo and Deborah Khider and Ankush Khandelwal and Minh Pahm and Jay Pujara and Varun Ratnakar and Maria Stoica and Binh Vu",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2018mint"
    },
    {
        "title": "A Semantic Model Catalog to Support Comparison and Reuse",
        "implementation_urls": [],
        "abstract": "Abstract: Model repositories are key resources for scientists in terms of model discovery and reuse, but do not focus on important tasks such as model comparison and composition. Model repositories do not typically capture important comparative metadata to describe assumptions and model variables that enable a scientist to discern which models would be better for their purposes. Furthermore, once a scientist selects a model from a repository it takes significant effort to understand and use the model.  Our goal is to develop model repositories with machine-actionable model metadata that can be used to provide intelligent assistance to scientists in model selection and reuse. We are extending the OntoSoft semantic software metadata registry (http://www.ontosoft.org/) to include machine-readable metadata. This work includes: 1) exposing model variables and their relationships; 2) exposing model processes and how they group and relate to model variables; 3) adopting a standardized representation of model variables based on the conventions of the Geoscience Standard Names ontology (GSN) (http://www.geoscienceontology.org/); 4) capturing the semantic structure of model invocation signatures based on functional inputs and outputs and their correspondence to model variables; 5) associating models with readily reusable workflow fragments for data preparation, model calibration, and visualization of results. The extended OntoSoft framework will reduce the time to find, understand, compare, and reuse models.    Keywords: Model metadata, scientific software, model catalogs, model repositories     1 INTRODUCTION  Models developed by scientists contain important scientific knowledge that should be explicitly captured and disseminated to facilitate model reusability, comparison and composition. Scientists recognize the value of sharing these models to avoid replicating effort and to inspect and reproduce results from other models.  A key issue for reusing scientific models is their dissemination and documentation. Model repositories already exist and are used by many scientists (e.g., CSDMS [Peckham et al. (2013)]; CSDMS (2018)], ESMF [ESMF (2018)], HydroShare [Hydroshare (2017)]). However, they lack important information such as model variables or model processes, which are used by scientist to discern whether the model is appropriate for their analyses or not. Furthermore, once a model (or set of models) is selected, it takes significant effort to understand how to set up a model and how to interpret its results. The OntoSoft software metadata registry [Gil et al (2015); Gil et al (2016), OntoSoft (2018)] mailto:dgarijo@isi.edumailto:khider@usc.edumailto:gil@isi.edumailto:lucas.carvalho@ic.unicamp.brmailto:bte2rn@virginia.edumailto:spierce@tacc.utexas.edumailto:danielhardestylewis@utexas.edumailto:varunr@isi.edumailto:scott.peckham@colorado.edumailto:cxd11@psu.edumailto:goodall@virginia.eduhttp://www.ontosoft.org/)http://www.geoscienceontology.org/)D. Garijo et al. / OntoSoft: A Semantic Model Registry to Support Comparison and Reuse was developed to capture extensive information that is needed by scientists to understand how models work.  Most of that information is available, but scattered in publications, manuals, code documentation, ",
        "publication_date": "2018-01-01",
        "authors": "Daniel Garijo, Deborah Khider, Yolanda Gil, Lucas Carvalho, Bakinam T. Essawy, Suzanne A. Pierce, D. H. LEWIS, Varun Ratnakar, S. D. Peckham, Christopher Duffy, Jonathan L. Goodall",
        "file_name": "20250512000701.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/IEMSs2018-OntoSoft/PDFs/20250512000701.pdf",
        "pdf_link": "https://dgarijo.com/papers/IEMSs2018-OntoSoft.pdf",
        "file_html": "html/20250512000701.html",
        "url": "https://dgarijo.com/papers/IEMSs2018-OntoSoft.pdf",
        "booktitle": "Proceedings of the 9th International Congress on Environmental Modelling and Software",
        "year": "2018",
        "author": "Daniel Garijo and Deborah Khider and Yolanda Gil and Lucas Carvalho and Bakinam Essawy and Suzanne Pierce and Daniel Hardesty Lewis and Varun Ratnakar and Scott Peckham and Chris Duffy and Jonathan Goodal",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2018ontosoft"
    },
    {
        "title": "Inspect4py: {A} Knowledge Extraction Framework for Python Code Repositories",
        "implementation_urls": [],
        "abstract": "ABSTRACTThis work presents inspect4py, a static code analysis frameworkdesigned to automatically extract the main features, metadata anddocumentation of Python code repositories. Given an input folderwith code, inspect4py uses abstract syntax trees and state of theart tools to find all functions, classes, tests, documentation, callgraphs, module dependencies and control flows within all code filesin that repository. Using these findings, inspect4py infers differ-ent ways of invoking a software component. We have evaluatedour framework on 95 annotated repositories, obtaining promisingresults for software type classification (over 95% F1-score). Withinspect4py, we aim to ease the understandability and adoption ofsoftware repositories by other researchers and developers.Code: https://github.com/SoftwareUnderstanding/inspect4pyDOI: https://doi.org/10.5281/zenodo.5907936License: Open (BSD3-Clause)CCS CONCEPTS• General and reference → Surveys and overviews; • Appliedcomputing → Document capture; Document analysis.KEYWORDSCode mining, software code, software classification, software docu-mentation, code understandingACM Reference Format:Rosa Filgueira and Daniel Garijo. 2022. Inspect4py: A Knowledge ExtractionFramework for PythonCode Repositories. In 19th International Conference onMining Software Repositories (MSR ’22), May 23–24, 2022, Pittsburgh, PA, USA.ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3524842.35284971 INTRODUCTIONAn increasing number of research results depend on software, inareas ranging from High-Energy Physics [15] to ComputationalBiology [12]. Research software is used to clean and analyze data,simulate real systems or visualize scientific results [3].In the last years, research software has become a subject ofinterest for the scientific community for two main reasons. First,software itself has become a research topic, with multiple researchPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-9303-4/22/05. . . $15.00https://doi.org/10.1145/3524842.3528497challenges such as using efficient code representations [14] forfunction similarity [7] or generating documentation from code [4].Second, the importance of software for Science has promoted theworldwide adoption of the Findable, Accessible, Interoperable and",
        "file_name": "20250511235523.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/inspect4py_MSR2022/PDFs/20250511235523.pdf",
        "pdf_link": "https://dgarijo.com/papers/inspect4py_MSR2022.pdf",
        "file_html": "html/20250511235523.html",
        "doi": "10.1145/3524842.3528497",
        "url": "https://dgarijo.com/papers/inspect4py_MSR2022.pdf",
        "year": "2022",
        "publisher": "{IEEE}",
        "pages": "232--236",
        "booktitle": "{IEEE/ACM} 19th International Conference on Mining Software Repositories,\n{MSR} 2022, Pittsburgh, PA, USA, May 23-24, 2022",
        "author": "Rosa Filgueira and\nDaniel Garijo",
        "ENTRYTYPE": "inproceedings",
        "ID": "FilgueiraG22"
    },
    {
        "title": "Semantic Modelling of Plans and Execution Traces for Enhancing Transparency of IoT Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TrustLens/EP-PLAN",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/IoT_Systems_AAM2019/PDFs/20250512002738.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://www.w3.org/TR/2013/REC-prov-o-20130430/[4] D."
                    }
                ]
            }
        ],
        "doi": "10.1109/IOTSMS48152.2019.8939260",
        "abstract": "Abstract—Transparency of IoT systems is an essential require-ment for enhancing user’s trust towards such systems. Prove-nance mechanisms documenting the execution of IoT systemsare often cited as an enabler of such transparency. However,provenance records often lack detailed descriptions of a system’sexpected behaviour. Plan specifications describe the steps neededto achieve a certain goal by a human or an automated system.Once plans reach a certain level of complexity, they are typicallydecomposed in different levels of abstraction. However, thisdecomposition makes it difficult to relate high level abstract plansto their granular execution traces. This paper introduces EP-Plan, a vocabulary for linking the different levels of granularityof a plan with their respective provenance traces. EP-Plan alsoprovides the means to describe plan metadata such as constraints,policies, rationales, and expected participating agents associatedwith a plan.I. INTRODUCTIONAn increasing number of systems provide means to docu-ment provenance records of past executions [1] (i.e., executiontraces), in order to inspect and explain the creation process ofexisting results or behaviour of a system. In this paper, weargue that recording such provenance can be further enhancedby recording the set of planned steps that guided its execution,and we refer to such records as plans.Plans document intended system behaviour, which is ben-eficial at the point when no runtime provenance is available(e.g. to assess risks associated with a planned IoT deployment).Plans are also critical to understand errors, enabling a pointof reference for comparison when the execution deviates fromwhat was planned to happen.Gateway CloudPlanRelay data from sensorsPlanProcess and store dataPlanDeliver smart temperature monitoring at Alice’s homeSensorPlanProduce observationsIoT SystemExecution TraceA log of activities generating and publishing raw data values and timestampsExecution TraceA log of activities receiving and uploading sensor data  Execution Trace",
        "publication_date": "2019-10-01",
        "authors": "Milan Marković, Daniel Garijo, Peter Edwards, Wamberto Vasconcelos",
        "file_name": "20250512002738.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/IoT_Systems_AAM2019/PDFs/20250512002738.pdf",
        "pdf_link": "https://dgarijo.com/papers/IoT_Systems_AAM2019.pdf",
        "file_html": "html/20250512002738.html",
        "keywords": "",
        "url": "https://dgarijo.com/papers/IoT_Systems_AAM2019.pdf",
        "pages": "110--115",
        "number": "",
        "volume": "",
        "booktitle": "2019 Sixth International Conference on Internet of Things: Systems, Management and Security (IOTSMS)",
        "month": "October",
        "year": "2019",
        "author": "M. {Markovic} and D. {Garijo} and P. {Edwards} and W. {Vasconcelos}",
        "ENTRYTYPE": "inproceedings",
        "ID": "markovic-et-al2019"
    },
    {
        "title": "Softalias-KG: Reconciling Software Mentions in Scientific Literature",
        "implementation_urls": [],
        "abstract": "AbstractResearch software (i.e., the tools and scripts developed as part of a research investigation) are key tosupport the results described in academic publications. However, current citation practices followed byresearchers make it difficult to automatically identify and reconcile the tools used in existing publications(different names are used for the same tool, different citing styles, etc.). In this demo we address thisissue by integrating SofCite, a state of the art named entity recognition model designed to find softwarementions in the biomedical domain, with Softalias-KG, a Knowledge Graph of software aliases, in orderto reconcile software tools in a text.Demo URL: https://w3id.org/softalias/demoCode: https://github.com/SoftwareUnderstanding/softalias-rsKeywordssoftware alias, software metadata, software reconciliation, knowledge graph1. IntroductionResearch Software [1] is increasingly becoming a first-class citizen research product due to itsrole in supporting computational results.1 Open platforms like Papers with Code2 or OpenAire3are dedicated to tracking the links between research articles and code, and paper preprintarchives like Arxiv4 are starting to display such information to readers.In order to ease software citation, the scientific community has developed guidelines andsoftware citation formats for developers [2]. However, researchers often use different namesto refer to the tools they refer to in their work. For example, the “Statistical Package for theSocial Sciences”5 is also known as “SPSS” and “SPSS Statistics” among many other variations.This makes tool reconciliation challenging, making it difficult to provide tool developers theircorresponding credit.In this work introduce Softalias-KG, a Knowledge Graph of software aliases created from arecent analysis and software mention dataset of over 3.8 million papers from PubMed Central [3].ISWC 2023 Posters and Demos: 22nd International Semantic Web Conference, November 6–10, 2023, Athens, GreeceEnvelope-Open egonzalez@fi.upm.es (E. González-Guardia); daniel.garijo@upm.es (D. Garijo)Orcid 0000-0003-4112-6825 (E. González-Guardia); 0000-0003-0454-7145 (D. Garijo)© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org)1https://sfdora.org/read/2https://paperswithcode.com/3https://graph.openaire.eu/4https://arxiv.org/5https://www.ibm.com/products/spss-statisticshttps://w3id.org/softalias/demohttps://github.com/SoftwareUnderstanding/softalias-rsmailto:egonzalez@fi.upm.esmailto:daniel.garijo@upm.eshttps://orcid.org/0000-0003-4112-6825https://orcid.org/0000-0003-0454-7145https://creativecommons.org/licenses/by/4.0http://ceur-ws.orghttp://ceur-ws.orghttps://sfdora.org/read/https://paperswithcode.com/",
        "file_name": "20250511234809.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/ISWC2023_demo_softalias/PDFs/20250511234809.pdf",
        "pdf_link": "https://dgarijo.com/papers/ISWC2023_demo_softalias.pdf",
        "file_html": "html/20250511234809.html",
        "url": "https://dgarijo.com/papers/ISWC2023_demo_softalias.pdf",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "International Semantic Web Conference (ISWC) 2023: Posters, Demos, and Industry Tracks",
        "year": "2023",
        "author": "Gonzalez Guardia, Esteban and Lopez, Hector and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "softalias2023"
    },
    {
        "title": "Towards Automating Data Narratives",
        "implementation_urls": [
            {
                "identifier": "http://doi.org/10.5281/zenodo.234660",
                "type": "zenodo",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "DOI",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1145/3025171.3025193",
        "abstract": "ABSTRACT  We propose a new area of research on automating data narratives. Data narratives are containers of information about computationally generated research findings. They have three major components: 1) A record of events, that describe a new result through a workflow and/or provenance of all the computations executed; 2) Persistent entries for key entities involved for data, software versions, and workflows; 3) A set of narrative accounts that are automatically generated human-consumable renderings of the record and entities and can be included in a paper. Different narrative accounts can be used for different audiences with different content and details, based on the level of interest or expertise of the reader. Data narratives can make science more transparent and reproducible, because they ensure that the text description of the computational experiment reflects with high fidelity what was actually done. Data narratives can be incorporated in papers, either in the methods section or as supplementary materials. We introduce DANA, a prototype that illustrates how to generate data narratives automatically, and describe the information it uses from the computational records. We also present a formative evaluation of our approach and discuss potential uses of automated data narratives. Author Keywords Reproducibility; computational workflows; semantic workflows; WINGS; data narratives; explanation ACM Classification Keywords H.5.3. Information interfaces and presentation (e.g., HCI): Group and organization interfaces. INTRODUCTION The reproducibility of published scientific research has received significant attention [48]. For computational experiments, published papers often provide insufficient information about the data, protocols, software, and overall method used to obtain the new results [53]. A major challenge in reproducibility is the lack of appropriate support for authors to capture exactly how experiments were performed. Once the work is finished, authors write an account in their articles by retrospective reconstruction of the work that was actually done, relying on their memory and notes kept along the way. For research carried out in the field or laboratory, tracking accurately what was done can require discipline and effort. But why should reproducibility be challenging for computational experiments, when we have the infrastructure to capture accurately the computations that were carried out? We should have automated tools that ensure that the descriptions that are written about computational experiments are in fact accurate and provide enough detail for transparency and reproducibility. In this paper, we propose data narratives as a new approach to automatically generate text to describe computational analyses. ",
        "publication_date": "2017-03-07",
        "authors": "Yolanda Gil, Daniel Garijo",
        "file_name": "20250512001643.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/iui2017/PDFs/20250512001643.pdf",
        "pdf_link": "https://dgarijo.com/papers/iui2017.pdf",
        "file_html": "html/20250512001643.html",
        "funding": "DARPA W911NF-15-1-0555, NIH 1R01GM117097",
        "organization": "ACM",
        "url": "https://dgarijo.com/papers/iui2017.pdf",
        "pages": "565--576",
        "booktitle": "Proceedings of the 22nd International Conference on Intelligent User Interfaces",
        "year": "2017",
        "author": "Gil, Yolanda and Garijo, Daniel",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2017towards"
    },
    {
        "title": "Towards Human-guided Machine Learning",
        "implementation_urls": [],
        "doi": "10.1145/3301275.3302324",
        "abstract": "ABSTRACT Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user’s knowledge about the data available.  We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements. CCS CONCEPTS • Human-centered computing KEYWORDS Human-guided machine learning; Automated machine learning (AutoML); Task analysis; Scientific workflows. ACM Reference format: Yolanda Gil, James Honaker, Shikhar Gupta, Yibo Ma, Vito D’Orazio, Daniel Garijo, Shruti Gadewar, Qifan Yang and Neda Jahanshad. 2019. Towards Human-Guided Machine Learning. In 24th International Confer- ence on Intelligent User Interfaces (IUI ’19), March 17–20, 2019, Marina del Rey, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3301275.3302324 1 Introduction In recent years, Automated Machine Learning (AutoML) approaches are making great strides to automatically search for machine learning solutions from a large space of possible kinds of models.  Typically, a solution is created by choosing a model (e.g., random forest, SVM, etc.) and then configuring those models by assigning (hyper)parameter values [1]–[4]. A series of challenges and workshops have led to steadfast improvements [5]. Commercial products are now becoming available that automate machine learning, notably for image classification and select natural language processing tasks [6].  While fully automated model learning is appropriate for many applications, there are many contexts where full automation is not desirable or possible.  This is the case when users have knowledge that supplements the available data, particularly in a scientific research context. An AutoML system would look at a set of instances or images the same whether they are about tumor tissues or ad placements.  However, biologists would bring to bear extensive knowledge about human disease in the development of a machine learning model. Without this knowledge, machine learning ",
        "publication_date": "2019-02-19",
        "authors": "Yolanda Gil, James Honaker, Shikhar Gupta, Yibo Ma, Vito D’Orazio, Daniel Garijo, Shruti P. Gadewar, Qifan Yang, Neda Jahanshad",
        "file_name": "20250512002706.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/iui2019/PDFs/20250512002706.pdf",
        "pdf_link": "https://dgarijo.com/papers/iui2019.pdf",
        "file_html": "html/20250512002706.html",
        "keywords": "automated machine learning (AutoML), human-guided machine learning, scientific workflows, task analysis",
        "acmid": "3302324",
        "numpages": "11",
        "url": "https://dgarijo.com/papers/iui2019.pdf",
        "isbn": "978-1-4503-6272-6",
        "pages": "614--624",
        "series": "IUI '19",
        "address": "New York, NY, USA",
        "publisher": "ACM",
        "location": "Marina del Ray, California",
        "booktitle": "Proceedings of the 24th International Conference on Intelligent User Interfaces",
        "year": "2019",
        "author": "Gil, Yolanda and Honaker, James and Gupta, Shikhar and Ma, Yibo and D'Orazio, Vito and Garijo, Daniel and Gadewar, Shruti and Yang, Qifan and Jahanshad, Neda",
        "ENTRYTYPE": "inproceedings",
        "ID": "Gil:2019:THM:3301275.3302324"
    },
    {
        "title": "An Intelligent Interface for Integrating Climate, Hydrology, Agriculture, and Socioeconomic Models",
        "implementation_urls": [],
        "doi": "10.1145/3308557.3308711",
        "abstract": "ABSTRACT Understanding the interactions between natural processes and human activities poses major challenges as it requires the integration of models and data across disparate disciplines.  It typically takes many months and even years to create valid end-to-end simulations as different models need to be configured in consistent ways and generate data that is usable by other models. MINT is a novel framework for model integration that captures extensive knowledge about models and data and aims to automatically compose them together. MINT guides a user to pose a well-formed modeling question, select and configure appropriate models, find and prepare appropriate datasets, compose data and models into end-to-end workflows, run the simulations, and visualize the results. MINT currently includes hydrology, agriculture, and socioeconomic models.  Author Keywords Intelligent workflow systems; model integration; environmental modeling; scientific discovery.  CCS CONCEPTS H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous. ACM Reference format: Daniel Garijo, Deborah Khider, Varun Ratnakar, Yolanda Gil, Kelly Cobourn, Ewa Deelman, Chris Duffy, Rafael Ferreira da Silva, Armen Kemanian, Craig Knoblock, Vipin Kumar, Sco Peckham, Yao-Yi Chiang, Ankush Khandelwal, Minh Pham, Jay Pujara, Maria Stoica, Kshitij Tayal, Binh Vu, Dan Feldman, Lele Shu, Rajiv Mayani, Anna Dabrowski, Daniel Hardesty-Lewis, Suzanne Pierce. 2018. An Intelligent Interface for Integrating Climate, Hydrology, Agriculture, and Socioeconomic Models. In Proceedings of IUI '19 Companion, March 17–20, 2019, Marina del Ray, CA, USA  hps://doi.org/10.1145/3308557.3308711 1 Introduction Understanding how human activities affect natural resources, and how natural processes affect societies requires complex computational simulations that cut across disciplinary boundaries. For example, in order to understand how weather predictions and agriculture practices affect water availability or how flooding affects planting strategies and population migration, modelers integrate physics-based climate and hydrology models with biologically-informed agriculture models and market distribution economic models. While the questions are short fused in order to prepare for natural disasters or to make near-term policy decisions, integrating these diverse models may take many months or even years.  Major challenges include finding relevant models to address a question and datasets with the necessary granularity and quality to run the models, developing sophisticated data transformations to set up and execute a model, and checking for models compatibility. Existing infrastructure supports some aspects of the modeling ",
        "publication_date": "2019-02-28",
        "authors": "Daniel Garijo, Deborah Khider, Varun Ratnakar, Yolanda Gil, Ewa Deelman, Rafael Ferreira da Silva, Craig A. Knoblock, Yao‐Yi Chiang, Tam Minh Pham, Jay Pujara, Bình Dương Vũ, Dan Feldman, Rajiv Mayani, Kelly M. Cobourn, Christopher Duffy, Armen R. Kemanian, Lele Shu, Vipin Kumar, Ankush Khandelwal, Kshitij Tayal, S. D. Peckham, Maria Stoica, Anna Dabrowski, Daniel Hardesty-Lewis, Suzanne A. Pierce",
        "file_name": "20250512002653.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/iuiDemo2019/PDFs/20250512002653.pdf",
        "pdf_link": "https://dgarijo.com/papers/iuiDemo2019.pdf",
        "file_html": "html/20250512002653.html",
        "keywords": "environmental modeling, intelligent workflow systems, model integration, scientific discovery",
        "acmid": "3308711",
        "numpages": "2",
        "url": "https://dgarijo.com/papers/iuiDemo2019.pdf",
        "isbn": "978-1-4503-6673-1",
        "pages": "111--112",
        "series": "IUI '19",
        "address": "New York, NY, USA",
        "publisher": "ACM",
        "location": "Marina del Ray, California",
        "booktitle": "Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion",
        "year": "2019",
        "author": "Garijo, Daniel and Khider, Deborah and Ratnakar, Varun and Gil, Yolanda and Deelman, Ewa and da Silva, Rafael Ferreira and Knoblock, Craig and Chiang, Yao-Yi and Pham, Minh and Pujara, Jay and Vu, Binh and Feldman, Dan and Mayani, Rajiv and Cobourn, Kelly and Duffy, Chris and Kemanian, Armen and Shu, Lele and Kumar, Vipin and Khandelwal, Ankush and Tayal, Kshitij and Peckham, Scott and Stoica, Maria and Dabrowski, Anna and Hardesty-Lewis, Daniel and Pierce, Suzanne",
        "ENTRYTYPE": "inproceedings",
        "ID": "Garijo:2019:III:3308557.3308711"
    },
    {
        "title": "Automating ontology engineering support activities with OnToology",
        "implementation_urls": [
            {
                "identifier": "https://github.com/OnToology/OnToology",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/JWSsupporting/PDFs/20250512002902.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "OnToology is available as a web service, and its code has been released online [17]."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2018.09.003",
        "abstract": "Due to the increasing uptake of semantic technologies, ontologies are now part of a good number of information systems. As a result, software development teams that have to combine ontology engineering activities with software development practices are facing several challenges, since these two areas have evolved, in general, separately. In this paper we present OnToology, an approach to manage ontology engineering support activities (i.e., documentation, evaluation, releasing and versioning). OnToology is a web-based application that builds on top of Git-based environments and integrates existing semantic web technologies. We have validated OnToology against a set of representative requirements for ontology development support activities in distributed environments, and report on a survey of the system to assess its usefulness and usability.",
        "publication_date": "2018-10-09",
        "authors": "Ahmad Alobaid, Daniel Garijo, María Poveda‐Villalón, Idafen Santana-Pérez, Alba Fernández-Izquierdo, Óscar Corcho",
        "file_name": "20250512002902.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/JWSsupporting/PDFs/20250512002902.pdf",
        "pdf_link": "https://dgarijo.com/papers/JWSsupporting.pdf",
        "file_html": "html/20250512002902.html",
        "keywords": "Ontology engineering, Ontology evaluation, Ontology documentation, Ontology publication",
        "url": "https://dgarijo.com/papers/JWSsupporting.pdf",
        "issn": "1570-8268",
        "pages": "100472",
        "volume": "57",
        "journal": "Journal of Web Semantics",
        "year": "2019",
        "author": "Ahmad Alobaid and Daniel Garijo and María Poveda-Villalón and Idafen Santana-Perez and Alba Fernández-Izquierdo and Oscar Corcho",
        "ENTRYTYPE": "article",
        "ID": "ALOBAID2019100472"
    },
    {
        "title": "Crossing the chasm between ontology engineering and application development: A survey",
        "implementation_urls": [],
        "abstract": "The adoption of Knowledge Graphs (KGs) by public and private organizations to integrate and publish data has increased in recent years. Ontologies play a crucial role in providing the structure for KGs, but are usually disregarded when designing Application Programming Interfaces (APIs) to enable browsing KGs in a developer-friendly manner. In this paper we provide a systematic review of the state of the art on existing approaches to ease access to ontology-based KG data by application developers. We propose two comparison frameworks to understand specifications, technologies and tools responsible for providing APIs for KGs. Our results reveal several limitations on existing API-based specifications, technologies and tools for KG consumption, which outline exciting research challenges including automatic API generation, API resource path prediction, ontology-based API versioning, and API validation and testing.",
        "publication_date": "2021-01-01",
        "authors": "Jorge Chávez, Rosa Montaño, Rosa Barrera, Jaime Sánchez, Jaime Fauré",
        "file_name": "20250512000324.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/jws_survey_2021/PDFs/20250512000324.pdf",
        "pdf_link": "https://dgarijo.com/papers/jws_survey_2021.pdf",
        "file_html": "html/20250512000324.html",
        "keywords": "Ontology, OWL, Ontology engineering, Web API, Application development, Knowledge graph",
        "author": "Paola Espinoza-Arias and Daniel Garijo and Oscar Corcho",
        "url": "https://dgarijo.com/papers/jws_survey_2021.pdf",
        "doi": "10.1016/j.websem.2021.100655",
        "issn": "1570-8268",
        "year": "2021",
        "pages": "100655",
        "journal": "Journal of Web Semantics",
        "ENTRYTYPE": "article",
        "ID": "ESPINOZAARIAS2021"
    },
    {
        "title": "Declarative RDF construction from in-memory data structures with RML",
        "implementation_urls": [],
        "abstract": "AbstractKnowledge graphs are often constructed from heterogeneous data sources using declarative mappinglanguages. Mapping languages define rules that apply ontology terms to raw data to describe howa knowledge graph should be constructed from these raw data. While most mapping languages andsystems support knowledge graph construction from different data formats, e.g., CSV, XML or JSON, anddifferent types of data sources, e.g., files, Web APIs or databases, there is still no support for mappingin-memory data structures to knowledge graphs, i.e. data which is temporarily stored in RAM. Currently,this data must first be stored in HDD, locally or in the cloud, for RDF construction systems to accessthem and construct a knowledge graph. However, writing these data to HDD and reading from HDDis a computationally expensive and redundant task. In this paper, we propose a method to constructRDF graphs from data produced by a software process and stored in RAM. We introduce an extension ofRML’s Logical Source to describe data structures produced by software, and exemplify our proposal withPython data structures. We extend a well-known RML system, Morph-KGC, to show the feasibility ofour method and validate this extension with two use cases: OpenML, which translates machine learningexecutions into RDF, and SOMEF, which extracts software metadata from its documentation, convertingthem to triples. This proposal simplifies the construction of RDF graphs from in-memory data structuresstored temporarily in RAM and enables the integration of data stored both in RAM and HDD.KeywordsKnowledge Graphs, Mapping Languages, RML1. IntroductionGraphs represented with the Resource Description Framework (RDF)1 and knowledge graphs(KGs), in general, have become increasingly popular as a means of representing and analyzingHersonissos’23: Fourth International Workshop On Knowledge Graph Construction, May 28, 2023, Hersonissos, GR*Corresponding author.$ ioannis.dasoulas@kuleuven.be (I. Dasoulas); david.chaves@upm.es (D. Chaves-Fraga); daniel.garijo@upm.es(D. Garijo); anastasia.dimou@kuleuven.be (A. Dimou)� https://orcid.org/0000-0002-8803-1244 (I. Dasoulas); http://davidchavesfraga.com (D. Chaves-Fraga);https://dgarijo.com (D. Garijo); https://orcid.org/0000-0003-2138-7972 (A. Dimou)� 0000-0002-8803-1244 (I. Dasoulas); 0000-0003-3236-2789 (D. Chaves-Fraga); 0000-0003-0454-7145 (D. Garijo);0000-0003-2138-7972 (A. Dimou)© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org)1https://www.w3.org/RDF/mailto:ioannis.dasoulas@kuleuven.bemailto:david.chaves@upm.esmailto:daniel.garijo@upm.esmailto:anastasia.dimou@kuleuven.behttps://orcid.org/0000-0002-8803-1244http://davidchavesfraga.comhttps://dgarijo.comhttps://orcid.org/0000-0003-2138-7972https://orcid.org/0000-0002-8803-1244https://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0003-0454-7145https://orcid.org/0000-0003-2138-7972https://creativecommons.org/licenses/by/4.0",
        "file_name": "20250511234846.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/KGC_ESWC_2023/PDFs/20250511234846.pdf",
        "pdf_link": "https://dgarijo.com/papers/KGC_ESWC_2023.pdf",
        "file_html": "html/20250511234846.html",
        "url": "https://dgarijo.com/papers/KGC_ESWC_2023.pdf",
        "booktitle": "Knowledge Graph Construction Workshop, co-located with ESWC 2023",
        "year": "2023",
        "author": "Dasoulas, Ioannis and Chaves-Fraga, David and Garijo, Daniel and Dimou, Anastasia",
        "ENTRYTYPE": "misc",
        "ID": "dasoulas2023declarative"
    },
    {
        "title": "A Controlled Crowdsourcing Approach for Practical Ontology Extensions and Metadata Annotations",
        "implementation_urls": [
            {
                "identifier": "https://github.com/nickmckay/GeoChronR",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/linkedEarth-iswc2017/PDFs/20250512001842.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The Linked Earth Platform is accessi-ble online12."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_24",
        "abstract": "Abstract Traditional approaches to ontology development have a large lapse between the time when a user using the ontology has found a need to extend it and the time when it does get extended. For scientists, this delay can be weeks or months and can be a significant barrier for adoption. We present a new ap-proach to ontology development and data annotation enabling users to add new metadata properties on the fly as they describe their datasets, creating terms that can be immediately adopted by others and eventually become standardized. This approach combines a traditional, consensus-based approach to ontology development, and a crowdsourced approach where expert users (the crowd) can dynamically add terms as needed to support their work. We have implemented this approach as a socio-technical system that includes: 1) a crowdsourcing plat-form to support metadata annotation and addition of new terms, 2) a range of social editorial processes to make standardization decisions for those new terms, and 3) a framework for ontology revision and updates to the metadata created with the previous version of the ontology. We present a prototype im-plementation for the paleoclimate community, the Linked Earth Framework, currently containing 700 datasets and engaging over 50 active contributors. Us-ers exploit the platform to do science while extending the metadata vocabulary, thereby producing useful and practical metadata. Keywords: Metadata, crowdsourcing, semantic wiki, collaborative ontology engineering, semantic science, incremental vocabulary development. 1 Introduction Existing frameworks for collaborative ontology development assume a clearly phased separation between ontology creation, release of the ontology, and use of the ontology [12, 8, 25]. These frameworks do not fit many areas of science, notably field-based sciences like ecology, Earth, and environmental sciences. These areas are extremely diverse, with data collected by many individual scientists each with idiosyncratic 2 instruments, methodologies, representations, and requirements. As soon as an ontolo-gy is put to the test through practical use, we can anticipate the need for many addi-tions and extensions to accommodate that diversity. Therefore, an ontology would need to be part of a framework that supports constant change while being used. More-over, the involvement of diverse experts in the community would be needed, raising challenges about incentives and more importantly about coordination of requirements.  Our goal is to support the paleoclimate community, which studies past climate based on the imprint on various systems like trees, glacier ice, or lake sediments. This community employs such a diverse array of data collection and analytical techniques that it has been very challenging to develop shared ontologies. As a result, it is hard to find and aggregate datasets contributed by diverse scientists to paint a global picture of past climate change. Many other scientific communities face similar challenges, as do any organizations with highly heterogeneous or dynamic knowledge environments. We propose a new approach to ontology development based on controlled crowdsourcing. Users (the crowd, who are experts in the domain rather than generic workers) concurrently create new terms as needed to describe their data, making the terms immediately available to others. Once the new terms are agreed upon, they can become part of the next version of the ontology. To coordinate the growth of the on-tology and to create necessary incentives, we organize the community so that proper editorial control is exercised. We have implemented this approach in the Linked Earth Framework and deployed it for the paleoclimate community. The Linked Earth Framework is a socio-technical system that includes a crowdsource annotation plat-",
        "publication_date": "2017-01-01",
        "authors": "Yolanda Gil, Daniel Garijo, Varun Ratnakar, Deborah Khider, Julien Emile‐Geay, Nicholas P. McKay",
        "file_name": "20250512001842.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/linkedEarth-iswc2017/PDFs/20250512001842.pdf",
        "pdf_link": "https://dgarijo.com/papers/linkedEarth-iswc2017.pdf",
        "file_html": "html/20250512001842.html",
        "organization": "Springer, Cham",
        "funding": "USNSF  IIS-1344272",
        "url": "https://dgarijo.com/papers/linkedEarth-iswc2017.pdf",
        "pages": "231--246",
        "volume": "10588",
        "booktitle": "d'Amato C. et al. (eds) The Semantic Web – ISWC 2017",
        "year": "2017",
        "author": "Gil, Yolanda and Garijo, Daniel and Ratnakar, Varun and Khider, Deborah and Emile-Geay, Julien and McKay, Nicholas",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2017controlled"
    },
    {
        "title": "Mapping the Web Ontology Language to the OpenAPI Specification",
        "implementation_urls": [
            {
                "identifier": "https://github.com/KnowledgeCaptureAndDiscovery/OBA",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/mapping_owl_to_oas_2020/PDFs/20250512000450.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The implementation release is available at the OBA’s GitHub repository.7 Our implementation allows generating API definitions for ontologies of dif-ferent sizes with a reasonable overhead."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-65847-2_11",
        "abstract": "Abstract. Many organizations maintain knowledge graphs that are or-ganized according to ontologies. However, these ontologies are imple-mented in languages (e.g. OWL) that are difficult to understand by userswho are not familiar with knowledge representation techniques. In par-ticular, this affects web developers who want to develop ontology-basedapplications but may find challenging accessing ontology-based data inknowledge graphs through SPARQL queries. To address this problem, wepropose an accessible layer for ontology-based knowledge graphs throughREST APIs. We define a mapping specification between the Web On-tology Language (OWL) and the OpenAPI Specification (OAS) to pro-vide ontology-based API definitions in a language well-known to webdevelopers. Our mapping specification identifies key similarities betweenOWL and OAS and provides implementation guidelines and examples.We also developed a reference mapping implementation that automati-cally transforms OWL ontologies into OpenAPI specifications in a matterof seconds.Keywords: OWL · OpenAPI · REST API · ontologies1 IntroductionMany public and private organizations have adopted a knowledge-driven ap-proach to make publicly available their knowledge graphs. Ontologies [10] playa crucial role in this approach, as they describe the knowledge about a domainin an agreed and unambiguous manner; and they allow organizing data, ease itsreusability, and facilitate its interoperability. Ontologies are usually formalizedin the Web Ontology Language (OWL) [6], a W3C recommendation to representthe semantics of a domain in a machine-readable way. However, OWL has a steeplearning curve due its inherent complexity [12], and newcomers may get confusedwith the meaning of constraints, axioms or the Open World Assumption.This problem has become evident in the case of developers who have an in-terest in taking advantage of the data available in existing knowledge graphsbut are not used to Semantic Web technologies. Instead, developers are famil-iar with REST APIs as a resource-access way which hides details about the2 P. Espinoza-Arias et al.implementation of operations for resource management or how such resourceshave been described according to the data models. To describe APIs, severalInterface Description Languages have been defined to document their domain,functional and non-functional aspects. The OpenAPI Specification3 (OAS) is abroadly adopted de facto standard for describing REST APIs in a programminglanguage-agnostic interface. OAS allows both humans and computers to under-stand and interact with a remote service. Due to its wide adoption, OAS has abig community behind, wich has provided tools to allow developers to generateAPI documentation, server generation, mockup design, etc.In this paper we describe additional work in the direction of making ontology-based data available though REST APIs. We define a mapping specificationbetween OWL and OAS to facilitate the work of those who are interested inusing data represented by semantic technologies and have to face the challeng-ing task of developing ontology-based applications. Our mapping also aims toenhance adherence to the FAIR data principles [13] by facilitating: Findability,as it provides a template for finding types of available resources in knowledgegraphs; Accessibility because it allows translating the ontology to an interfacethat developers are used to; Interoperability because the mapping matches two",
        "publication_date": "2020-01-01",
        "authors": "Paola Espinoza-Arias, Daniel Garijo, Óscar Corcho",
        "file_name": "20250512000450.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/mapping_owl_to_oas_2020/PDFs/20250512000450.pdf",
        "pdf_link": "https://dgarijo.com/papers/mapping_owl_to_oas_2020.pdf",
        "file_html": "html/20250512000450.html",
        "url": "https://dgarijo.com/papers/mapping_owl_to_oas_2020.pdf",
        "pages": "117--127",
        "booktitle": "2nd Workshop on Conceptual Modeling, Ontologies and (Meta)data Management for Findable, Accessible, Interoperable and Reusable (FAIR) Data (CMOMM4FAIR) Colocated with the 2020 Conference on Advances in Conceptual Modeling",
        "month": "12",
        "year": "2020",
        "author": "Espinoza{-}Arias, Paola and Garijo, Daniel and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "inbook_2020"
    },
    {
        "title": "Transforming meteorological data into linked data",
        "implementation_urls": [],
        "doi": "10.3233/SW-120089",
        "abstract": "Abstract. We describe the AEMET meteorological dataset, which makes available some data sources from the Agencia Estatalde Meteorología (AEMET, Spanish Meteorological Office) as Linked Data. The data selected for publication are generated everyten minutes by approximately 250 automatic weather stations deployed across Spain and made available as CSV files in theAEMET FTP server. These files are retrieved from the server, processed with Python scripts, transformed to RDF according toan ontology network (which reuses the W3C SSN Ontology), published in a triple store and visualized using Map4RDF.Keywords: meteorology, ontology, Linked Data, Sensor Networks1. IntroductionGovernments and their associated agencies world-wide are making some of their data sources availableunder open data licenses, so as to ensure consump-tion by the general public and other public and pri-vate organisations. In this context, AEMET1, the Span-ish Public Weather Service, announced on November2010 a major change in its data policy, offering a grad-ual, free and public access to its data in electronicformat. As a first step AEMET made publicly avail-able in its website meteorological data registered by itsweather stations, radars, lightning detectors and ozonesoundings. These data are currently offered with a freeand open license2 as spreadsheets in the AEMET FTPserver.Our work aims at facilitating the use of these databy processing them and offering them as free and openLinked Data (LD)3. Following our method for LD gen-*Alphabetical order1http://www.aemet.es/es/portada2ftp://ftpdatos.aemet.es/NOTA_LEGAL.txt3http://thedatahub.org/dataset/aemeteration [13], which we have successfully used in otherdomains, we start by processing these data and gen-erating RDF according to a meterology ontology net-work that extends the W3C Semantic Sensor NetworkOntology (SSN). The aim of this ontology network isto represent knowledge related to measurements madeby weather stations. These measurements represent thestate of the atmosphere (humidity, pressure, wind, etc.)in a particular place and time, and is conducted throughthe sensors equipped at each weather station. Finallywe publish these data according to LD principles andvisualize it with Map4RDF [5].The structure of this paper corresponds to the stepsfollowed to create the dataset. Section 2 describes themain features of the data selected to be converted asLD. Section 3 presents the design decisions for theURIs, while Section 4 explains the development of ourOntology Network for Weather Data. Section 5 de-scribes the RDF generation process. Next, Section 6presents the exploitation of the metereological LD. Fi-nally, we present the conclusions and future lines ofwork.",
        "publication_date": "2013-01-01",
        "authors": "Ghislain Auguste Atemezing, Óscar Corcho, Daniel Garijo, José Ferrater Mora, María Poveda‐Villalón, Pablo Rozas, Daniel Vila-Suero, Boris Villazón-Terrazas",
        "file_name": "20250512001001.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/meteo/PDFs/20250512001001.pdf",
        "pdf_link": "https://dgarijo.com/papers/meteo.pdf",
        "file_html": "html/20250512001001.html",
        "url": "https://dgarijo.com/papers/meteo.pdf",
        "pages": "285--290",
        "number": "3",
        "volume": "4",
        "publisher": "IOS Press",
        "journal": "Semantic Web",
        "year": "2013",
        "author": "Atemezing, Ghislain and Corcho, Oscar and Garijo, Daniel and Mora, Jos{\\'e} and Poveda-Villal{\\'o}n, Mar{\\'i}a and Rozas, Pablo and Vila-Suero, Daniel and Villaz{\\'o}n-Terrazas, Boris",
        "ENTRYTYPE": "article",
        "ID": "atemezing2013transforming"
    },
    {
        "title": "Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dtarb/TauDEM",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/MINT-tiis/PDFs/20250512000337.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available from https://w3id.org/okn/o/sdm/."
                    }
                ]
            }
        ],
        "doi": "10.1145/3453172",
        "abstract": "drought indices), in order to provide variables and abstractions that can convey the state of the system and behavior patterns that are useful to audiences beyond the model developers.  Indices are often statistical in nature and describe deviation from an average condition. For instance, a drought index value of 4 means extreme drought conditions (4 standard deviations from the mean).  • Visualization designs that are useful for a model need to be also captured.  This includes extracting useful variables from the inputs and outputs of the model, and creating appropriate views for visualization (e.g., coarser-grained results, statistical properties, etc.).  Generating proper visualizations may include fetching other data as reference (e.g. fetching historical data in order to compare the model predictions with annual averages).   • Combined results often need to be generated from many model executions, and these combinations are specific to each model.    • Responses that are not directly generated by the model but can be derived from model outputs. Table 4. Glossary of major concepts to describe models. Concept Description Model theories The principles underpinning the design and implementation of the model, including physical laws, biological postulates, chemical reactions, or socioeconomic theories. Model parameters The parameters in the equations that express model theories.  To apply the model to a specific system, model parameters are often adjusted based on the observations collected for that specific system. Model variables The observed or inferred quantities that can be measured or estimated about a complex system to describe its state over time.  A model can have input variables, internal variables, and output variables.     18 Model processes The dynamic drivers that make a system change state and therefore the value of its variables.   Model software A software package that includes many different functions to set up and run a model under a variety of assumptions.  A model software can have different versions. Model configurations A specific invocation function for model software that ensures the inclusion of certain model processes and variables while excluding others. Model set ups The adaptation of a generic model configuration to a specific system, so that model parameters are adjusted to that system based on the observations collected about the system’s past behaviors. Adjustable parameters A parameter of a model whose value affects an input variable, and can be adjusted to explore different situations.  For example, an agriculture model can have an adjustable parameter that sets the crop to weeds ratio (or range thereof) so users can explore different weed growth situations. Interventions Human actions that can change the course of a system’s behavior, and can be explored through the settings of adjustable parameters and input variables.   Model files Files that are inputs to a model or generated by a model, and contain input and output variables as well as model parameters. Table 4 summarizes major concepts to describe models as problem solving components.  The rest of this section provides formal definitions for the terms that are used in our work.  Important modeling processes, such as model calibration, gridding, and sensitivity analysis, are not currently included in our framework.  Model calibration (or parameterization) requires adjusting model parameters so its predictions are consistent with historical data.  Gridding requires setting up spatial grids of a shape (e.g., regular cubes, irregular polygons) and size (e.g., 1 km, 1m) that allow the model to capture the physical environment with adequate granularity. Sensitivity analysis provides information about uncertain the results would be given underdetermined parameter values. These processes can be largely automated, but may require manual ",
        "publication_date": "2021-06-30",
        "authors": "Yolanda Gil, Daniel Garijo, Deborah Khider, Craig A. Knoblock, Varun Ratnakar, Maximiliano Osorio, Hernán Vargas, Tam Minh Pham, Jay Pujara, Basel Shbita, Bình Dương Vũ, Yao‐Yi Chiang, Dan Feldman, Yijun Lin, Hae Jin Song, Vipin Kumar, Ankush Khandelwal, Michael Steinbach, Kshitij Tayal, Shaoming Xu, Suzanne A. Pierce, Lissa Pearson, Daniel Hardesty-Lewis, Ewa Deelman, Rafael Ferreira da Silva, Rajiv Mayani, Armen R. Kemanian, Yuning Shi, Lorne Leonard, S. D. Peckham, Maria Stoica, Kelly M. Cobourn, Zeya Zhang, Christopher Duffy, Lele Shu",
        "file_name": "20250512000337.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/MINT-tiis/PDFs/20250512000337.pdf",
        "pdf_link": "https://dgarijo.com/papers/MINT-tiis.pdf",
        "file_html": "html/20250512000337.html",
        "url": "https://dgarijo.com/papers/MINT-tiis.pdf",
        "numpages": "49",
        "articleno": "11",
        "month": "July",
        "journal": "ACM Trans. Interact. Intell. Syst.",
        "issn": "2160-6455",
        "number": "2",
        "volume": "11",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "issue_date": "July 2021",
        "year": "2021",
        "author": "Yolanda Gil and Daniel Garijo and Deborah Khider and Craig A. Knoblock and Varun Ratnakar and Maximiliano Osorio and Hernán Vargas and Minh Pham and Jay Pujara and Basel Shbita and  Binh Vu and Yao-Yi Chiang and Dan Feldman and Yijun Lin and Hayley Song and Vipin Kumar and Ankush Khandelwal and Michael Steinbach and Kshitij Tayal and Shaoming Xu and Suzanne A. Pierce and Lissa Pearson and Daniel Hardesty-Lewis and Ewa Deelman and Rafael Ferreira da Silva and Rajiv Mayani and Armen R. Kemanian and Yuning Shi and Lorne Leonard and Scott Peckham and Maria Stoica and Kelly Cobourn and Zeya Zhang and Christopher Duffy and Lele Shu",
        "ENTRYTYPE": "article",
        "ID": "tiis2021"
    },
    {
        "title": "A framework for the broad dissemination of hydrological models for non-expert users",
        "implementation_urls": [
            {
                "identifier": "https://github.com/mintproject/mic",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/MINT_MIC_Paper_Submitted/PDFs/20250511234900.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "URL: https://doi.org/10.101775 6/j.gloplacha.2013.02.004, doi:10.1016/j.gloplacha.2013.02.004.776 USArmyCorps of Engineers, 2000."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.envsoft.2023.105695",
        "abstract": "12 ABSTRACT1314Hydrological models are essential in water resources management, but the expertise required to15operate them often exceeds that of potential stakeholders. We present an approach that facili-16tates the dissemination of hydrological models, and its implementation in the Model INTegra-17tion (MINT) framework. Our approach follows principles from software engineering to create18software components that reveal only selected functionality of models which is of interest to19users while abstracting from implementation complexity, and to generate metadata for the model20components. This methodology makes the models more findable, accessible, interoperable, and21reusable in support of FAIR principles. We showcase ourmethodology and its implementation in22MINT using two case studies. We illustrate how the models SWAT and MODFLOW are turned23into software components by hydrology experts, and how users without hydrology expertise can24find, adapt, and execute them. The two models differ in terms of represented processes and in25model design and structure. Our approach also benefits expert modelers, by simplifying model26sharing and the execution of model ensembles. MINT is a general modeling framework that uses27artificial intelligence techniques to assist users, and is released as open-source software.2829ORCID(s):First Author et al.: Preprint submitted to Elsevier Page 1 of 37Short Title of the ArticleHighlights30• An approach that facilitates hydrological model dissemination from expert modelers to non-experts31• Software engineering methods are proposed to simplify model complexity by creating software components32• Non-experts can easily modify selected parameters and execute models provided by experts33• Our approach makes models more findable, accessible, interoperable, and reusable in support of FAIR principles34• Various applications benefited from this approach within the MINT framework35First Author et al.: Preprint submitted to Elsevier Page 2 of 37Short Title of the Article1. Introduction36Hydrological models (HMs) are commonly used for water resources management and are mainly developed and37used by expert researchers or engineers working in the water sector. The results of HMs are important and considered38in decision-making processes of government agencies (Ruiz-Ortiz et al., 2019; Andreu et al., 1996). HM applications39include estimation of water availability (Döll et al., 2003), development of water management strategies (Haasnoot40et al., 2011), flood risk assessment (Merz et al., 2010), climate impact analysis (Krysanova and Hattermann, 2017;41Lobanova et al., 2018; Hattermann et al., 2018), solute transport (Konikow, 2010; Morales et al., 2010) and spatial42characterization of hydrological system variables such as soil water content (Brocca et al., 2017), desalination and43industrial wastewater treatment (Panagopoulos, 2022) as well as groundwater heads (Reinecke et al., 2019). HMs44vary widely in terms of their mathematical description of prevalent hydrological processes and their spatial model45structure, ranging from lumped conceptual models (Bittner et al., 2018; Booij and Krol, 2010) to distributed physical46models (Brunner and Simmons, 2012; Newman et al., 2017).47A fundamental understanding of hydrological processes is needed in order to reasonably set up a hydrologicalmodel48for a new region or modeling problem. This may become an obstacle for the use of HMs by decision-makers and other49users (Lüke and Hack, 2018). In practice, model results are presented to decision-makers as a summary focusing only50on a few specific variables of interest, such as streamflow or groundwater heads. The interests and requirements of51decision-makers and various stakeholders can diverge widely from what may be hydrologically interesting. Decision52makers in water resources management are usually interested in the assessment of the water balance, primarily the53availability of water in space and time. HMs allow a holistic view on the components of the water cycle, from which54insightful information, e.g. limiting factors in space and/or time, can be derived. These variables do not necessarily be55restricted towater availability, but could also refer to evapotranspiration, soil water or precipitation. Miscommunication56between science and non-expert groups is therefore not a rarity (Timmerman and Langaas, 2005). This increases the57",
        "publication_date": "2023-04-10",
        "authors": "Timo Schaffhauser, Daniel Garijo, Maximiliano Osorio, Daniel Bittner, Suzanne A. Pierce, Hernán Vargas, Markus Disse, Yolanda Gil",
        "file_name": "20250511234900.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/MINT_MIC_Paper_Submitted/PDFs/20250511234900.pdf",
        "pdf_link": "https://dgarijo.com/papers/MINT_MIC_Paper_Submitted.pdf",
        "file_html": "html/20250511234900.html",
        "keywords": "Software metadata, Model metadata, Model encapsulation, Model catalogs, MINT, Hydrological models",
        "author": "Timo Schaffhauser and Daniel Garijo and Maximiliano Osorio and Daniel Bittner and Suzanne Pierce and Hernán Vargas and Markus Disse and Yolanda Gil",
        "url": "https://dgarijo.com/papers/MINT_MIC_Paper_Submitted.pdf",
        "issn": "1364-8152",
        "year": "2023",
        "pages": "105695",
        "journal": "Environmental Modelling & Software",
        "ENTRYTYPE": "article",
        "ID": "SCHAFFHAUSER2023105695"
    },
    {
        "title": "Common motifs in scientific workflows: An empirical analysis",
        "implementation_urls": [],
        "doi": "10.1109/eScience.2012.6404427",
        "abstract": "Abstract—While workflow technology has gained momentumin the last decade as a means for specifying and enacting compu-tational experiments in modern science, reusing and repurposingexisting workflows to build new scientific experiments is still adaunting task. This is partly due to the difficulty that scientistsexperience when attempting to understand existing workflows,which contain several data preparation and adaptation steps inaddition to the scientifically significant analysis steps. One wayto tackle the understandability problem is through providingabstractions that give a high-level view of activities undertakenwithin workflows. As a first step towards abstractions, we reportin this paper on the results of a manual analysis performed overa set of real-world scientific workflows from Taverna and Wingssystems. Our analysis has resulted in a set of scientific workflowmotifs that outline i) the kinds of data intensive activities that areobserved in workflows (data oriented motifs), and ii) the differentmanners in which activities are implemented within workflows(workflow oriented motifs). These motifs can be useful to informworkflow designers on the good and bad practices for workflowdevelopment, to inform the design of automated tools for thegeneration of workflow abstractions, etc.I. INTRODUCTIONScientific workflows have been increasingly used in the lastdecade as an instrument for data intensive scientific analysis.In these settings, workflows serve a dual function: first asdetailed documentation of the method (i. e. the input sourcesand processing steps taken for the derivation of a certaindata item) and second as re-usable, executable artifacts fordata-intensive analysis. Workflows stitch together a varietyof data manipulation activities such as data movement, datatransformation or data visualization to serve the goals of thescientific study. The stitching is realized by the constructsmade available by the workflow system used and is largelyshaped by the environment in which the system operates andthe function undertaken by the workflow.A variety of workflow systems are in use [10] [3] [7] [2]serving several scientific disciplines. A workflow is a softwareartifact, and as such once developed and tested, it can beshared and exchanged between scientists. Other scientists canthen reuse existing workflows in their experiments, e.g., assub-workflows [17]. Workflow reuse presents several advan-tages [4]. For example, it enables proper data citation andimproves quality through shared workflow development byleveraging the expertise of previous users. Users can alsore-purpose existing workflows to adapt them to their needs[4]. Emerging workflow repositories such as myExperiment[14] and CrowdLabs [8] have made publishing and findingworkflows easier, but scientists still face the challenges of re-use, which amounts to fully understanding and exploiting theavailable workflows/fragments. One difficulty in understanding",
        "publication_date": "2013-09-21",
        "authors": "Daniel Garijo, Pinar Alper, Khalid Belhajjame, Óscar Corcho, Yolanda Gil, Carole Goble",
        "file_name": "20250511232315.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/motifs2012/PDFs/20250511232315.pdf",
        "pdf_link": "https://dgarijo.com/papers/motifs2012.pdf",
        "file_html": "html/20250511232315.html",
        "funding": "USAFOSR  FA9550-11-1-0104",
        "keywords": "natural sciences computing;workflow management software;Taverna systems;Wings systems;common motifs;data adaptation steps;data oriented motifs;data preparation;empirical analysis;scientific workflows;understandability problem;workflow abstraction generation;workflow oriented motifs;Bioinformatics;Cleaning;Data visualization;Drugs;Genomics;Humans;Servers",
        "url": "https://dgarijo.com/papers/motifs2012.pdf",
        "pages": "1--8",
        "booktitle": "2012 IEEE 8th International Conference on E-Science",
        "month": "October",
        "year": "2012",
        "author": "D. Garijo and P. Alper and K. Belhajjame and O. Corcho and Y. Gil and C. Goble",
        "ENTRYTYPE": "inproceedings",
        "ID": "6404427"
    },
    {
        "title": "Nine Best Practices for Research Software Registries and Repositories: A Concise Guide",
        "implementation_urls": [
            {
                "identifier": "https://github.com/force11/force11-sciwg",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "README_TEXT",
                        "location_type": "ARXIV",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "RELATED_PAPERS",
                        "location_type": "ARXIV",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_README",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    }
                ]
            }
        ],
        "arxiv": "2012.13117",
        "publication_date": "2021-01-04",
        "authors": "Task Force On Best Practices For Software Registries, Alain Monteil, Alejandra González-Beltrán, Alexandros Ioannidis, Alice Allen, Allen Lee, Anita Bandrowski, Bruce E. Wilson, Bryce Mecum, Cai Du, Carly Robinson, Daniel Garijo, Daniel S. Katz, David G. Long, Genevieve Milliken, Hervé Ménager, Jessica Hausman, Jurriaan H. Spaaks, Katrina Fenlon, Kristin Vanderbilt, Lorraine Hwang, Lynn Davis, Martin Fenner, Michael R. Crusoe, Michael Hucka, Mingfang Wu, Neil Chue Hong, Peter Teuben, Shelley Stall, Stephan Druskat, Ted Carnevale, Thomas E. Morrell",
        "file_name": "20250512000532.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/nine_best_2020/PDFs/20250512000532.pdf",
        "pdf_link": "https://dgarijo.com/papers/nine_best_2020.pdf",
        "file_html": "html/20250512000532.html",
        "url": "https://dgarijo.com/papers/nine_best_2020.pdf",
        "journal": "arXiv preprint",
        "year": "2020",
        "author": "Monteil, Alain and Gonzalez-Beltran, Alejandra and Ioannidis, Alexandros and Allen, Alice and Lee, Allen and Bandrowski, Anita and Wilson, Bruce E and Mecum, Bryce and Du, Cai Fan and Robinson, Carly and Daniel Garijo and others",
        "ENTRYTYPE": "article",
        "ID": "monteil2020nine"
    },
    {
        "title": "Nine best practices for research software registries and repositories",
        "implementation_urls": [
            {
                "identifier": "https://github.com/force11/force11-sciwg",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "CITATION_README",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    }
                ]
            },
            {
                "identifier": "https://github.com/codemeta/codemeta",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/nine_best_practices/PDFs/20250511234951.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available at https://github.com/codemeta/codemeta."
                    }
                ]
            }
        ],
        "doi": "10.7717/peerj-cs.1023",
        "abstract": "ABSTRACTScientific software registries and repositories improve software findability andresearch transparency, provide information for software citations, and fosterpreservation of computational methods in a wide range of disciplines. Registries andrepositories play a critical role by supporting research reproducibility andreplicability, but developing them takes effort and few guidelines are available to helpprospective creators of these resources. To address this need, the FORCE11 SoftwareCitation Implementation Working Group convened a Task Force to distill theexperiences of the managers of existing resources in setting expectations for allstakeholders. In this article, we describe the resultant best practices which includedefining the scope, policies, and rules that govern individual registries andrepositories, along with the background, examples, and collaborative work that wentinto their development. We believe that establishing specific policies such as thosepresented here will help other scientific software registries and repositories betterserve their users and their disciplines.Subjects Computer Education, Databases, Digital LibrariesKeywords Best practices, Research software repository, Research software registry, Softwaremetadata, Repository policies, Research software registry guidelinesINTRODUCTIONResearch software is an essential constituent in scientific investigations (Wilson et al., 2014;Momcheva & Tollerud, 2015; Hettrick, 2018; Lamprecht et al., 2020), as it is often used totransform and prepare data, perform novel analyses on data, automate manual processes,and visualize results reported in scientific publications (Howison & Herbsleb, 2011).Research software is thus crucial for reproducibility and has been recognized by thescientific community as a research product in its own right—one that should be properlydescribed, accessible, and credited by others (Smith, Katz & Niemeyer, 2016; Chue Honget al., 2021). As a result of the increasing importance of computational methods,communities such as Research Data Alliance (RDA) (Berman & Crosas, 2020) (https://www.rd-alliance.org/) and FORCE11 (Bourne et al., 2012) (https://www.force11.org/)How to cite this article Garijo D, Ménager H, Hwang L, Trisovic A, Hucka M, Morrell T, Allen A. et al., 2022. Nine best practices forresearch software registries and repositories. PeerJ Comput. Sci. 8:e1023 DOI 10.7717/peerj-cs.1023Submitted 28 September 2021Accepted 9 June 2022Published 8 August 2022Corresponding authorDaniel Garijo, daniel.garijo@upm.esAcademic editorVarun GuptaAdditional Information andDeclarations can be found onpage 24DOI 10.7717/peerj-cs.1023Copyright2022 Garijo et al.Distributed underCreative Commons CC-BY 4.0https://github.com/force11/force11-sciwghttps://github.com/force11/force11-sciwghttps://www.rd-alliance.org/https://www.rd-alliance.org/",
        "publication_date": "2022-08-08",
        "authors": "Daniel Garijo, Hervé Ménager, Lorraine Hwang, A. Trisovic, Michael Hucka, Thomas E. Morrell, Alice Allen",
        "file_name": "20250511234951.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/nine_best_practices/PDFs/20250511234951.pdf",
        "pdf_link": "https://dgarijo.com/papers/nine_best_practices.pdf",
        "file_html": "html/20250511234951.html",
        "pages": "e1023",
        "year": "2022",
        "month": "August",
        "author": "Garijo, Daniel and Ménager, Hervé and Hwang, Lorraine and Trisovic, Ana and Hucka, Michael and Morrell, Thomas and Allen, Alice and {Task Force on Best Practices for Software Registries} and {SciCodes Consortium}",
        "journal": "PeerJ Computer Science",
        "url": "https://dgarijo.com/papers/nine_best_practices.pdf",
        "issn": "2376-5992",
        "volume": "8",
        "ENTRYTYPE": "article",
        "ID": "garijo_nine_2022"
    },
    {
        "title": "{OBA}: An Ontology-Based Framework for Creating REST APIs for Knowledge Graphs",
        "implementation_urls": [
            {
                "identifier": "https://github.com/KnowledgeCaptureAndDiscovery/OBA",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "FILE_CFF",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "README_BIBTEX",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "README_TEXT",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_FILE",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    },
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/OBA__Ontology_Based_APIs/PDFs/20250512002002.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Resource type: Software License: Apache 2.0 DOI: https://doi.org/10.5281/zenodo.3686266 Repository: https://github.com/KnowledgeCaptureAndDiscovery/OBA/Keywords: Ontology · API · REST · JSON · JSON-LD · data accessi-bility · knowledge graph · OpenAPI · OAS."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62466-8_4",
        "arxiv": "2007.09206",
        "abstract": "Abstract. In recent years, Semantic Web technologies have been in-creasingly adopted by researchers, industry and public institutions todescribe and link data on the Web, create web annotations and consumelarge knowledge graphs like Wikidata and DBpedia. However, there isstill a knowledge gap between ontology engineers, who design, popu-late and create knowledge graphs; and web developers, who need tounderstand, access and query these knowledge graphs but are not fa-miliar with ontologies, RDF or SPARQL. In this paper we describe theOntology-Based APIs framework (OBA), our approach to automaticallycreate REST APIs from ontologies while following RESTful API bestpractices. Given an ontology (or ontology network) OBA uses standardtechnologies familiar to web developers (OpenAPI Specification, JSON)and combines them with W3C standards (OWL, JSON-LD frames andSPARQL) to create maintainable APIs with documentation, unit tests,automated validation of resources and clients (in Python, Javascript,etc.) for non Semantic Web experts to access the contents of a targetknowledge graph. We showcase OBA with three examples that illustratethe capabilities of the framework for different ontologies.Resource type: SoftwareLicense: Apache 2.0DOI: https://doi.org/10.5281/zenodo.3686266Repository: https://github.com/KnowledgeCaptureAndDiscovery/OBA/Keywords: Ontology · API · REST · JSON · JSON-LD · data accessi-bility · knowledge graph · OpenAPI · OAS.1 IntroductionKnowledge graphs have become a popular technology for representing structuredinformation on the Web. The Linked Open Data Cloud1 contains more than1200 linked knowledge graphs contributed by researchers and public institutions.Major companies like Google,2 Microsoft,3 or Amazon [16] use knowledge graphs1https://lod-cloud.net/2https://developers.google.com/knowledge-graph3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/https://lod-cloud.net/https://developers.google.com/knowledge-graphhttps://www.microsoft.com/en-us/research/project/microsoft-academic-graph/https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/2 Daniel Garijo and Maximiliano Osorioto represent some of their information. Recently, crowdsourced knowledge graphssuch as Wikidata [15] have surpassed Wikipedia in the number of contributionsmade by users.In order to create and structure these knowledge graphs, ontology engineersdevelop vocabularies and ontologies defining the semantics of the classes, objectproperties and data properties represented in the data. These ontologies are thenused in extraction-transform-load pipelines to populate knowledge graphs withdata and make the result accessible on the Web to be queried by users (usually asan RDF dump or a SPARQL endpoint). However, consuming and contributingto knowledge graphs exposed in this manner is problematic for two main reasons.First, exploring and using the contents of a knowledge graph is a time consumingtask, even for experienced ontology engineers (common problems include lack",
        "publication_date": "2020-01-01",
        "authors": "Daniel Garijo, Maximiliano Osorio",
        "file_name": "20250512002002.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/OBA__Ontology_Based_APIs/PDFs/20250512002002.pdf",
        "pdf_link": "https://dgarijo.com/papers/OBA__Ontology_Based_APIs.pdf",
        "file_html": "html/20250512002002.html",
        "organization": "Springer, Cham",
        "url": "https://dgarijo.com/papers/OBA__Ontology_Based_APIs.pdf",
        "isbn": "978-3-030-62466-8",
        "year": "2020",
        "pages": "48--64",
        "booktitle": "International Semantic Web Conference",
        "author": "Garijo, Daniel and Osorio, Maximiliano",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2020OBA"
    },
    {
        "title": "{OKG}-{Soft}: {An} {Open} {Knowledge} {Graph} with {Machine} {Readable} {Scientific} {Software} {Metadata}",
        "implementation_urls": [
            {
                "identifier": "https://github.com/mintproject/MINT-ModelCatalogIngestionAPI",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/OKG-SoftEscience2019/PDFs/20250512003031.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The latest version of the ontology is available and documented at https://w3id.org/okn/o/sd."
                    }
                ]
            }
        ],
        "doi": "10.1109/eScience.2019.00046",
        "abstract": "Abstract—Scientific software is crucial for understanding, reusing and reproducing results in computational sciences. Software is often stored in code repositories, which may contain human readable instructions necessary to use it and set it up. However, a significant amount of time is usually required to understand how to invoke a software component, prepare data in the format it requires, and use it in combination with other software. In this paper we introduce OKG-Soft, an open knowledge graph that describes scientific software in a machine readable manner. OKG-Soft includes: 1) an ontology designed to describe software and the specific data formats it uses; 2) an approach to publish software metadata as an open knowledge graph, linked to other Web of Data objects; and 3) a framework to annotate, query, explore and curate scientific software metadata. OKG-Soft supports the FAIR principles of findability, accessibility, interoperability, and reuse for software. We demonstrate the benefits of OKG-Soft with two applications: a browser for understanding scientific models in the environmental and social sciences, and a portal to combine climate, hydrology, agriculture, and economic software models.  Keywords—software metadata, software registries, FAIR, knowledge graphs, software composition, software interoperability I. INTRODUCTION  Software is a key product of scientific research, as it can be used to understand and reproduce the findings reported in a publication (e.g., by rerunning a hydrology model, a genome sequence analysis or testing a trained machine learning model). The importance of software is increasingly recognized [1], with publishers and community initiatives encouraging researchers to make their software openly available to others.1 Scientific software created by scientists should be appropriately documented and curated to facilitate reuse by other researchers. Code repositories such as GitHub 2  or BitBucket3 provide the means to store and version code, while software container repositories such as DockerHub4 capture the execution environment required to run software. However, there is usually a lack of important information that makes software difficult to discover and reuse, such as descriptions of the main features of the software, unambiguous usage instructions, incomplete sample data, etc. Moreover, when                                                  1 https://paperswithcode.com 2 github.com/ 3 https://bitbucket.org/ this kind of information is present, it is not machine readable, so it is hard to develop tools to facilitate those tasks for users. A major barrier to reuse is the time and effort required to understand how to run scientific software. Researchers need to understand how to prepare data for software, how to invoke ",
        "publication_date": "2019-09-01",
        "authors": "Daniel Garijo, Maximiliano Osorio, Deborah Khider, Varun Ratnakar, Yolanda Gil",
        "file_name": "20250512003031.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/OKG-SoftEscience2019/PDFs/20250512003031.pdf",
        "pdf_link": "https://dgarijo.com/papers/OKG-SoftEscience2019.pdf",
        "file_html": "html/20250512003031.html",
        "urldate": "2020-04-08",
        "url": "https://dgarijo.com/papers/OKG-SoftEscience2019.pdf",
        "isbn": "978-1-72812-451-3",
        "pages": "349--358",
        "address": "San Diego, CA, USA",
        "publisher": "IEEE",
        "booktitle": "2019 15th {International} {Conference} on {eScience} ({eScience})",
        "month": "September",
        "year": "2019",
        "author": "Garijo, Daniel and Osorio, Maximiliano and Khider, Deborah and Ratnakar, Varun and Gil, Yolanda",
        "shorttitle": "{OKG}-{Soft}",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo_okg-soft_2019"
    },
    {
        "title": "OnToology, a tool for collaborative development of ontologies.",
        "implementation_urls": [],
        "abstract": "ABSTRACTIn this demo we present OnToology, a tool for developing onto-logies collaboratively using Github. OnToology addresses severalsteps of the ontology development lifecycle, including documentation,representation, evaluation and publication in a non-intrusive way.1 INTRODUCTIONThe rise of collaborative technologies has sped up the developmentof software on the last decade. When working as a team, it is com-mon to use repositories for software development, open discussionsand having a ticketing system that warns and keeps track of the mainissues to be solved.This paradigm is slowly moving towards other domains, likeontology development. Ontologies, like software, require a set ofrequirements to be stablished and are usually discussed in a groupbefore agreeing on a design decision. Therefore, they benefit hea-vily from the ticketing system, versioning and decision tracking thatcollaborative environments offer. However, this is often not enough,as ontologies need to be further documented and published online.Although some tools cover part of these activities e.g. documenta-tion and evaluation, there are no tools that integrate them with acollaborative environment.In this demo we present OnToology1 a tool for documenting,evaluating, presenting and publishing ontologies developed col-laboratively. Section 2 describes the requirements for developingontologies collaboratively, while Section 3 describes our approach.Finally Section 4 describes related work and Section 5 introducesour efforts for improving the tool.2 ONTOLOGY DEVELOPMENT LIFE CYCLETypically, the ontology development process can be divided inseveral independent activities:• Ontology requirements: before committing to implement anontology, it is advised to write a set of competency questi-ons (CQs) in an ontology requirements specification documentas mentioned in NeOn methodology (Suárez-Figueroa et al.,2012), which will be used to test the ontology.• Ontology Implementation: once agreed on the ontology requi-rements, one can use an ontology editor such as NeOn-toolkit2or Protégé3 to design the properties and classes of the proposedontology.• Ontology evaluation: the resultant ontology can be evaluatedin two different ways: by checking whether the requirements∗To whom correspondence should be addressed: dgarijo@fi.upm.es1 http://purl.org/net/OnToology2 http://neon-toolkit.org/3 http://protege.stanford.edu/Fig. 1: Ontology development life cycle(i.e., CQs) are answered properly and by checking whether theontology follows design patterns and well stablished practicesfor its implementation or not.• Ontology documentation: an ontology is unlikely to be reused",
        "publication_date": "2015-01-01",
        "authors": "Ahmad Alobaid, Daniel Garijo, María Poveda‐Villalón, Idafen Santana-Pérez, Óscar Corcho",
        "file_name": "20250512001340.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/ontoology/PDFs/20250512001340.pdf",
        "pdf_link": "https://dgarijo.com/papers/ontoology.pdf",
        "file_html": "html/20250512001340.html",
        "url": "https://dgarijo.com/papers/ontoology.pdf",
        "booktitle": "ICBO",
        "year": "2015",
        "author": "Alobaid, Ahmad and Garijo, Daniel and Poveda-Villal{\\'o}n, Mar{\\'i}a and P{\\'e}rez, Idafen Santana and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "alobaid2015ontoology"
    },
    {
        "title": "OntoSoft: A distributed semantic registry for scientific software",
        "implementation_urls": [],
        "doi": "10.1109/eScience.2016.7870916",
        "abstract": "Abstract— OntoSoft is a distributed semantic registry for scientific software. This paper describes three major novel contributions of OntoSoft: 1) a software metadata registry designed for scientists, 2) a distributed approach to software registries that targets communities of interest, and 3) metadata crowdsourcing through access control. Software metadata is organized using the OntoSoft ontology along six dimensions that matter to scientists: identify software, understand and assess software, execute software, get support for the software, do research with the software, and update the software. OntoSoft is a distributed registry where each site is owned and maintained by a community of interest, with a distributed semantic query capability that allows users to search across all sites. The registry has metadata crowdsourcing capabilities, supported through access control so that software authors can allow others to expand on specific metadata properties.  Keywords—software registries, software metadata, scientific software, software catalogs, software repositories I. INTRODUCTION The software developed by scientists embodies important scientific knowledge that should be explicitly captured, curated, managed, and disseminated. Software captures mathematical models, statistical analyses, and causal reasoning that are used to generate new results. Scientists recognize the value of sharing software to avoid replicating effort and to inspect and reproduce results from others. In addition, recurring issues of provenance and uncertainty in the context of data could be better addressed with improved treatment of software: one of the best ways to understand data is to look at the software that uses it or generates it.  A major issue for scientific software reuse is the dissemination and documentation of existing codes. Although code repositories already exist and are used by many scientists, they typically contain basic metadata such as authors, license but lack appropriate metadata to facilitate discovery and reuse.  A second major issue in scientific software sharing is the limited sharing of codes used in scientific publications. While the loss of “dark data” in science is well recognized [1], we see an analogous problem in the pervasive loss of “dark software”. Many scientists do not share their software, because they are unaware of its value, or they do not know how, or they are worried about not getting proper acknowledgment, or they do not see its value. Studies show that scientists spend between 60% to 80% of a project’s effort collecting and preparing data before doing new science (e.g., [2]). This would indicate a significant overhead in developing software for data preparation that is only rarely shared and rarely reused. A common concern is the relatively lower quality of such software, since it is typically not written with robustness or generality in mind and scientists do not want their reputations ",
        "publication_date": "2016-10-01",
        "authors": "Yolanda Gil, Daniel Garijo, Saurabh Mishra, Varun Ratnakar",
        "file_name": "20250512001441.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/ontoSoft2016/PDFs/20250512001441.pdf",
        "pdf_link": "https://dgarijo.com/papers/ontoSoft2016.pdf",
        "file_html": "html/20250512001441.html",
        "organization": "IEEE",
        "url": "https://dgarijo.com/papers/ontoSoft2016.pdf",
        "pages": "331--336",
        "booktitle": "e-Science (e-Science), 2016 IEEE 12th International Conference on",
        "year": "2016",
        "author": "Gil, Yolanda and Garijo, Daniel and Mishra, Saurabh and Ratnakar, Varun",
        "ENTRYTYPE": "inproceedings",
        "ID": "gil2016ontosoft"
    },
    {
        "title": "Augmenting PROV with Plans in P-PLAN: Scientific Processes as Linked Data",
        "implementation_urls": [],
        "abstract": "Abstract. Provenance models are crucial for describing experimental results in science. The W3C Provenance Working Group has recently released the PROV family of specifications for provenance on the Web. While provenance focuses on what is executed, it is important in science to publish the general methods that describe scientific processes at a more abstract and general level. In this paper, we propose P-PLAN, an extension of PROV to represent plans that guid-ed the execution and their correspondence to provenance records that describe the execution itself. We motivate and discuss the use of P-PLAN and PROV to publish scientific workflows as Linked Data. Keywords: provenance, plan, PROV, scientific workflows, Linked Data. 1 Linked Data and Scientific Processes A crucial element of Linked Data for science is the publication and sharing of scien-tific processes to document how scientific results are generated. There are many kinds of scientific processes that could be shared as Linked Data. Recent research in this area includes publishing scientific workflows that capture data analysis processes [2] documenting scientific experiments, structuring claims and conclusions in scientific publications (as in SWAN1), and more recently organizing Research Objects [1]. For all these efforts, it is important to publish how scientific processes were executed, but it is also important to publish how they were planned. For example, assays are used to describe an experimental procedure in a biology laboratory and are very precise, while protocols are more general descriptions of those procedures. In essence, these protocols represent the plans that are followed in carrying out the assays.  There is now an emerging standard for publishing the provenance of processes on the Web.  This standard could be used for the publication of scientific processes, im-proving interoperability across scientific software as well as interoperability with                                                            1  http://www.w3.org/TR/hcls-swan/ mailto:dgarijo@delicias.dia.fi.upm.esother web provenance. The W3C PROV model2 describes the provenance of objects (prov:Entities) as a record of assertions about the steps (prov:Activities) that generat-ed them and the entities used in those steps. Provenance describes past execution, but does not offer a vocabulary to express the plan that the execution was supposed to follow. In terms of our example above, provenance vocabularies are appropriate for describing assays once they are executed, but are not designed to describe protocols. Therefore, in addition to the provenance record, it is often desirable to publish the plan that was followed during the execution. This would allow the provenance record to include what was envisioned would happen prior to the execution. Publishing the plan has several benefits: 1) the plan can provide a higher-level, more abstract de-scription of what was executed, which improves understandability and facilitates re-use in future situations; 2) the plan can describe the expectations for the execution, which can then be contrasted with the provenance to detect deviations and correct abnormalities. Acknowledging this need, PROV includes the term “prov:Plan”. How-ever, it does not elaborate any further how plans can be described or related to other provenance elements of the execution. Several vocabularies have been proposed to represent different aspects of scientific processes, including SWAN and OBI3. Ideally, the PROV standard would be adopted for all provenance aspects of these vocabularies, enabling interoperability of their records. However PROV will not address aspects concerned with methods and ab-stract plans, which would be useful for interoperability of linked science data. In this paper we propose to address this necessity by extending PROV with P-",
        "publication_date": "2012-01-01",
        "authors": "Daniel Garijo, Yolanda Gil",
        "file_name": "20250512000948.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/p-plan-lisc2012/PDFs/20250512000948.pdf",
        "pdf_link": "https://dgarijo.com/papers/p-plan-lisc2012.pdf",
        "file_html": "html/20250512000948.html",
        "url": "https://dgarijo.com/papers/p-plan-lisc2012.pdf",
        "address": "Boston, MA",
        "booktitle": "Second International Workshop on Linked Science: Tackling Big Data (LISC), held in conjunction with the International Semantic Web Conference (ISWC)",
        "year": "2012",
        "author": "Daniel Garijo and Yolanda Gil",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo-gil-lisc12"
    },
    {
        "title": "Workflow-Centric Research Objects: First Class Citizens in Scholarly Discourse",
        "implementation_urls": [],
        "abstract": "Abstract. A workflow-centric research object bundles a workflow, theprovenance of the results obtained by its enactment, other digital objectsthat are relevant for the experiment (papers, datasets, etc.), and anno-tations that semantically describe all these objects. In this paper, wepropose a model to specify workflow-centric research objects, and showhow the model can be grounded using semantic technologies and exist-ing vocabularies, in particular the Object Reuse and Exchange (ORE)model and the Annotation Ontology (AO). We describe the life-cycle of aresearch object, which resembles the life-cycle of a scientific experiment.1 IntroductionScientific workflows are used to describe series of structured activities and com-putations that arise in scientific problem-solving, providing scientists from vir-tually any discipline with a means to specify and enact their experiments [3].From a computational perspective, such experiments (workflows) can be definedas directed acyclic graphs where the nodes correspond to analysis operations,which can be supplied locally or by third party web services, and where theedges specify the flow of data between those operations.Besides being useful to describe and execute computations, workflows alsoallow encoding of scientific methods and know-how. Hence they are valuable ob-jects from a scholarly point of view, for several reasons: (i) to allow assessmentof the reproducability of results; (ii) to be reused by the same or by a differ-ent scientist; (iii) to be repurposed for other goals than those for which it wasoriginally built; (iv) to validate the method that led to a new scientific insight;(v) to serve as live-tutorials, exposing how to take advantage of existing datainfrastructure, etc. This follows a trend that can be observed in disciplines such2as Biology and Astronomy, with other types of objects, such as databases, in-creasingly becoming part of the research outcomes of an individual or a group,and hence also being shared, cited, reused, versioned, etc. [11]However, the use of workflow specifications on their own does not guaran-tee to support reusability, shareability, reproducibility, or better understandingof scientific methods. Workflow environment tools evolve across the years, orthey may even disappear. The services and tools used by the workflow maychange or evolve too. Finally, the data used by the workflow may be updatedor no longer available. To overcome these issues, additional information may beneeded. This includes annotations to describe the operations performed by theworkflow; annotations to provide details like authors, versions, citations, etc.;links to other resources, such as the provenance of the results obtained by ex-ecuting the workflow, datasets used as input, etc.. Such additional annotationsenable a comprehensive view of the experiment, and encourage inspection ofthe different elements of that experiment, providing the scientist with a pictureof the strengths and weaknesses of the digital experiment in relation to decay,adaptability, stability, etc.These richly annotation objects are what we call workflow-centric researchobjects. The notion of Research Object has been introduced in previous work[20, 19, 1] – here we focus on Research Objects that encapsulate scientific work-flows (hence workflow-centric). In particular, we build on earlier work on my-Experiment packs, which are bundles that contain elements such as workflows,documents and presentations [15]. Other related work is presented in Section2. In this paper we extend that work making the following contributions: we",
        "publication_date": "2012-05-28",
        "authors": "Khalid Belhajjame, Óscar Corcho, Daniel Garijo, Jun Zhao, Paolo Missier, David Newman, Raúl Palma, Sean Bechhofer, Esteban García-Cuesta, Asunción Gómez‐Pérez, Graham Klyne, Kevin Page, Marco Roos, José Enrique Ruiz, Stian Soiland‐Reyes, L. Verdes–Montenegro, David De Roure, Carole Goble",
        "file_name": "20250512000840.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paper-01/PDFs/20250512000840.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-903/paper-01.pdf",
        "file_html": "html/20250512000840.html",
        "url": "http://ceur-ws.org/Vol-903/paper-01.pdf",
        "booktitle": "Proceedings of the 2nd Workshop on Semantic Publishing",
        "pages": "1--12",
        "year": "2012",
        "author": "Belhajjame, Khalid and Corcho, Oscar and Garijo, Daniel and Zhao, Jun and Missier, Paolo and Newman, David and Palma, Ra{\\'u}l and Bechhofer, Sean and Garc{\\'i}a, Esteban and Cuesta, Jos{\\'e} Manuel G{\\'o}mez-P{\\'e}rez and others",
        "ENTRYTYPE": "article",
        "ID": "belhajjame2012workflow"
    },
    {
        "title": "Towards Assessing FAIR Research Software Best Practices in an Organization Using RDF-star",
        "implementation_urls": [],
        "abstract": "AbstractAn increasing number of scientists share the source code used or developed during their research(i.e., their research software) in open repositories, in order to support the results described in theirpublications. Recent best practices have been proposed by the community by aligning the Findable,Accessible, Interoperable and Reusable (FAIR) principles to Research Software. However, there arecurrently no means to assess the systematic adoption of these practices in a research organization. Inthis poster, we propose an automated pipeline to transform the software metadata of an organization asa Knowledge Graph in order to assess the current adoption of FAIR Research Software best practices.Our poster shows results from our own GitHub organization, the Ontology Engineering Group.KeywordsResearch Software, Metadata, FAIR software, FAIR, RDF-star1. IntroductionResearch Software (RS) [1] plays a crucial role in reproducing computational experiments, whereit can range from simple visualization scripts or data cleaning libraries to complex computationalpipelines that deliver the main findings described in a publication. RS has become key in manyapplication domains, ranging from Astronomy1 to Computational Biology [2]. Following theFindable, Accessible, Interoperable and Reusable (FAIR) principles for data [3] the scientificcommunity has discussed and adapted FAIR to RS [1], making available guidelines and bestpractices for researchers and RS engineers [4, 5]. Unfortunately, while there are tools forhelping developers adopt some of these practices [6], there is little work on assessing theiroverall adoption within a given organization.In this poster we propose an automated pipeline that creates a Knowledge Graph (KG) ofRS metadata to quantify the adoption of FAIR best RS practices within an organization. Ourpipeline assesses online code repositories using existing software metadata extraction tools[7, 8] and combines them with RML-star mappings [9] to quantify the adoption of an illustrativeset of best practices while tracking the provenance of each assertion.The reminder of the paper first describes the best practices we focus on in Section 2, followedby the data model and mappings used to create our KG in Section 3. We show how we quantifybest practices in Section 4, concluding the paper in Section 5.SEMANTICS 2023 EU: 19th International Conference on Semantic Systems, September 20-22, 2023, Leipzig, GermanyEnvelope-Open ana.iglesiasm@upm.es (A. Iglesias-Molina); daniel.garijo@upm.es (D. Garijo)Orcid 0000-0001-5375-8024 (A. Iglesias-Molina); 0000-0003-0454-7145 (D. Garijo)© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org)1https://www.ligo.caltech.edu/news/ligo20160211mailto:ana.iglesiasm@upm.esmailto:daniel.garijo@upm.eshttps://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0003-0454-7145https://creativecommons.org/licenses/by/4.0http://ceur-ws.orghttp://ceur-ws.orghttps://www.ligo.caltech.edu/news/ligo20160211Table 1Ten key requirements to assess FAIR research software best practices of a code repository.ID Best practice FAIR Principle Source",
        "file_name": "20250511234756.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paper-09/PDFs/20250511234756.pdf",
        "pdf_link": "https://ceur-ws.org/Vol-3526/paper-09.pdf",
        "file_html": "html/20250511234756.html",
        "url": "https://ceur-ws.org/Vol-3526/paper-09.pdf",
        "volume": "3526",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "Proceedings of the Posters and Demo Track of the 19th International Conference on Semantic Systems co-located with 19th International Conference on Semantic Systems (SEMANTiCS 2023)",
        "year": "2023",
        "author": "Iglesias-Molina, Ana and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "iglesias2023towards"
    },
    {
        "title": "Creating and Querying Personalized Versions of Wikidata on a Laptop",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/kgtk",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/paper-4/PDFs/20250511235616.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Type: Research paper Code repository: https://github.com/usc-isi-i2/kgtk/Keywords: Wikidata · Knowledge Graphs · KGTK · Kypher · Cypher 1 Introduction Modern Knowledge Graphs (KGs) are increasingly focused on improving their coverage of instances and statements and enhancing their expressivity in order to support application needs such as question answering and entity linking."
                    }
                ]
            }
        ],
        "doi": "10.48550/arxiv.2108.07119",
        "abstract": "Abstract. Application developers today have three choices for exploit-ing the knowledge present in Wikidata: they can download the Wikidatadumps in JSON or RDF format, they can use the Wikidata API to getdata about individual entities, or they can use the Wikidata SPARQLendpoint. None of these methods can support complex, yet common,query use cases, such as retrieval of large amounts of data or aggregationsover large fractions of Wikidata. This paper introduces KGTK Kypher, aquery language and processor that allows users to create personalizedvariants of Wikidata on a laptop. We present several use cases that il-lustrate the types of analyses that Kypher enables users to run on thefull Wikidata KG on a laptop, combining data from external resourcessuch as DBpedia. The Kypher queries for these use cases run much fasteron a laptop than the equivalent SPARQL queries on a Wikidata clonerunning on a powerful server with 24h time-out limits.Type: Research paperCode repository: https://github.com/usc-isi-i2/kgtk/Keywords: Wikidata · Knowledge Graphs · KGTK · Kypher · Cypher1 IntroductionModern Knowledge Graphs (KGs) are increasingly focused on improving theircoverage of instances and statements and enhancing their expressivity in orderto support application needs such as question answering and entity linking. As aresult, Wikidata [9], a popular and representative KG, contains nearly 95 millionentities described with over 1.3 billion statements.1 Wikidata is also highly ex-pressive, using a reification model where each statement includes qualifiers (e.g.,to indicate temporal validity) and references (which provide the source(s) fromwhich the statement comes from).Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).1 https://grafana.wikimedia.org/d/000000175/wikidata-datamodel-statements2 Chalupsky et. al.Application developers today have three choices for exploiting the knowledgepresent in Wikidata. They can download the Wikidata dumps in JSON or RDF[10] format, they can use the Wikidata API to get data about individual entities,or they can use the Wikidata SPARQL [6] endpoint for more elaborate andcomplex queries.2 The public Wikidata SPARQL endpoint restricts queries to 5minutes, returning an error when a query exceeds (or plans to exceed) that time.To mitigate the time-limit restriction, developers can load the massive WikidataRDF dump on their own servers, a relatively complex process that requires alarge server and several days.This paper introduces Kypher, the query language and processor of theKGTK Knowledge Graph Toolkit [3], which allows creating personalized vari-ants of Wikidata on a laptop, and enables running analytic queries faster than aWikidata SPARQL endpoint. Because Kypher uses the KGTK representation,it is not restricted to Wikidata, and can be used to query RDF KGs such asDBpedia [1]. The key advantages of Kypher over existing tooling are:1. Ability to extract large amounts of data from Wikidata.2. Ability to execute queries that retrieve large portions of the full Wikidata.3. Ability to build personalized versions of Wikidata, and extending it withother datasets for specific use cases.4. Easy installation as there are no databases to set up or administer.",
        "publication_date": "2021-01-01",
        "authors": "Hans Chalupsky, Pedro Szekely, Filip Ilievski, Daniel Garijo, Kartik Shenoy",
        "file_name": "20250511235616.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paper-4/PDFs/20250511235616.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2982/paper-4.pdf",
        "file_html": "html/20250511235616.html",
        "url": "http://ceur-ws.org/Vol-2982/paper-4.pdf",
        "archiveprefix": "arXiv",
        "eprint": "2108.07119",
        "year": "2021",
        "author": "Hans Chalupsky and Pedro Szekely and Filip Ilievski and Daniel Garijo and Kartik Shenoy",
        "ENTRYTYPE": "misc",
        "ID": "chalupsky2021creating"
    },
    {
        "title": "Semantic Workflows and Machine Learning for the Assessment of Carbon Storage by Urban Trees",
        "implementation_urls": [],
        "doi": "10.48550/arxiv.2009.10263",
        "abstract": "ABSTRACTClimate science is critical for understanding both the causes andconsequences of changes in global temperatures and has becomeimperative for decisive policy-making. However, climate sciencestudies commonly require addressing complex interoperability is-sues between data, software, and experimental approaches frommultiple fields. Scientific workflow systems provide unparalleledadvantages to address these issues, including reproducibility ofexperiments, provenance capture, software reusability and knowl-edge sharing. In this paper, we introduce a novel workflow witha series of connected components to perform spatial data prepa-ration, classification of satellite imagery with machine learningalgorithms, and assessment of carbon stored by urban trees. To thebest of our knowledge, this is the first study that estimates carbonstorage for a region in Africa following the guidelines from theIntergovernmental Panel on Climate Change (IPCC).KEYWORDSReproducibility, scientific workflows, machine learning, land covermapping, carbon assessment, Sentinel-21 INTRODUCTIONClimate science requires modeling natural and man-made processesthat are highly complex, exhibit non-linear dynamics and possessdisparate spatial and temporal scales. Handling this complexityrequires a holistic approach among multiple disciplines [29], butscientists from different fields may also need to use domain-specificdata sources, methods, and computational models. The integrationof their knowledge and experiments is a challenging task [7], espe-cially when a study is expected to provide actionable insights fordecision making at regional and local scale.Scientific workflows have emerged as an integrated solution tomanage this challenge, as they capture the computational stepsand data dependencies required to carry out a computational ex-periment [40]. Scientific workflows ease data handling (metadata,provenance), component versioning (parametrization, calibration)Copyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).and have a clear separation between workflow design and work-flow execution [4] [43]. One of the major advantages of scientificworkflow systems is their role in improving the reproducibility ofscientific studies. Reproducibility plays a critical role in climatesciences due to their impact in our society [38]. In fact, due to is-sues with the documentation of experiments, some climate sciencestudies have been re-examined lately due to their impact in globalpolicy-making [19] and water resources management [25].In this paper we describe the process we followed to design andimplement reusable scientific workflows for the climate sciences.In particular, we focus on carbon storage assessment by using ur-ban trees, a common requirement for cities to reduce their carbonemissions globally. Our contributions include the development ofa library of components for preparing geospatial data by doing",
        "publication_date": "2020-01-01",
        "authors": "Juan A. Cabrera, Daniel Garijo, Mark Crowley, Rober Carrillo, Yolanda Gil, Katherine Borda",
        "file_name": "20250512002822.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paper1/PDFs/20250512002822.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2526/paper1.pdf",
        "file_html": "html/20250512002822.html",
        "url": "http://ceur-ws.org/Vol-2526/paper1.pdf",
        "issn": "1613-0073",
        "volume": "2526",
        "address": "Los Angeles, California",
        "booktitle": "Proceedings of the Third Workshop on Capturing Scientific Knowledge (SciKnow 2019), held in conjunction with the 2019 ACM International Conference on Knowledge Capture (K-CAP)",
        "year": "2019",
        "author": "Juan Manuel Carrillo Garcia and Daniel Garijo and Mark Crowley and Rober Carrillo and Yolanda Gil and Katherine Borda",
        "ENTRYTYPE": "inproceedings",
        "ID": "carrillo-et-al-sciknow2019"
    },
    {
        "title": "SALTBot: Linking Software and Articles in Wikidata",
        "implementation_urls": [],
        "abstract": "Abstract. The web contains millions of useful spreadsheets and CSVfiles, but these files are difficult to use in applications because they usea wide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes it easyto map and link arbitrary spreadsheets and CSV files to the Wikidatadata model. The output of T2WML consists of Wikidata statementsthat can be loaded in the public Wikidata, or loaded in a Wikidataclone, creating an augmented Wikidata knowledge graph that applicationdevelopers can query using SPARQL.1Keywords: Knowledge Graphs, RDF, Entity Linking, Wikidata1 IntroductionThe web contains millions of useful spreadsheets and CSV files, including datafrom many government and international organizations. Organizations that offerdata often have web sites where users can search, browse and download dataon a large number of topics. Most institutions offer their data in Excel andCSV formats. The downloaded data is seldom directly usable because, unlikedatabases, which use one column per variable, spreadsheets often arrange thedata in different layouts.Fig. 1 illustrates the problem using data about homicide rates in differentcountries, downloaded from the United Nations web site2. We truncated andcolored the files for ease of presentation. The cells with the homicide numbersare highlighted in green, the cells that provide contextual information for thevalue are highlighted in blue, and header cells are highlighted in dark blue.Fig. 1a shows the layout of the data provided in the UN website; Fig. 1b showsa more compact representation using multi-level headers; Fig. 1c shows a layout1 Copyright (c) 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0). This material is basedupon work supported by United States Air Force under Contract No. FA8650-17-C-7715.2 https://dataunodc.un.org/crime/intentional-homicide-victimsFig. 1. Intentional Homicide Data (Excel file downloaded from dataunodc.un.org)that could be used to store the data in a database, and that can be used directlyin tools such as Pandas; Fig. 1d illustrates a common convention for arrangingdata by topic, by creating stacked tables that share common headings. All tablespresent the same homicide data. The interpretation of each value is defined byfour cells (country, year, population and source) that identify the context for avalue. In each table, the context cells are located in different parts of the data.Only in Fig. 1c (Database) the context cells are in the same row as the value; inthe other tables, context cells appear in different rows, in header rows (examplesa and b), or in visually distinct rows within the table (example d).Existing languages for mapping structured data to RDF, including R2RML3,RML [1], Karma [3] and CSV2RDF [2] process tabular data row by row, requiringtabular data to be in database format (Fig. 1c). RML supports non-tabularformats (JSON and XML) and Karma provides folding and unfolding operatorsto rearrange data for row-based processing. None support complex layouts suchas those in examples b or d.T2WML is a mapping language designed to meet three objectives: 1) Identifyand map data and their context qualifiers in arbitrary data layouts found in Exceland CSV files without the need of complex preprocessing steps to transformtables into a canonical \"Database\" representation; 2) Enable users who are not",
        "publication_date": "2019-01-01",
        "authors": "Pedro Szekely, Daniel Garijo, Jay Pujara, Divij Bhatia, Jiasheng Wu",
        "file_name": "20250512002808.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paper12/PDFs/20250512002808.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2456/paper12.pdf",
        "file_html": "html/20250511234507.html",
        "url": "https://ceur-ws.org/Vol-3640/paper12.pdf",
        "volume": "3640",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "Proceedings of the Wikidata Workshop 2023 co-located with 22nd International Semantic Web Conference (ISWC 2023)",
        "year": "2023",
        "author": "Bolinches, Jorge and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "bolinches2023saltbot"
    },
    {
        "title": "SALTBot: Linking Software and Articles in Wikidata",
        "implementation_urls": [],
        "abstract": "Abstract. The web contains millions of useful spreadsheets and CSVfiles, but these files are difficult to use in applications because they usea wide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes it easyto map and link arbitrary spreadsheets and CSV files to the Wikidatadata model. The output of T2WML consists of Wikidata statementsthat can be loaded in the public Wikidata, or loaded in a Wikidataclone, creating an augmented Wikidata knowledge graph that applicationdevelopers can query using SPARQL.1Keywords: Knowledge Graphs, RDF, Entity Linking, Wikidata1 IntroductionThe web contains millions of useful spreadsheets and CSV files, including datafrom many government and international organizations. Organizations that offerdata often have web sites where users can search, browse and download dataon a large number of topics. Most institutions offer their data in Excel andCSV formats. The downloaded data is seldom directly usable because, unlikedatabases, which use one column per variable, spreadsheets often arrange thedata in different layouts.Fig. 1 illustrates the problem using data about homicide rates in differentcountries, downloaded from the United Nations web site2. We truncated andcolored the files for ease of presentation. The cells with the homicide numbersare highlighted in green, the cells that provide contextual information for thevalue are highlighted in blue, and header cells are highlighted in dark blue.Fig. 1a shows the layout of the data provided in the UN website; Fig. 1b showsa more compact representation using multi-level headers; Fig. 1c shows a layout1 Copyright (c) 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0). This material is basedupon work supported by United States Air Force under Contract No. FA8650-17-C-7715.2 https://dataunodc.un.org/crime/intentional-homicide-victimsFig. 1. Intentional Homicide Data (Excel file downloaded from dataunodc.un.org)that could be used to store the data in a database, and that can be used directlyin tools such as Pandas; Fig. 1d illustrates a common convention for arrangingdata by topic, by creating stacked tables that share common headings. All tablespresent the same homicide data. The interpretation of each value is defined byfour cells (country, year, population and source) that identify the context for avalue. In each table, the context cells are located in different parts of the data.Only in Fig. 1c (Database) the context cells are in the same row as the value; inthe other tables, context cells appear in different rows, in header rows (examplesa and b), or in visually distinct rows within the table (example d).Existing languages for mapping structured data to RDF, including R2RML3,RML [1], Karma [3] and CSV2RDF [2] process tabular data row by row, requiringtabular data to be in database format (Fig. 1c). RML supports non-tabularformats (JSON and XML) and Karma provides folding and unfolding operatorsto rearrange data for row-based processing. None support complex layouts suchas those in examples b or d.T2WML is a mapping language designed to meet three objectives: 1) Identifyand map data and their context qualifiers in arbitrary data layouts found in Exceland CSV files without the need of complex preprocessing steps to transformtables into a canonical \"Database\" representation; 2) Enable users who are not",
        "publication_date": "2019-01-01",
        "authors": "Pedro Szekely, Daniel Garijo, Jay Pujara, Divij Bhatia, Jiasheng Wu",
        "file_name": "20250512002808.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paper12/PDFs/20250512002808.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2456/paper12.pdf",
        "file_html": "html/20250512002808.html",
        "url": "https://ceur-ws.org/Vol-3640/paper12.pdf",
        "volume": "3640",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "Proceedings of the Wikidata Workshop 2023 co-located with 22nd International Semantic Web Conference (ISWC 2023)",
        "year": "2023",
        "author": "Bolinches, Jorge and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "bolinches2023saltbot"
    },
    {
        "title": "FOOPS!: An Ontology Pitfall Scanner for the FAIR Principles",
        "implementation_urls": [],
        "abstract": "Abstract. This paper presents FOOPS!, a web service designed to as-sess the compliance of vocabularies or ontologies against the FAIR princi-ples. FOOPS! performs a total of 24 different checks from the four FAIRdimensions, reflecting the best practices and latest community discus-sions to adapt FAIR to semantic artefacts. The web service not onlydetect best practices according to each principle, but also offers an ex-planation of why a particular principle fails, and helpful suggestions toovercome common issues.Keywords: Ontology development · FAIR principles · FAIR semanticsPaper type: Demo (available at https://w3id.org/foops)1 IntroductionThe Findable, Accessible, Interoperable, Reusable (FAIR) data principles [6]have become increasingly relevant in the context of research data managementand reproducibility; being a main subject of discussion and adoption in commu-nity initiatives such as the Research Data Alliance, FORCE 11 and the EuropeanOpen Science Cloud. As a result, the FAIR principles have been adapted to otherresearch artifacts, such as software,1 and semantic resources such as ontologies.2In order to help researchers adopt best practices around FAIR, the scien-tific community has developed self-assessment tools and validators that helpresearchers assess the FAIRness of their resources. These are typically targeted? Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).?? The authors would like to thank Raúl Alcazar and Jacobo Mata for their help.This work has been supported by the Madrid Government under the MultiannualAgreement with Universidad Politécnica de Madrid in the line Support for R&Dprojects for Beatriz Galindo researchers, the HORIZON2020 project OntoCommons:Ontology-driven data documentation for Industry Commons (H2020-958371) and byKnowledgeSpaces: Técnicas y herramientas para la gestión de grafos de conocimien-tos para dar soporte a espacios de datos (PID2020-118274RB-I00).1 https://www.rd-alliance.org/groups/fair-research-software-fair4rs-wg2 https://www.fairsfair.eu/fair-semantics-interoperability-and-services-0Daniel Garijo, Oscar Corcho, and Maŕıa Poveda-Villalóntowards research data, such as AmIFAIR [7],3 F-UJI,4 or fair-checker,5 withsome recent additions for research software (e.g., howfairis6). However, there isno FAIR validator specifically targeted towards ontologies.In this demo we present FOOPS!, an ontology pitfall scanner for the FAIRprinciples. FOOPS! works for both OWL and SKOS vocabularies, and distin-guishes itself from existing services such as Vapour7 (focused on the quality ofthe content negotiation of resources) and OOPS! [3] (focused on common pitfallson the ontology itself); to provide a comprehensive overview of how a vocabularycomplies with current FAIR best practices for ontologies [4, 2].2 FOOPS! featuresFOOPS! is a web service and application that takes as input an OWL ontologyor SKOS thesauri and runs 24 different checks distributed across the FAIR di-mensions. These checks are based on the best practices and recommendations in[1], [4], [2], and can be summarized as follows:– Findable (9 checks): the service assesses whether the ontology URI is persis-tent, resolvable, has a resolvable version IRI, and whether that IRI is uniquefor that version. FOOPS! will also assess if minimum descriptive metadatais included (e.g., title, description, etc.) and whether the ontology prefix and",
        "publication_date": "2021-01-01",
        "authors": "Daniel Garijo, Óscar Corcho, María Poveda‐Villalón",
        "file_name": "20250512000310.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paper321/PDFs/20250512000310.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2980/paper321.pdf",
        "file_html": "html/20250512000310.html",
        "url": "http://ceur-ws.org/Vol-2980/paper321.pdf",
        "volume": "2980",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "International Semantic Web Conference (ISWC) 2021: Posters, Demos, and Industry Tracks",
        "year": "2021",
        "author": "Garijo, Daniel and Corcho, Oscar and Poveda-Villal{\\'o}n, Mar{\\i}a",
        "ENTRYTYPE": "article",
        "ID": "foops2021"
    },
    {
        "title": "Detecting common scientific workflow fragments using templates and execution provenance",
        "implementation_urls": [],
        "doi": "10.1145/2479832.2479848",
        "abstract": "ABSTRACTProvenance plays a major role when understanding and re-using the methods applied in a scientific experiment, as itprovides a record of inputs, the processes carried out andthe use and generation of intermediate and final results. Inthe specific case of in-silico scientific experiments, a largevariety of scientific workflow systems (e.g., Wings, Taverna,Galaxy, Vistrails) have been created to support scientists.All of these systems produce some sort of provenance aboutthe executions of the workflows that encode scientific ex-periments. However, provenance is normally recorded at avery low level of detail, which complicates the understandingof what happened during execution. In this paper we pro-pose an approach to automatically obtain abstractions fromlow-level provenance data by finding common workflow frag-ments on workflow execution provenance and relating themto templates. We have tested our approach with a datasetof workflows published by the Wings workflow system. Ourresults show that by using these kinds of abstractions wecan highlight the most common abstract methods used inthe executions of a repository, relating different runs andworkflow templates with each other.Categories and Subject DescriptorsI.2.6 [Learning]; I.2 [Artificial Intelligence]General TermsEXPERIMENTATIONKeywordsScientific workflow, provenance, abstraction, Wings1. INTRODUCTIONA scientific workflow can be seen as a digital instrumentthat allows scientists to encode a scientific experiment in thePermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee.K-CAP ’13, June 23-26, 2013, Banff, Canada.Copyright 2013 ACM 978-1-45-03-2102-0/13/06 ...$15.00.form of a set of computational or data manipulation steps.Scientific workflows play an important role in the repro-ducibility and replicability of scientific experiments, as wellas in repurposing and reusing results from previous experi-ments [13]. Given their importance in the research lifecycle,scientific workflows are beginning to be included in scientificpublications, together with datasets and other elements usedin the context of an experiment. At the same time, reposi-tories of workflows like myExperiment [24], Crowdlabs [19]or Galaxy [10] facilitate workflow publication, exchange andreuse. These repositories currently store thousands1 2 of",
        "publication_date": "2013-06-23",
        "authors": "Daniel Garijo, Óscar Corcho, Yolanda Gil",
        "file_name": "20250512001112.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/paperKCAP2013/PDFs/20250512001112.pdf",
        "pdf_link": "https://dgarijo.com/papers/paperKCAP2013.pdf",
        "file_html": "html/20250512001112.html",
        "funding": "USAFOSR FA9550-11-1-0104",
        "organization": "ACM",
        "url": "https://dgarijo.com/papers/paperKCAP2013.pdf",
        "pages": "33--40",
        "booktitle": "Proceedings of the seventh international conference on Knowledge capture",
        "year": "2013",
        "author": "Garijo, Daniel and Corcho, Oscar and Gil, Yolanda",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2013detecting"
    },
    {
        "title": "Quantifying reproducibility in computational biology: the case of the tuberculosis drugome",
        "implementation_urls": [],
        "doi": "10.1371/journal.pone.0080278",
        "abstract": "AbstractHow easy is it to reproduce the results found in a typical computational biology paper? Either through experience orintuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify thisdifficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertiseto domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimatingthe time required to reproduce each of the steps in the method described in the original paper and make them part of anexplicit workflow that reproduces the original results. Reproducing the method took several months of effort, and requiredusing new versions and new software that posed challenges to reconstructing and validating the results. The quantificationleads to ‘‘reproducibility maps’’ that reveal that novice researchers would only be able to reproduce a few of the steps in themethod, and that only expert researchers with advance knowledge of the domain would be able to reproduce the methodin its entirety. The workflow itself is published as an online resource together with supporting software and data. The paperconcludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and adesiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducingthe work of others from published papers, but reproducing work from one’s own laboratory.Citation: Garijo D, Kinnings S, Xie L, Xie L, Zhang Y, et al. (2013) Quantifying Reproducibility in Computational Biology: The Case of the TuberculosisDrugome. PLoS ONE 8(11): e80278. doi:10.1371/journal.pone.0080278Editor: Christos A. Ouzounis, The Centre for Research and Technology, Hellas, GreeceReceived September 18, 2012; Accepted October 10, 2013; Published November 27, 2013Copyright: � 2013 Garijo et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permitsunrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Funding: This research is sponsored by Elsevier Labs, the National Science Foundation with award number -0 , the Air Force Office of ScientificResearch with award number FA9550-11-1-0104, internal funds from the University of Southern California’s Information Sciences Institute and from the Universityof California, San Diego, and by a Formacistudy design, data collection and analysis, decision to publish, or preparation of the manuscript.Competing Interests: The research presented here has been sponsored partly by Elsevier Labs. This does not alter the authors’ adherence to all the PLOS ONEpolicies on sharing data and materials.* E-mail: pbourne@ucsd.edu (PEB); gil@isi.edu (YG)IntroductionComputation is now an integral part of the biological scienceseither applied as a technique or as a science in its own right -bioinformatics. As a technique, software becomes an instrument toanalyze data and uncover new biological insights. By reading thepublished article describing these insights, another researcherhopes to understand what computations were carried out, replicatethe software apparatus originally used and reproduce theexperiment. This is rarely the case without significant effort, andsometimes impossible without asking the original authors. In short,reproducibility in computational biology is aspired to, but rarelyachieved. This is unfortunate since the quantitative nature of thescience makes reproducibility more obtainable than in cases whereexperiments are qualitative and hard to describe explicitly.An intriguing possibility where potential quantification exists isto extend articles through the inclusion of scientific workflows thatrepresent computations carried out to obtain the published results,thereby capturing data analysis methods explicitly [1]. This wouldmake scientific results more reproducible because articles wouldhave not only a textual description of the computational processdescribed in the article but also a workflow that, as acomputational artifact, could be analyzed and re-run automati-cally. Consequently, workflows can make scientists more produc-",
        "publication_date": "2013-11-27",
        "authors": "Daniel Garijo, Sarah Kinnings, Li Xie, Lei Xie, Yinliang Zhang, Philip E. Bourne, Yolanda Gil",
        "file_name": "20250512001137.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/plos/PDFs/20250512001137.pdf",
        "pdf_link": "https://dgarijo.com/papers/plos.pdf",
        "file_html": "html/20250512001137.html",
        "url": "https://dgarijo.com/papers/plos.pdf",
        "pages": "e80278",
        "number": "11",
        "volume": "8",
        "publisher": "Public Library of Science",
        "journal": "PloS one",
        "year": "2013",
        "author": "Garijo, Daniel and Kinnings, Sarah and Xie, Li and Xie, Lei and Zhang, Yinliang and Bourne, Philip E and Gil, Yolanda",
        "ENTRYTYPE": "article",
        "ID": "garijo2013quantifying"
    },
    {
        "title": "Publishing Linked Data-There is no One-Size-Fits-All Formula",
        "implementation_urls": [],
        "abstract": "Abstract. Publishing Linked Data is a process that involves several de-sign decisions and technologies. Although some initial guidelines havebeen already provided by Linked Data publishers, these are still far fromcovering all the steps that are necessary (from data source selection topublication) or giving enough details about all these steps, technolo-gies, intermediate products, etc. Furthermore, given the variety of datasources from which Linked Data can be generated, we believe that it ispossible to have a single and unified method for publishing Linked Data,but we should rely on different techniques, technologies and tools forparticular datasets of a given domain. In this paper we present a generalmethod for publishing Linked Data and the application of the methodto cover different sources from different domains.Key words: Linked Data, Publishing Linked Data1 Introduction and MotivationSo far, Linked Data principles and practices are being adopted by an increasingnumber of data providers, getting as result a global data space on the Webcontaining hundreds of LOD datasets [3]. Moreover, Linked Data generationand publication does not follow a set of common and clear guidelines to scaleout the generation and publication of Linked Data.Furthermore, given the variety of data sources from which Linked Data canbe generated, we believe that it is possible to have a single and unified method forpublishing Linked Data, but we should rely on different techniques, technologiesand tools for particular datasets of a given domain. The rest of the paper isorganized as follows: Section 2 introduces our method for publishing linked data,then, Section 3 describes the application of the method to cover different sourcesfrom different domains, and finally, Section 4 presents the conclusions.2 A Method for Publishing Linked DataIn previous work [6] we have already presented the Linked Data GenerationProcess as one that follows an iterative incremental life cycle model. In thatwork we proposed a method that covers the following activities (1) specification,for analyzing and selecting the data sources, (2) modelling, for developing themodel that represents the information domain of the data sources, (3) genera-tion, for transforming the data sources into RDF, (4) linking, for creating linksbetween the RDF resources, of our dataset, with other RDF resources, of ex-ternal datasets, (5) publication, for publishing the model, RDF resources andlinks generated, and (6) exploitation, for developing applications that consumethe dataset. Each activity is decomposed into one or more tasks, and some tech-niques, technologies and tools are provided for carrying out them. It is worthmentioning that the order of the activities and tasks might be changed base onparticular needs of the data owners and publishers. Moreover, we are continu-ously getting feedback about this method, and therefore, we are improving itconstantly. Figure 1 depicts the main activities.Fig. 1. Main Activities for Publishing Linked Data (extended from [6])3 Application of the Method to Different DomainsIn this section we present the application of the method to cover different sourcesfrom different domains.3.1 GeoLinkedDataGeoLinkedData1 aims at enriching the Web of Data with Spanish geospatialdata into the context of INSPIRE themes2. This initiative has started off by1 http://geo.linkeddata.es/",
        "publication_date": "2012-06-01",
        "authors": "Boris Villazón-Terrazas, Daniel Vila-Suero, Daniel Garijo, Luis M. Vilches‐Blázquez, María Poveda‐Villalón, José Ferrater Mora, Óscar Corcho, Asunción Gómez‐Pérez",
        "file_name": "20250512000922.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/poster5/PDFs/20250512000922.pdf",
        "pdf_link": "https://dgarijo.com/papers/poster5.pdf",
        "file_html": "html/20250512000922.html",
        "url": "https://dgarijo.com/papers/poster5.pdf",
        "year": "2012",
        "author": "Villaz{\\'o}n-Terrazas, Boris and Vila-Suero, Daniel and Garijo, Daniel and Vilches-Bl{\\'a}zquez, Luis M and Poveda-Villal{\\'o}n, Mar{\\'i}a and Mora, Jos{\\'e} and Corcho, Oscar and G{\\'o}mez-P{\\'e}rez, Asunci{\\'o}n",
        "ENTRYTYPE": "article",
        "ID": "villazon2012publishing"
    },
    {
        "title": "Ontology Engineering and the FAIR principles: A Gap Analysis",
        "implementation_urls": [],
        "abstract": "AbstractOntologies and vocabularies play a key role when standardising, organizing and integrating data fromheterogeneous data sources into Knowledge Graphs. In order to develop ontologies, different engineeringmethodologies have been proposed throughout the years, whose application resulted in thousands ofsemantic artefacts (taxonomies, vocabularies and ontologies) in a wide range of domains. But how toensure that ontologies follow the Findable, Accessible, Interoperable and Reusable principles (FAIR) fromtheir inception? In this paper, we review existing guidelines to help make ontologies FAIR and mapthem to the ontology development lifecycle activities. Our analysis outlines the current gaps, where noguidelines exist for ontologies to become FAIRbyDesign.KeywordsOntology Engineering, FAIR principles, Semantic Artifacts, Ontologies, Vocabularies1. IntroductionOntologies and vocabularies play a key role in data integration by defining the structure, guidingthe construction, and validating Knowledge Graphs. Ontologies are widely used in multipledomains, ranging from Bioinformatics [1] and Astrophysics [2] to Smart Cities [3] or Webcontent annotation [4].A number of ontology engineering methodologies have been proposed by researchers throughthe years in order to build ontologies [5] [6] [7] [8] [9] [10] [11] [12] [13]. These methodologiesdefine the steps and activities needed to gather ontology requirements, discuss with domainexperts, reuse existing vocabularies, validate the results, etc. Among them, Linked Open TermsFOAM2024: FAIR principles for Ontologies and Metadata in Knowledge Management, July 15–19, 2024, Enschede,Netherlands∗Corresponding author.†These authors contributed equally.Envelope-Open m.poveda@upm.es (M. Poveda-Villalón); daniel.garijo@upm.es (D. Garijo);alejandra.gonzalez-beltran@stfc.ac.uk (A. N. Gonzalez-Beltran); clement.jonquet@inrae.fr (C. Jonquet);ylefranc@esciencefactory.com (Y. L. Franc)GLOBE https://agbeltran.github.io/ (A. N. Gonzalez-Beltran)Orcid 0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0003-0454-7145 (D. Garijo); 0000-0003-3499-8262(A. N. Gonzalez-Beltran); 0000-0002-2404-1582 (C. Jonquet); 0000-0003-4631-418X (Y. L. Franc)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).mailto:m.poveda@upm.esmailto:daniel.garijo@upm.esmailto:alejandra.gonzalez-beltran@stfc.ac.ukmailto:clement.jonquet@inrae.frmailto:ylefranc@esciencefactory.comhttps://agbeltran.github.io/https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0003-0454-7145https://orcid.org/0000-0003-3499-8262https://orcid.org/0000-0002-2404-1582https://orcid.org/0000-0003-4631-418Xhttps://creativecommons.org/licenses/by/4.0(LOT) [13] is the only one addressing an online publication and maintenance activity, key forsustaining the obtained results.With the growing adoption of the Findable, Accessible, Interoperable and Reusable (FAIR)principles for data [14], different efforts have proposed guidelines to apply FAIR in ontologiesand vocabularies [15] [16] [17] [18] [19]. However, no alignment between ontology developmentmethodologies and the guidelines/recommendations for FAIR ontologies has been developed",
        "publication_date": "2024-07-15",
        "authors": "María Poveda‐Villalón, Daniel Garijo, Alejandra González-Beltrán, Clément Jonquet, Yann Le Franc",
        "file_name": "20250511232637.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/poveda_et_al_24/PDFs/20250511232637.pdf",
        "pdf_link": "https://dgarijo.com/papers/poveda_et_al_24.pdf",
        "file_html": "html/20250511232637.html",
        "year": "2024",
        "url": "https://dgarijo.com/papers/poveda_et_al_24.pdf",
        "booktitle": "To appear in Proceedings of the FAIR principles for Ontologies and Metadata in Knowledge Management (FOAM 2024), Co-located with FOIS 2024 (JOWO Workshops)",
        "author": "Poveda-Villal{\\'o}n, Mar{\\'i}a and Garijo, Daniel and Gonzalez-Beltran, Alejandra N and Jonquet, Clement and Le Franc, Yann",
        "ENTRYTYPE": "article",
        "ID": "poveda2024ontology"
    },
    {
        "title": "A Template-Based Approach for Annotating Long-Tail Datasets",
        "implementation_urls": [],
        "abstract": "Abstract. An increasing amount of data is shared on the Web throughheterogeneous spreadsheets and CSV files. In order to homogenize andquery these data, the scientific community has developed Extract, Trans-form and Load (ETL) tools and services that help making these files ma-chine readable in Knowledge Graphs (KGs). However, tabular data maybe complex; and the level of expertise required by existing ETL toolsmakes it difficult for users to describe their own data. In this paper wepropose a simple annotation schema to guide users when transformingcomplex tables into KGs. We have implemented our approach by extend-ing T2WML, a table annotation tool designed to help users annotatetheir data and upload the results to a public KG. We have evaluated oureffort with six non-expert users, obtaining promising preliminary results.Keywords: Dataset annotation · Metadata · Knowledge Graph.1 IntroductionAn increasing amount of data is shared on the Web by multiple organizationsusing Excel and CSV formats. Content creators usually prefer to use tabulardata because it is simple to generate, manipulate and visualize by humans; andthere is a significant number of tools to help explore and edit the contents ofspreadsheets. These data need to be properly understood by others, and hencedocumentation (e.g., variables captured, provenance, usage notes, etc.) is usuallyincluded in auxiliary files or the spreadsheets themselves. As a result, many ofthese spreadsheets have comments, clarifications, notes and references to otherfiles explaining how to interpret the information contained in them.In order to convert tabular data to a machine readable format, the SemanticWeb community has created Extract, Transform and Load (ETL) tools (e.g.,[4]) and mapping languages (e.g., [1, 5]) that help translating spreadsheets intoKnowledge Graphs. However, these tools and languages require significant exper-tise when transforming heterogeneous tabular data with comments, incompletevalues or columns that are interrelated to each other, making it difficult fordomain experts to integrate their own datasets with existing KGs.? Copyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 Garijo et al.In this paper we describe an approach to help non-experts transform theirdata into a structured representation through dataset annotations. Our contri-butions include 1) a dataset annotation schema that helps generating templatesfor translating datasets into KGs; 2) an extension of the T2WML dataset an-notation tool [6] to accommodate the proposed schema; and 3) an approach toupload annotated datasets to a registry once the dataset annotation is complete.In order to assess our approach, we conducted a preliminary evaluation with 6users unfamiliar with Knowledge Representation or Semantic Web technologies,who were able to describe and integrate their annotated datasets as a KG.2 Challenges in Long-Tail Dataset AnnotationWe focus on those datasets that are not straightforward to map into a structuredrepresentation. Consider for example Table 1, which depicts the food prices indifferent regions of Ethiopia at different points in time. The table has a time seriesfor the price value of different items at different dates, a repeated column withthe item being described (ignore), the item category and different informationabout the region where that item was produced. The dataset has also somemissing values and labels marked as ”unknown”, which we may want to skip.",
        "publication_date": "2020-01-01",
        "authors": "Daniel Garijo, Ke-Thia Yao, Amandeep Singh, Pedro Szekely",
        "file_name": "20250512000631.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/profiles2020-paper-3/PDFs/20250512000631.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2722/profiles2020-paper-3.pdf",
        "file_html": "html/20250512000631.html",
        "url": "http://ceur-ws.org/Vol-2722/profiles2020-paper-3.pdf",
        "pages": "97--102",
        "volume": "2722",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "Joint Proceedings of Workshops AI4LEGAL2020, NLIWOD, {PROFILES} 2020, QuWeDa 2020 and {SEMIFORM2020} Colocated with the 19th International Semantic Web Conference {(ISWC} 2020), Virtual Conference, November, 2020",
        "year": "2020",
        "author": "Daniel Garijo and Ke{-}Thia Yao and Amandeep Singh and Pedro A. Szekely",
        "ENTRYTYPE": "inproceedings",
        "ID": "DBLP:conf/semweb/GarijoYSS20"
    },
    {
        "title": "A workflow PROV-corpus based on taverna and wings",
        "implementation_urls": [
            {
                "identifier": "https://github.com/provbench/Wf4Ever-PROV",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/prov-bench-paper/PDFs/20250512001014.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "SUMMARY We describe a corpus1 of provenance traces that we have collected by executing 120 real world scientific workflows."
                    }
                ]
            }
        ],
        "doi": "10.1145/2457317.2457376",
        "abstract": "workflows: Abstractions, standards, and linked data. InProceedings of the 6th workshop on Workflows insupport of large-scale science, pages 47–56, Seattle,2011. ACM.[3] Y. Gil, V. Ratnakar, J. Kim, et al. Wings: Intelligentworkflow-based design of computational experiments.IEEE Intelligent Systems, 26(1):62–72, 2011.[4] T. Lebo, S. Sahoo, D. McGuinness, K. Belhajjame,J. Cheney, D. Corsar, D. Garijo, S. Soiland-Reyes,S. Zednik, and J. Zhao. Prov-o: The prov ontology.Technical report, 2012.[5] P. Missier, S. Soiland-Reyes, S. Owen, W. Tan,A. Nenadic, I. Dunlop, A. Williams, T. Oinn, andC. Goble. Taverna, reloaded. In M Gertz, T Hey, andB Ludaescher, editors, Procs. SSDBM 2010,Heidelberg, Germany, 2010.",
        "publication_date": "2013-03-18",
        "authors": "Khalid Belhajjame, Jun Zhao, Daniel Garijo, Aleix Garrido, Stian Soiland‐Reyes, Pinar Alper, Óscar Corcho",
        "file_name": "20250512001014.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/prov-bench-paper/PDFs/20250512001014.pdf",
        "pdf_link": "https://dgarijo.com/papers/prov-bench-paper.pdf",
        "file_html": "html/20250512001014.html",
        "organization": "ACM",
        "url": "https://dgarijo.com/papers/prov-bench-paper.pdf",
        "pages": "331--332",
        "booktitle": "Proceedings of the Joint EDBT/ICDT 2013 Workshops",
        "year": "2013",
        "author": "Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Garrido, Aleix and Soiland-Reyes, Stian and Alper, Pinar and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "belhajjame2013workflow"
    },
    {
        "title": "PSM-Flow: Probabilistic Subgraph Mining for Discovering Reusable Fragments in Workflows",
        "implementation_urls": [],
        "doi": "10.1109/WI.2018.00-93",
        "abstract": "Abstract—Scientific workflows define computational processesneeded for carrying out scientific experiments. Existing workflowrepositories contain hundreds of scientific workflows, wherescientists can find materials and knowledge to facilitate workflowdesign for running related experiments. Identifying reusablefragments in growing workflow repositories has become in-creasingly important. In this paper, we present PSM-Flow, aprobabilistic subgraph mining algorithm designed to discovercommonly occurring fragments in a workflow corpus using amodified version of the Latent Dirichlet Allocation algorithm. Theproposed model encodes the geodesic distance between workflowsteps into the model for implicitly modeling fragments. PSM-Flow captures variations of frequent fragments while maintainingits space complexity bounded polynomially, as it requires nocandidate generation. We applied PSM-Flow to three real-worldscientific workflow datasets containing more than 750 workflowsfor neuroimaging analysis. Our results show that PSM-Flowoutperforms three state of the art frequent subgraph miningtechniques. We also discuss other potential future improvementsof the proposed method.I. INTRODUCTIONScientific workflows describe computational experimentswhich typically involve computational steps, along with thedatasets used and generated by those steps. Scientific work-flows are created in workflow systems that manage theirexecution with the required computational resources [13]. Rep-resenting workflows explicitly improves the reproducibility ofscientific experiments [12].Scientific workflow repositories contain collections ofrecorded scientific workflows. [23] Users may explore andreuse workflows created by others to facilitate the developmentof their computational experiments. While one can directlyreuse an existing workflow, only a portion or fragment of aworkflow is often reused. In addition, identifying commonlyused fragments of workflows facilitates overviewing and ex-ploring the contents of a workflow repository. [11]In [10], the authors formulated reusable workflow fragmentidentification as a frequent subgraph mining problem and ap-plied frequent subgraph mining algorithms (FSM) to detect thesubgraphs with high support count (number of occurrences)as candidate fragments. However, frequent subgraph miningtechniques present several limitations. First, these techniquestypically involve a candidate fragment generation process anda subgraph isomorphism test. Both present a time complex-ity (combinatorial exploration of candidate fragments) and aspace complexity (a large number of candidate subgraphs aregenerated in memory) that are exponential in the worst case.Second, frequent fragments may appear in different workflowswith small variations (e.g., with changes in node labels, orwith an additional node). Conventional frequent subgraph",
        "publication_date": "2018-12-01",
        "authors": "Chin Wang Cheong, Daniel Garijo, Cheung Kwok Wai, Yolanda Gil",
        "file_name": "20250512002640.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/psmFlow/PDFs/20250512002640.pdf",
        "pdf_link": "https://dgarijo.com/papers/psmFlow.pdf",
        "file_html": "html/20250512002640.html",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/bib/conf/webi/CheongGCG18",
        "crossref": "DBLP:conf/webi/2018",
        "url": "https://dgarijo.com/papers/psmFlow.pdf",
        "pages": "166--173",
        "booktitle": "2018 {IEEE/WIC/ACM} International Conference on Web Intelligence, {WI} 2018, Santiago, Chile, December 3-6, 2018",
        "year": "2018",
        "author": "Chin Wang Cheong and Daniel Garijo and Kwok{-}Wai Cheung and Yolanda Gil",
        "ENTRYTYPE": "inproceedings",
        "ID": "DBLP:conf/webi/CheongGCG18"
    },
    {
        "title": "{DockerPedia}: {A} {Knowledge} {Graph} of {Software} {Images} and {Their} {Metadata}",
        "implementation_urls": [],
        "abstract": "An increasing amount of researchers use software images to capture the requirements and code dependencies needed to carry out computational experiments. Software images preserve the computational environment required to execute a scientific experiment and have become a crucial asset for reproducibility. However, software images are usually not properly documented and described, making it challenging for scientists to find, reuse and understand them. In this paper, we propose a framework for automatically describing software images in a machine-readable manner by (i) creating a vocabulary to describe software images; (ii) developing an annotation framework designed to automatically document the underlying environment of software images and (iii) creating DockerPedia, a Knowledge Graph with over 150,000 annotated software images, automatically described using our framework. We illustrate the usefulness of our approach in finding images with specific software dependencies, comparing similar software images, addressing versioning problems when running computational experiments; and flagging problems with vulnerable software dependencies.",
        "file_name": "20250511235549.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/reproducibility_docker/PDFs/20250511235549.pdf",
        "pdf_link": "https://dgarijo.com/papers/reproducibility_docker.pdf",
        "file_html": "html/20250511235549.html",
        "pages": "71--89",
        "year": "2022",
        "month": "January",
        "author": "Osorio, Maximiliano and Buil-Aranda, Carlos and Santana-Perez, Idafen and Garijo, Daniel",
        "journal": "International Journal of Software Engineering and Knowledge Engineering",
        "urldate": "2022-03-02",
        "number": "01",
        "language": "en",
        "doi": "10.1142/S0218194022500036",
        "url": "https://dgarijo.com/papers/reproducibility_docker.pdf",
        "shorttitle": "{DockerPedia}",
        "issn": "0218-1940, 1793-6403",
        "volume": "32",
        "ENTRYTYPE": "article",
        "ID": "osorio_dockerpedia_2022"
    },
    {
        "title": "The Research Object Suite of Ontologies: Sharing and Exchanging Research Data and Methods on the Open Web",
        "implementation_urls": [],
        "abstract": "AbstractResearch in life sciences is increasingly being conducted in a digital and online environment. In particular, lifescientists have been pioneers in embracing new computational tools to conduct their investigations. To support thesharing of digital objects produced during such research investigations, we have witnessed in the last few years theemergence of specialized repositories, e.g., DataVerse and FigShare. Such repositories provide users with the meansto share and publish datasets that were used or generated in research investigations. While these repositories haveproven their usefulness, interpreting and reusing evidence for most research results is a challenging task. Additionalcontextual descriptions are needed to understand how those results were generated and/or the circumstances underwhich they were concluded. Because of this, scientists are calling for models that go beyond the publication ofdatasets to systematically capture the life cycle of scientific investigations and provide a single entry point to accessthe information about the hypothesis investigated, the datasets used, the experiments carried out, the results of theexperiments, the conclusions that were derived, the people involved in the research, etc.In this paper we present the Research Object (RO) suite of ontologies, which provide a structured container toencapsulate research data and methods along with essential metadata descriptions. Research Objects are portableunits that enable the sharing, preservation, interpretation and reuse of research investigation results. The ontologieswe present have been designed in the light of requirements that we gathered from life scientists. They have been builtupon existing popular vocabularies to facilitate interoperability. Furthermore, we have developed tools to support thecreation and sharing of Research Objects, thereby promoting and facilitating their adoption.Key words: Scholarly communication, Semantic Web, Ontologies, Provenance, Scientific Workflow, Scientific Methods.? The Research Object Ontologies have been devel-oped under the aegis of the EU Wf4Ever project (http://www.wf4ever-project.org).Preprint submitted to Arxiv 4 February 2014arXiv:1401.4307v2  [cs.DL]  3 Feb 20141. IntroductionResearch in life sciences is increasingly digital[25,11]. Life scientists have been pioneers in embrac-ing new computational tools to conduct their inves-tigations. For example, they have adopted scientificworkflows as a means for designing and automatingthe execution of their in silico experiments [24].Life scientists are also one of the main adopters and",
        "publication_date": "2014-01-17",
        "authors": "Khalid Belhajjame, Jun Zhao, Daniel Garijo, Kristina Hettne, Raúl Palma, Óscar Corcho, José-Manuel Gómez-Pérez, Sean Bechhofer, Graham Klyne, Carole Goble",
        "file_name": "20250512001150.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/ro/PDFs/20250512001150.pdf",
        "pdf_link": "https://dgarijo.com/papers/ro.pdf",
        "file_html": "html/20250512001150.html",
        "url": "https://dgarijo.com/papers/ro.pdf",
        "volume": "abs/1401.4307",
        "journal": "CoRR",
        "year": "2014",
        "author": "Khalid Belhajjame and Jun Zhao and Daniel Garijo and Kristina M. Hettne and Ra{\\'{u}}l Palma and {\\'{O}}scar Corcho and Jos{\\'{e}} Manu{\\'{e}}l G{\\'{o}}mez{-}P{\\'{e}}rez and Sean Bechhofer and Graham Klyne and Carole A. Goble",
        "ENTRYTYPE": "article",
        "ID": "DBLP:journals/corr/BelhajjameZGHPCGBKG14"
    },
    {
        "title": "Using a suite of ontologies for preserving workflow-centric research objects",
        "implementation_urls": [],
        "abstract": "[35] Carl Lagoze, Herbert Van de Sompel, ORE specification—abstract data model. http://www.openarchives.org/ore/1.0/datamodel.html (Accessed on February 28, 2014).[36] Paolo Ciccarese, Marco Ocana, Leyla J. Garcia Castro, Sudeshna Das, Tim Clark, An open annotation ontology for science onweb 3.0, J. Biomed. Semant. 2 (Suppl 2) (2011)S4.[37] K. Wolstencroft, R. Haines, D. Fellows, et al., The taverna workflow suite: designing and executing workflows of web services on the desktop, web or in the cloud, Nucl.Acids Res. (2013).[38] Yolanda Gil, Varun Ratnakar, Jihie Kim, et al., Wings: Intelligent workflow-based design of computational experiments, IEEE Intell. Syst. 26 (1) (2011) 62–72.[39] Jeremy Goecks, Anton Nekrutenko, James Taylor, Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational researchin the life sciences, Genome Biol. 11 (8) (2010) R86.[40] Carl Lagoze, Herbert Van de Sompel, Ore specification—vocabulary. http://www.openarchives.org/ore/1.0/vocabulary.html (Accessed on February 28, 2014).[41] Rudolf Mayer, Andreas Rauber, Martin Alexander Neumann, John Thomson, Gonçalo Antunes, Preserving scientific processes from design to publications, in: Theoryand Practice of Digital Libraries, Springer, 2012, pp. 113–124.[42] Angela Dappert, Sébastien Peyrard, Carol C.H. Chou, Janet Delve, Describing and preserving digital object environments, New Rev. Inf. Netw. 18 (2) (2013) 106–173.[43] Stian Soiland-Reyes, Matthew Gamble, Robert Haines, Research Object Bundle 1.0. November 2014. http://dx.doi.org/10.5281/zenodo.12586.[44] Steven P. Callahan, Juliana Freire, Emanuele Santos, et al., Vistrails: Visualization meets data management, in: ACM SIGMOD, ACM Press, 2006, pp. 745–747.[45] Khalid Belhajjame, Annotating the behavior of scientificmodules using data examples: A practical approach, in: Proc. of International Conference on ExtendingDatabaseTechnology, 2014, pp. 726–737.[46] Dagmar Krefting, Tristan Glatard, Vladimir Korkhov, Johan Montagnat, Silvia Olabarriaga, Enabling grid interoperability at workflow level. Grid Workflow Workshop2011, 2011.[47] Paolo Missier, Saumen Dey, Khalid Belhajjame, Víctor Cuevas-Vicenttín, Bertram Ludäscher, D-PROV: extending the PROV provenance model with workflow structure,in: Computing Science, Newcastle University, 2013.[48] Daniel Garijo, Yolanda Gil, A new approach for publishing workflows: Abstractions, standards, and linked data, in: Proceedings of the 6th Workshop on Workflows inSupport of Large-scale Science, ACM, 2011, pp. 47–56.[49] Alejandra Gonzalez-Beltran, Peter Li, Jun Zhao, Maria Susana Avila-Garcia, Marco Roos, Mark Thompson, Eelke van der Horst, Rajaram Kaliyaperumal, Ruibang Luo,Lee Tin-Lap, Lam Tak-wah, Scott C. Edmunds, Susanna-Assunta Sansone, Philippe Rocca-Serra, From peer-reviewed to peer-reproduced: a role for data standards,models and computational workflows in scholarly publishing, in: bioRxiv, 2014.[50] Thomas Russ, Cartic Ramakrishnan, Eduard Hovy, Mihail Bota, Gully Burns, Knowledge engineering tools for reasoning with scientific observations and interpretations:a neural connectivity use case, BMC Bioinform. 12 (1) (2011) 351.[51] Brian Matthews, Shoaib Sufi, Damian Flannery, Laurent Lerusse, Tom Griffin, Michael Gleaves, Kerstin Kleese, Using a core scientific metadata model in large-scalefacilities, Int. J. Digit. Curation 5 (1) (2010) 106–118.[52] Science and Technology Facilities Council. Isis. http://www.isis.stfc.ac.uk/index.html. Accessed on the 20th of June 2014.[53] Science and Technology Facilities Council. Diamond light source. http://www.diamond.ac.uk. Accessed on the 20th of June 2014.[54] J. Hunter, Scientific publication packages: A selective approach to the communication and archival of scientific output, Int. J. Digit. Curation 1 (1) (2006).[55] Fernando Chirigati, Dennis Shasha, Juliana Freire, Packing experiments for sharing and publication, in: Proceedings of the 2013 ACM SIGMOD International Conferenceon Management of Data, 2013, pp. 977–980.[56] Quan Pham, Tanu Malik, Ian Foster, Roberto Di Lauro, Raffaele Montella, Sole: linking research papers with science objects, in: Provenance and Annotation of Data andProcesses, Springer, 2012, pp. 203–208.[57] Victoria Stodden, Christophe Hurlin, Christophe Pérignon, Runmycode.org: a novel dissemination and collaboration platform for executing published computationalresults, in: Proc. of IEEE 8th International Conference on e-Science, IEEE, 2012, pp. 1–8.[58] Pieter Van Gorp, Steffen Mazanek, Share: a web portal for creating and sharing executable research papers, Proc. Comput. Sci. 4 (2011) 589–597.[59] Terri K. Attwood, Douglas B. Kell, Philip McDermott, James Marsh, Stephen Pettifer, David Thorne, Utopia documents: linking scholarly literature with research data,Bioinformatics 26 (18) (2010) 568–574.[60] Friedrich Leisch, Sweave: Dynamic generation of statistical reports using literate data analysis, in: Compstat, Springer, 2002, pp. 575–580.[61] Fernando Pérez, Brian E. Granger, IPython: a system for interactive scientific computing, Comput. Sci. Eng. 9 (3) (2007) 21–29.[62] Marian Petre, Greg Wilson, Plos/mozilla scientific code review pilot: Summary of findings, 2013.[63] M. Crosas, The dataverse network: An open-source application for sharing, discovering and preserving data, D-Lib. Mag. 17 (1/2) (2011).http://refhub.elsevier.com/S1570-8268(15)00004-9/sbref29http://refhub.elsevier.com/S1570-8268(15)00004-9/sbref30http://refhub.elsevier.com/S1570-8268(15)00004-9/sbref31http://refhub.elsevier.com/S1570-8268(15)00004-9/sbref33http://refhub.elsevier.com/S1570-8268(15)00004-9/sbref34",
        "publication_date": "2009-01-01",
        "authors": "Elsevier Sdol",
        "file_name": "20250512001249.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/ros/PDFs/20250512001249.pdf",
        "pdf_link": "https://dgarijo.com/papers/ros.pdf",
        "file_html": "html/20250512001249.html",
        "url": "https://dgarijo.com/papers/ros.pdf",
        "doi": "10.1016/j.websem.2015.01.003",
        "pages": "16--42",
        "volume": "32",
        "publisher": "Elsevier",
        "journal": "Web Semantics: Science, Services and Agents on the World Wide Web",
        "year": "2015",
        "author": "Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and G{\\'o}mez-P{\\'e}rez, Jos{\\'e} Manuel and Bechhofer, Sean and others",
        "ENTRYTYPE": "article",
        "ID": "belhajjame2015using"
    },
    {
        "title": "Packaging research artefacts with RO-Crate",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ResearchObject/ro-crate",
                "type": "git",
                "paper_frequency": 7,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/ro_crate_data_science/PDFs/20250511235719.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "See https://github.com/ResearchObject/ro-crate/issues/82."
                    }
                ]
            },
            {
                "identifier": "https://github.com/researchobject/ro-crate-py",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "README_TEXT",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_README",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.5146227",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.3233/DS-210053",
        "arxiv": "2108.06503",
        "abstract": "Abstract. An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets,software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to processby automated systems. In this paper we introduce RO-Crate, an open, community-driven, and lightweight approach to packagingresearch artefacts along with their metadata in a machine readable manner. RO-Crate is based on Schema.org annotations inJSON-LD, aiming to establish best practices to formally describe metadata in an accessible and practical way for their use in awide variety of situations.An RO-Crate is a structured archive of all the items that contributed to a research outcome, including their identifiers,provenance, relations and annotations. As a general purpose packaging approach for data and their metadata, RO-Crate is usedacross multiple areas, including bioinformatics, digital humanities and regulatory sciences. By applying “just enough” LinkedData standards, RO-Crate simplifies the process of making research outputs FAIR while also enhancing research reproducibility.An RO-Crate for this article1 is archived at https://doi.org/10.5281/zenodo.5146227.Keywords: Data publishing, data packaging, FAIR, Linked Data, metadata, reproducibility, research object1. IntroductionThe move towards Open Science has increased the need and demand for the publication of artefactsof the research process [104]. This is particularly apparent in domains that rely on computational ex-periments; for example, the publication of software, datasets and records of the dependencies that suchexperiments rely on [113].It is often argued that the publication of these assets, and specifically software [80], workflows [55] anddata, should follow the FAIR principles [123]; namely, that they are Findable, Accessible, Interoperableand Reusable. These principles are agnostic to the implementation strategy needed to comply with them.Hence, there has been an increasing amount of work in the development of platforms and specificationsthat aim to fulfil these goals [91].Important examples include data publication with rich metadata (e.g. Zenodo [40]), domain-specificdata deposition (e.g. PDB [16]) and following practices for reproducible research software [101] (e.g.use of containers). While these platforms are useful, experience has shown that it is important to putgreater emphasis on the interconnection of the multiple artefacts that make up the research process [71].The notion of Research Objects [12] (RO) was introduced to address this connectivity, providingsemantically rich aggregations of (potentially distributed) resources with a layer of structure over aresearch study; this is then to be delivered in a machine-readable format.A Research Object combines the ability to bundle multiple types of artefacts together, such as spread-sheets, code, examples, and figures. The RO is augmented with annotations and relationships that de-scribe the artefacts’ context (e.g. a CSV being used by a script, a figure being a result of a workflow).*Corresponding author. E-mail: soiland-reyes@manchester.ac.uk.1https://w3id.org/ro/doi/10.5281/zenodo.5146227https://orcid.org/0000-0002-4763-3943https://orcid.org/0000-0002-1112-1292https://doi.org/10.5281/zenodo.5146227mailto:soiland-reyes@manchester.ac.ukhttps://w3id.org/ro/doi/10.5281/zenodo.5146227CORRECTED  PROOFS. Soiland-Reyes et al. / Packaging research artefacts with RO-Crate 3This notion of ROs provides a compelling vision as an approach for implementing FAIR data. How-ever, existing Research Object implementations require a large technology stack [14], are typically tai-lored to a particular platform and are also not easily usable by end-users.To address this gap, a new community came together [23] to develop RO-Crate – an approach topackage and aggregate research artefacts with their metadata and relationships. The aim of this paperis to introduce RO-Crate and assess it as a strategy for making multiple types of research artefacts FAIR.Specifically, the contributions of this paper are as follows:1. An introduction to RO-Crate, its purpose and context;",
        "publication_date": "2022-01-04",
        "authors": "Stian Soiland‐Reyes, Peter Sefton, Mercè Crosas, Leyla Jael Castro, Frederik Coppens, José M. Fernández, Daniel Garijo, Björn Grüning, M. Rosa, Simone Leo, Eoghan Ó Carragáin, Marc Portier, A. Trisovic, RO-Crate Community, Paul Groth, Carole Goble",
        "file_name": "20250511235719.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/ro_crate_data_science/PDFs/20250511235719.pdf",
        "pdf_link": "https://dgarijo.com/papers/ro_crate_data_science.pdf",
        "file_html": "html/20250511235719.html",
        "year": "2022",
        "volume": "Pre-press",
        "pages": "1--42",
        "url": "https://dgarijo.com/papers/ro_crate_data_science.pdf",
        "journal": "Data Science",
        "author": "Soiland-Reyes, Stian and Sefton, Peter and Crosas, Merc{\\`e} and Castro, Leyla Jael and Coppens, Frederik and Fern{\\'a}ndez, Jos{\\'e} M and Garijo, Daniel and Gr{\\\"u}ning, Bj{\\\"o}rn and La Rosa, Marco and Leo, Simone and others",
        "ENTRYTYPE": "article",
        "ID": "soiland2021packaging"
    },
    {
        "title": "Linking Abstract Plans of Scientific Experiments to their Corresponding Execution Traces",
        "implementation_urls": [],
        "publication_date": "2019-10-21",
        "authors": "Milan Marković, Daniel Garijo, Peter Edwards",
        "file_name": "20250512002835.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/short1/PDFs/20250512002835.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2526/short1.pdf",
        "file_html": "html/20250512002835.html",
        "url": "http://ceur-ws.org/Vol-2526/short1.pdf",
        "issn": "1613-0073",
        "volume": "2526",
        "address": "Los Angeles, California",
        "booktitle": "Proceedings of the Third Workshop on Capturing Scientific Knowledge (SciKnow 2019), held in conjunction with the 2019 ACM International Conference on Knowledge Capture (K-CAP)",
        "year": "2019",
        "author": "Milan Markovic and Daniel Garijo and Peter Edwards",
        "ENTRYTYPE": "inproceedings",
        "ID": "markovic-et-al-sciknow2019"
    },
    {
        "title": "WDPlus: Leveraging Wikidata to Link and Extend Tabular Data",
        "implementation_urls": [],
        "abstract": "ABSTRACTScientific observations and other open data are usually made avail-able online in a tabular manner as CSVs and spreadsheets. However,users of these data face three main challenges when attempting touse these products: finding which datasets are related to a topic ofinterest; determining which existing information can be used toextend a given dataset; and how to share their integrated datasetresults with the rest of the community. In this paper we presentWDPlus, a framework designed to address these challenges byleveraging Wikidata. WDPlus allows searching for heterogeneousdatasets, facilitates completing tabular data usingWikidata and pro-poses a mechanism to extend Wikidata in a decentralized manner.KEYWORDSKnowledge Graphs, Entity Linking, Wikidata, RDF1 INTRODUCTIONToday, data about any domain can be found on the web in datarepositories, web APIs and millions of spreadsheets and CSV files.These data comes in a myriad of formats, layouts, terminology andcleanliness that make them difficult to integrate together.Users of these data face three main challenges. The first one isfinding datasets related to a feature or topic of interest. For example,climate scientists often look for years of observational data fromauthoritative sources when estimating the climate of a region. Thesecond challenge is how to complete a given dataset with existingknowledge: machine learning applications are data hungry andrequire asmany data points and features as possible to improve theirpredictions, which often requires integrating data from differentsources. The final challenge is sharing integrated results: onceseveral datasets have been merged together, how to make themavailable to the rest of the community?Knowledge graphs have become the preferred technology to ad-dress these challenges. Large organizations, including search engineproviders, shopping giants and finance institutions are investing inlarge knowledge graphs to integrate and retrieve heterogeneousdata. However, data integration pipelines are usually created man-ually, require significant expertise, and are seldom available to thegeneral public. Similarly, linking to existing datasets in the theLinked Open Data Cloud1usually requires the expertise of a knowl-edge engineer to properly identify the appropriate target instancesto link to in other datasets.1https://lod-cloud.net/Copyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).Recent initiatives such as Data.world,2Google data search [2]and DataCommons",
        "publication_date": "2019-01-01",
        "authors": "Daniel Garijo, Pedro Szekely",
        "file_name": "20250512002848.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/short4/PDFs/20250512002848.pdf",
        "pdf_link": "http://ceur-ws.org/Vol-2526/short4.pdf",
        "file_html": "html/20250512002848.html",
        "url": "http://ceur-ws.org/Vol-2526/short4.pdf",
        "issn": "1613-0073",
        "volume": "2526",
        "address": "Los Angeles, California",
        "booktitle": "Proceedings of the Third Workshop on Capturing Scientific Knowledge (SciKnow 2019), held in conjunction with the 2019 ACM International Conference on Knowledge Capture (K-CAP)",
        "year": "2019",
        "author": "Daniel Garijo and Pedro Szekely",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo-and-szekely-sciknow2019"
    },
    {
        "title": "SoMEF: A Framework for Capturing Scientific Software Metadata from its Documentation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/KnowledgeCaptureAndDiscovery/SM2KG",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "README_BIBTEX",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "README_TEXT",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_README",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    }
                ]
            },
            {
                "identifier": "github.com/v3/2README",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/SoMEF/PDFs/20250511232328.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "These traits are available elements 1https://developer.github.com/v3/2README from https://github.com/whimian/pyGeoPressure https://developer.github.com/v3/https://github.com/whimian/pyGeoPressure pattern matching [4]."
                    }
                ]
            }
        ],
        "doi": "10.1109/BigData47090.2019.9006447",
        "abstract": "Abstract—Scientific software has become a key assetto reproduce and understand the products of scientificresearch in many disciplines. However, scientific software isbecoming increasingly complex and, as a result, researchersneed to spend a significant amount of time finding, readingand understanding software documentation to set it up.In this paper we describe SoMEF, a Software MetadataExtraction Framework designed to help highlighting themost important parts of scientific software documentation.SoMEF processes the README files in GitHub repositoriesto automatically extract which parts of their text referto the description, installation, invocation, or citation ofa software component. Despite its simple features, SoMEFsuccessfully categorizes README excerpts with a mini-mum 0.92 precision and 0.90 ROC AUC. These results,tested on a corpus of over 70 scientific software repositories,are a promising start towards automatically generatingknowledge graphs of scientific software metadata.I. INTRODUCTIONWithin the past few decades, computational sciencehas increasingly become recognized as a fundamentalapproach to answer scientific questions alongside theoryand experimentation [1]. However, the continuous devel-opment of new software makes it difficult for scientiststo keep track of different method implementations andto evaluate whether a certain piece of software suitstheir needs [2]. Scientists are required to spend timeporing through available software documentation andsource code in order to understand the software used ina project [3] and how to properly cite it. This process istime consuming and unappealing to scientists due to theheterogeneity and lack of unified structure in softwaredocumentation.Existing efforts have attempted to simplify this prob-lem by avoiding “wordy, unstructured, introductory de-scriptions” in favor of a specialized language just fordocumentation [3]. However, text documentation contin-ues to grow at an exponential rate [4].In this paper we aim to ease the process of un-derstanding, reusing and attributing scientific softwareby presenting SoMEF [5], a Software Metadata Ex-traction Framework that automatically extracts relevantsoftware metadata from its documentation. SoMEF takesas input a README file from a GitHub repositoryand identifies its description, installation instructions,invocation setup and citation. Our approach uses binaryclassification methods and organizes the results into astructured format that is comprehensible to both humansand machines. In addition, SoMEF will extract additionalmetadata about a piece of software beyond its documen-",
        "publication_date": "2019-12-01",
        "authors": "Allen Mao, Daniel Garijo, Shobeir Fakhraei",
        "file_name": "20250511232328.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/SoMEF/PDFs/20250511232328.pdf",
        "pdf_link": "https://dgarijo.com/papers/SoMEF.pdf",
        "file_html": "html/20250511232328.html",
        "url": "https://dgarijo.com/papers/SoMEF.pdf",
        "pages": "3032--3037",
        "booktitle": "2019 IEEE International Conference on Big Data (Big Data)",
        "year": "2019",
        "author": "A. {Mao} and D. {Garijo} and S. {Fakhraei}",
        "ENTRYTYPE": "inproceedings",
        "ID": "9006447"
    },
    {
        "title": "Semantic workflows for benchmark challenges: Enhancing comparability, reusability and reproducibility",
        "implementation_urls": [
            {
                "identifier": "https://github.com/arunima2/Supplementary_PSB_2019",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/srivastava/PDFs/20250512003505.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Supplementary material available at: https://github.com/arunima2/Supplementary_PSB_2019 References 1."
                    }
                ]
            }
        ],
        "doi": "10.1142/9789813279827_0019",
        "abstract": "altering results. Different challenge entries may be readily compared through the use of abstract workflows, which also facilitate reuse. WINGS is housed on a cloud based setup, which stores data, dependencies and workflows for easy sharing and utility.  It also has the ability to scale workflow executions using distributed computing through the Pegasus workflow execution system. We demonstrate the application of this architecture to the DREAM proteogenomic challenge.   Keywords: Workflows; Semantic Workflows; DREAM Challenges; Proteogenomics; Benchmarking; Big Data 1.  Introduction The volume of experimental data being generated in the field of experimental biology is growing at a rapid pace in both size and variety1,2. With the advent of increasingly diverse data types, many of which are high throughput, the bioinformatics community is introducing sophisticated computational approaches for data analysis3,4.   To compare different approaches, community-wide competitive benchmark challenges have gained popularity as an unbiased method to better understand the variety of pipelines proposed by different groups.  Popular challenges include the Dialogue for Reverse Engineering Assessments and Methods (DREAM)5, Critical Assessment of Structure Prediction (CASP) protein structure prediction6 and The Association of Biomolecular Resource Facilities’ (ABRF) Proteome Informatics Research Group’s (iPRG) detection and prediction challenges7. These challenges give competitors the opportunity to test (in a blind and unbiased manner) their approach against others in the field, and have been instrumental in advancing diverse areas from protein structure prediction8 to variant calling9 to analysis of pathology data10. Unfortunately, evaluations in these competitions have traditionally been limited to metrics that evaluate solely based on scores.  Comparisons of the methods that gave rise to those results are often left to manual interpretation.  When the difference between a winner and an extremely poor performer may come down to a handful of parameters in otherwise identical workflows, the lack of transparency in methods is a huge missed opportunity for the bioinformatics community. In addition, winning methods are rarely shared with the broader community, as it is cumbersome to make winning methods accessible beyond the competition framework. Thus, while these Pacific Symposium on Biocomputing 2019    210 challenges provide a forum for bioinformatics researchers to independently evaluate the performance of their approaches against others, the current execution environment for challenges does not facilitate deep comparison and sharing of approaches.   Consequently, there is a critical need to reconsider the infrastructure used for executing benchmark challenges.  Here we examine the potential benefits of conducting benchmark challenges within a semantic workflow environment.  Workflow environments, such as Galaxy11 and GenePattern12, would enable a challenge to examine not just the final results, but also all the steps of a method.  This could include all dependencies, relevant data, and workflow components.  By having challengers enter their submissions as workflows, which are executed on challenge data in the cloud, it becomes possible to more deeply perform a meta-analysis of the entries. In addition, submissions could be easily reused and shared by members of the broader scientific community.  This work describes our effort to date using the WINGS13 semantic workflow system to submit entries to the DREAM proteogenomic challenge. While WINGS is an established (ready-to-download for server) workflow system14, employing it as a submission and storing protocol for data analysis challenges is a novel use of this framework. In addition to the advantages typical of workflow systems, WINGS has additional features due to its use of semantic representations and ",
        "publication_date": "2018-11-01",
        "authors": "Arunima Srivastava, Ravali Adusumilli, Hunter Boyce, Daniel Garijo, Varun Ratnakar, Rajiv Mayani, Thomas Yu, Raghu Machiraju, Yolanda Gil, Parag Mallick",
        "file_name": "20250512003505.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/srivastava/PDFs/20250512003505.pdf",
        "pdf_link": "http://psb.stanford.edu/psb-online/proceedings/psb19/srivastava.pdf",
        "file_html": "html/20250512003505.html",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/psb/SrivastavaABGRM19.bib",
        "timestamp": "Thu, 12 Mar 2020 11:38:57 +0100",
        "editor": "Russ B. Altman and A. Keith Dunker and Lawrence Hunter and Marylyn D. Ritchie and Teri E. Klein",
        "url": "http://psb.stanford.edu/psb-online/proceedings/psb19/srivastava.pdf",
        "pages": "208--219",
        "booktitle": "Biocomputing 2019: Proceedings of the Pacific Symposium, The Big Island of Hawaii, Hawaii, USA, January 3-7, 2019",
        "year": "2019",
        "author": "Arunima Srivastava and Ravali Adusumilli and Hunter Boyce and Daniel Garijo and Varun Ratnakar and Rajiv Mayani and Thomas Yu and Raghu Machiraju and Yolanda Gil and Parag Mallick",
        "ENTRYTYPE": "inproceedings",
        "ID": "DBLP:conf/psb/SrivastavaABGRM19"
    },
    {
        "title": "RepoFromPaper: An Approach to Extract Software Code Implementations from Scientific Publications",
        "implementation_urls": [
            {
                "identifier": "https://github.com/StankovskiA/RepoFromPaper",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/stankovsi_2024/PDFs/20250511232730.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "In summary, the number of implementation links found only by the bi-directional approach was 4, the 14 RepoFromPaper is available at https://github.com/StankovskiA/RepoFromPaper 15 https://arxiv.org/list/cs.AI/recent 16 https://arxiv.org/list/cs.SE/recent 7 Discussion Our approach produces high evaluation results, presents several limitations."
                    }
                ]
            }
        ],
        "doi": "https://doi.org/10.1007/978-3-031-65794-8_7",
        "abstract": "Abstract. An increasing amount of scientists link to their research soft-ware code implementations in their academic publications in order tosupport the reusability of their results. However, research papers usuallycontain many code links (e.g., from reused tools or existing compet-ing efforts) making it challenging to automatically establish clear linksbetween papers and their corresponding implementations. This paperpresents RepoFromPaper, an approach for automatically extracting themain code implementation associated with a research paper, based on thecontext in which that link is mentioned. Our approach uses fine-tunedlanguage models to retrieve the top candidate sentences where a code im-plementation may be found, and uses custom heuristics to link candidatesentences back to their corresponding URL (footnote, reference or full-text mention). We evaluated RepoFromPaper on 150 research papers,obtaining an F1 score of 0.94. We also run our approach on nearly 1800papers from the CS.AI Arxiv category, discovering 604 paper-repositorylinks and making them available to the community.Keywords: Information extraction · Research Software · Software repos-itory · Open Science.1 IntroductionResearch Software, i.e., the source code files, algorithms, scripts, computationalworkflows and executables that were created during the research process [2] isbecoming recognized as a first class citizen in scientific curricula.1 In order tosupport the results described in academic publications, scientists often include alink to a code repository (e.g., GitHub, Gitlab) with their technical implemen-tations details.While efforts have been made by the scientific community to establish princi-ples[16] and formats for software citation [5], detecting the code repository linkassociated with a publication has two main challenges. First, authors often citeresearch software inconsistently, employing diverse formats and locations such as1 https://sfdora.org/read/2 A. Stankovski and D. Garijofull-text repository mentions (cases where the link is written in the paragraphs),footnotes, or references to refer to a software component [8]. Second, a publi-cation may contain several code repository links (from tools that are reused, orcompeting with the proposed approach) making it challenging to automaticallydetect the right code implementation.This paper introduces a methodology designed to address these challenges byautomatically extracting the software implementation repository link associatedwith a research paper, based on the context in which the link is mentioned. Thecore contributions of our work include:1. Training and validation datasets of labeled sentences designed to fine-tune and evaluate our approach [17]. The training dataset includes 61 re-search papers related to software engineering available on the PapersWith-Code2 platform. The validation dataset includes 150 software engineering re-search articles extracted from Arxiv. Both datasets encompass various typesof implementation mention sentences to cover the diverse ways authors ref-erence the implementation repository.2. RepoFromPaper3, a tool to automatically extract the code implementa-tion repository from a research paper, including PDF-to-Text conversion,sentence extraction, sentence classification and link search, as well as three",
        "publication_date": "2024-01-01",
        "authors": "Aleksandar Stankovski, Daniel Garijo",
        "file_name": "20250511232730.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/stankovsi_2024/PDFs/20250511232730.pdf",
        "pdf_link": "https://dgarijo.com/papers/stankovsi_2024.pdf",
        "file_html": "html/20250511232730.html",
        "isbn": "978-3-031-65794-7",
        "url": "https://dgarijo.com/papers/stankovsi_2024.pdf",
        "editor": "Rehm, Georg and Dietze, Stefan and Schimmler, Sonja and Kr{\\\"u}ger, Frank",
        "pages": "114--133",
        "address": "Cham",
        "publisher": "Springer Nature Switzerland",
        "booktitle": "Natural Scientific Language Processing and Research Knowledge Graphs",
        "year": "2024",
        "author": "Stankovski, Aleksandar and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "stankovski2024"
    },
    {
        "title": "Declarative generation of RDF-star graphs from heterogeneous data",
        "implementation_urls": [],
        "abstract": "Abstract. RDF-star has been proposed as an extension of RDF to annotate statements with triples. Libraries and graph storeshave started adopting RDF-star, but the generation of RDF-star data remains largely unexplored. To allow generating RDF-starfrom heterogeneous data, RML-star was proposed as an extension of RML. However, no implementation has been developedso far that implements the RML-star specification. In this work, we present Morph-KGCstar, which extends the Morph-KGCmaterialization engine to generate RDF-star datasets. We validate Morph-KGCstar by running test cases derived from the N-Triples-star syntax tests and we apply it to two real-world use cases from the biomedical and open science domains. We comparethe performance of our approach against other RDF-star generation methods (SPARQL-Anything), showing that Morph-KGCstarscales better for large input datasets, but it is slower when processing multiple smaller files.Keywords: Knowledge Graphs, RDF-star, RML-star, Data Integration1. IntroductionRDF-star [1] was proposed as an extension of RDF [2] to annotate statements and, thus, make statements aboutother statements (also known as reification [3]). RDF-star extends the RDF’s conceptual data model and concretesyntaxes by providing a compact alternative to other reification approaches, such as standard reification [4] orsingleton properties [5]. Following the uptake of the initial version of RDF-star, the W3C RDF-DEV CommunityGroup1 recently released a W3C Final Community Group Report [6] and the RDF-star Working Group2 has recentlybeen formed to extend related W3C Recommendations.Even though several libraries and graph stores have already adopted RDF-star3, the generation of RDF-star graphsremains largely unexplored. RDF graphs are often generated from heterogeneous semi-structured data, e.g., data inCSV, XML or JSON formats, etc. To generate RDF graphs, mapping languages are used to specify how RDF termsand triples can be generated from these data. The syntax of these mapping languages are either custom or repurposed.*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.**The authors contributed equally to this work.1https://www.w3.org/groups/cg/rdf-dev2https://www.w3.org/groups/wg/rdf-star3https://w3c.github.io/rdf-star/implementations1570-0844/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:julian.arenas.guerrero@upm.esmailto:ana.iglesiasm@upm.esmailto:daniel.garijo@upm.esmailto:oscar.corcho@upm.esmailto:david.chaves@kuleuven.bemailto:anastasia.dimou@kuleuven.bemailto:julian.arenas.guerrero@upm.eshttps://www.w3.org/groups/cg/rdf-devhttps://www.w3.org/groups/wg/rdf-starhttps://w3c.github.io/rdf-star/implementations2 J. Arenas-Guerrero et al. / Declarative generation of RDF-star graphs from heterogeneous data1 12 23 34 45 56 67 78 89 910 1011 1112 1213 13",
        "file_name": "20250511234452.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/swj_arenas_2024/PDFs/20250511234452.pdf",
        "pdf_link": "https://dgarijo.com/papers/swj_arenas_2024.pdf",
        "file_html": "html/20250511234452.html",
        "url": "https://dgarijo.com/papers/swj_arenas_2024.pdf",
        "year": "2025",
        "pages": "SW-243602",
        "number": "2",
        "volume": "16",
        "journal": "Semantic Web",
        "doi": "10.3233/SW-243602",
        "author": "Arenas-Guerrero, Juli{\\'a}n and Iglesias-Molina, Ana and Chaves-Fraga, David and Garijo, Daniel and Corcho, Oscar and Dimou, Anastasia",
        "ENTRYTYPE": "article",
        "ID": "arenas2024morph"
    },
    {
        "title": "T2WML: Table To Wikidata Mapping Language",
        "implementation_urls": [],
        "abstract": "ABSTRACTThe web contains millions of useful spreadsheets and CSV files, butthese files are difficult to use in applications because they use awide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes iteasy to map and link arbitrary spreadsheets and CSV files to theWikidata data model. The output of T2WML consists of Wikidatastatements that can be loaded in the public Wikidata knowledgebase or in a Wikidata clone repository, creating an augmentedWikidata knowledge graph that application developers can queryusing SPARQL.1CCS CONCEPTS• Information systems→Extraction, transformation and load-ing.KEYWORDSKnowledge Graphs; RDF; Entity Linking; WikidataACM Reference Format:Pedro Szekely, Daniel Garijo, Divij Bhatia, JiashengWu, Yixiang Yao and JayPujara. 2019. T2WML: Table ToWikidata Mapping Langauge. In Proceedingsof the 10th International Conference on Knowledge Capture (K-CAP ’19),November 19–21, 2019, Marina Del Rey, CA, USA. ACM, New York, NY, USA,4 pages. https://doi.org/10.1145/3360901.33644481 INTRODUCTIONThe web contains millions of useful spreadsheets and CSV files,including data from many government and international organi-zations. Most institutions offer their data in web sites where userscan download the data in Excel and CSV formats. The downloadeddata is seldom directly usable because, unlike databases (which useone column per variable), spreadsheets often arrange the data indifferent layouts.Fig. 1 illustrates the problem using data downloaded from theUnited Nations web site2 about homicide rates in different countries.We truncated and colored the files for ease of presentation. Thecells with the homicide numbers are highlighted in green, the cells1This material is based upon work supported by United States Air Force under ContractNo. FA8650-17-C-7715.2https://dataunodc.un.org/crime/intentional-homicide-victimsPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA© 2019 Association for Computing Machinery.ACM ISBN 978-1-4503-7008-0/19/11. . . $15.00https://doi.org/10.1145/3360901.3364448that provide contextual information for the value are highlightedin blue, and header cells are highlighted in dark blue. Fig. 1a shows",
        "file_name": "20250512002725.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/t2wmlKCAP2019/PDFs/20250512002725.pdf",
        "pdf_link": "https://dgarijo.com/papers/t2wmlKCAP2019.pdf",
        "file_html": "html/20250512002725.html",
        "keywords": "entity linking, wikidata, rdf, knowledge graphs",
        "numpages": "4",
        "url": "https://dgarijo.com/papers/t2wmlKCAP2019.pdf",
        "isbn": "9781450370080",
        "doi": "10.1145/3360901.3364448",
        "pages": "267–270",
        "series": "K-CAP ’19",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "location": "Marina Del Rey, CA, USA",
        "booktitle": "Proceedings of the 10th International Conference on Knowledge Capture",
        "year": "2019",
        "author": "Szekely, Pedro and Garijo, Daniel and Bhatia, Divij and Wu, Jiasheng and Yao, Yixiang and Pujara, Jay",
        "ENTRYTYPE": "inproceedings",
        "ID": "szekely-garijo-et-al2019"
    },
    {
        "title": "Assessing the Overlap of Science Knowledge Graphs: A Quantitative Analysis",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kuefmz/define_taxonomy",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/tabitha_2024/PDFs/20250511233055.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Correlation of Similarity and Agreement The scripts [2] 12 used to carry out our methodology and analysis are available online under the MIT license."
                    }
                ]
            },
            {
                "identifier": "https://zenodo.org/records/10974512",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.10974512",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "https://doi.org/10.1007/978-3-031-65794-8_11",
        "abstract": "Abstract. Science Knowledge Graphs (SKGs) have emerged as a meansto represent and capture research outputs (papers, datasets, software,etc.) and their relationships in a machine-readable manner. However,different SKGs use different taxonomies, making it challenging to under-stand their overlaps, gaps and differences. In this paper, we propose aquantitative bottom-up analysis to assess the overlap between two SKGs,based on the type annotations of their instances. We implement ourmethodology by assessing the category overlap of 100,000 publicationspresent both in OpenAlex and OpenAIRE. As a result, our approach pro-duces an alignment of 71 categories and discusses the level of agreementbetween both KGs when annotating research artefacts.Keywords: Scientific Knowledge Graph · Knowledge Graph · Taxon-omy · Alignment1 IntroductionAs the volume of scientific literature increases, the need for scalable and effi-cient systems to navigate this extensive information becomes crucial. ScienceKnowledge Graphs (SKGs) [11] have emerged as a key tool for representing re-search entities (publications, people, organizations, datasets, software, etc.) theirrelationships and metadata in a machine-readable manner.SKGs such as OpenAIRE 1 [23,24,19,18] and OpenAlex 2 [21] contain mil-lions of entities describing publications and research outputs. One of the mainchallenges when using SKGs is identifying and resolving overlaps in categoriza-tion, which is critical for querying them consistently and reliably. This challengeis complex due to the diversity and volume of data within these KGs, requiringadvanced methodologies for effective detection and resolution of overlaps. Un-derstanding these overlaps and disagreements is essential for insights into thestructure of scientific knowledge, highlighting patterns that are not immediatelyapparent due to data scale and diversity.This paper proposes a quantitative bottom-up methodology to assess theoverlap of SKGs categories, based on the annotations made on their instances.1 https://www.openaire.eu/2 https://openalex.org/https://www.openaire.eu/https://openalex.org/2 J. Tabita Ciuciu-Kiss and D. GarijoMore specifically we aim to explore the overlap of the taxonomies used in scien-tific literature [22]. Our contributions include:1. A novel methodology designed to explore the overlap between SKGs.2. An implementation of the methodology, based on two SKGs to validate itseffectiveness, resulting in 71 new aligned categories within these graphs.3. An initial exploration study of the intersection of two SKGs, based on100,000 papers that are jointly described in both of them.As a proof of concept, we have applied our methodology to a subset of Ope-nAlex and OpenAIRE SKGs, in the AI domain. We chose OpenAlex for its ex-tensive global database of academic research, and OpenAIRE for its Europeanfocus and its integration from heterogeneous data sources. This combinationoffers a comprehensive view of academic communication, providing a compre-hensive dataset for our methodology.The remainder of the paper is structured as follows. Section 2 describes ourmethodology, while Section 3 explains how we implemented our methodology",
        "publication_date": "2024-01-01",
        "authors": "Jenifer Tabita Ciuciu-Kiss, Daniel Garijo",
        "file_name": "20250511233055.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/tabitha_2024/PDFs/20250511233055.pdf",
        "pdf_link": "https://dgarijo.com/papers/tabitha_2024.pdf",
        "file_html": "html/20250511233055.html",
        "isbn": "978-3-031-65794-11",
        "url": "https://dgarijo.com/papers/tabitha_2024.pdf",
        "editor": "Rehm, Georg and Dietze, Stefan and Schimmler, Sonja and Kr{\\\"u}ger, Frank",
        "pages": "114--133",
        "address": "Cham",
        "publisher": "Springer Nature Switzerland",
        "booktitle": "Natural Scientific Language Processing and Research Knowledge Graphs",
        "year": "2024",
        "author": "Tabita Ciuciu-Kiss, Jenifer and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "tabitha2024"
    },
    {
        "title": "TEC: Transparent Emissions Calculation Toolkit",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TEC-Toolkit/PECO",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "FILE_CFF",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_FILE",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    }
                ]
            },
            {
                "identifier": "https://github.com/TEC-Toolkit/ECFO",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "FILE_CFF",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_FILE",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    }
                ]
            },
            {
                "identifier": "https://github.com/TEC-Toolkit/cfkg",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "FILE_CFF",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_FILE",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    },
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/TEC_Toolkit_ISWC_2023/PDFs/20250511234520.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The resultant KG and mappings are publicly available online17 (mappings are available under an Apache 2.0 license) [15]."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47243-5_5",
        "abstract": "Abstract. Greenhouse gas emissions have become a common means fordetermining the carbon footprint of any commercial activity, rangingfrom booking a trip or manufacturing a product to training a machinelearning model. However, calculating the amount of emissions associatedwith these activities can be a difficult task, involving estimations of en-ergy used and considerations of location and time period. In this paper,we introduce the Transparent Emissions Calculation (TEC) toolkit, anopen source effort aimed at addressing this challenge. Our contributionsinclude two ontologies (ECFO and PECO) that represent emissions con-version factors and the provenance traces of carbon emissions calcula-tions (respectively), a public knowledge graph with thousands of conver-sion factors (with their corresponding YARRRML and RML mappings)and a prototype carbon emissions calculator which uses our knowledgegraph to produce a transparent emissions report.Resource permanent URL: https://w3id.org/tec-toolkitKeywords: Ontology · GHG Emissions · Carbon Accounting · Trans-parency1 IntroductionThe Net Zero agenda has gained significant traction across the world, with over40 countries worldwide requiring organisations to periodically calculate and re-port their greenhouse gas (GHG) emissions [29]. Calculating them requires real-world data observations quantifying various aspects of business activities (e.g.,amount of fuel consumed by a fleet of vehicles) and additional resources such asmethodologies for transforming activity data into GHG estimates (also referredto as emissions scores). Reported emissions scores may differ depending on vari-ous factors including the calculation methodology and software used, geopoliticallocation, government requirements for reporting methods, applicable emissions2 M. Markovic et al.conversion factors (ECFs), and the type of reported GHG emissions. Emissionscalculations may also include unintentional errors, such as the use of ECFs whichmight be out of date, from unreliable publishers, or incorrectly applied to a spe-cific activity, thus causing erroneous results. In addition, organisations may havea vested interest in deliberately under-reporting on certain aspects of carbonfootprint if they deem it could have negative impact on the company image [20].While reporting requirements may differ from one country to another, organ-isations are expected to be transparent about their submitted results. Achievingsuch transparency may be challenging as it requires a clear history of whichECFs were used and how the emissions scores were calculated including detailsabout the origin and accuracy of the input data. These details are typicallycommunicated in the form of free text reports which are not suitable for auto-mated processing. However, such transparency is necessary to support assess-ments evaluating the trustworthiness and meaningful comparison of emissionsscores reported by organisations across different sectors over time. We arguethat provenance traces of such calculations described in the form of KnowledgeGraphs (KGs) potentially provide a machine-understandable solution to thischallenge by making the calculations more transparent and providing the meansfor automated processing and analysis. This is a core motivation for our Trans-parent Emissions Calculation (TEC) toolkit which aims to address this issueby providing ontologies and software tools for enhancing the transparency ofemissions calculations using KGs. Our contributions include:",
        "publication_date": "2023-01-01",
        "authors": "Milan Marković, Daniel Garijo, Stefano Germano, Iman Naja",
        "file_name": "20250511234520.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/TEC_Toolkit_ISWC_2023/PDFs/20250511234520.pdf",
        "pdf_link": "https://dgarijo.com/papers/TEC_Toolkit_ISWC_2023.pdf",
        "file_html": "html/20250511234520.html",
        "url": "https://dgarijo.com/papers/TEC_Toolkit_ISWC_2023.pdf",
        "pages": "76--93",
        "address": "Cham",
        "publisher": "Springer Nature Switzerland",
        "year": "2023",
        "booktitle": "The Semantic Web -- ISWC 2023",
        "editor": "Payne, Terry R. and Presutti, Valentina and Qi, Guilin and Poveda-Villal{\\'o}n, Mar{\\'i}a and Stoilos, Giorgos and Hollink, Laura and Kaoudi, Zoi and Cheng, Gong and Li, Juanzi",
        "author": "Markovic, Milan and Garijo, Daniel and Germano, Stefano and Naja, Iman",
        "ENTRYTYPE": "inproceedings",
        "ID": "10.1007/978-3-031-47243-5_5"
    },
    {
        "title": "Mining Abstractions in Scientific Workows",
        "implementation_urls": [],
        "abstract": "Mining Abstractions in ScientificWorkflowsAuthor: Daniel Garijo VerdejoSupervisors: Prof. Dr. Oscar CorchoProf. Dra. Yolanda GilDecember, 2015iiilTribunal nombrado por el Sr. Rector Magfco. de la Universidad Politécnica de Madrid,el d́ıa 30 de octubre de 2015Presidente: Dra. Asunción Gómez PérezVocal: Dr. Jose Manuel Gómez PérezVocal: Dr. Malcolm AtkinsonVocal: Dr. Rafael TolosanaSecretario: Dr. Mark WilkinsonSuplente: Dr. Mariano Fernández LópezSuplente: Dra. Belén Dı́az AgudoRealizado el acto de defensa y lectura de la Tesis el d́ıa 3 de diciembre de 2015 en laFacultad de InformáticaCalificaćıon:EL PRESIDENTE VOCAL 1 VOCAL 2VOCAL 3 EL SECRETARIOiiiivlvA mis padresvviviAcknowledgementsFinally, after five years, I can finally say that I see light at the end of thetunnel. Maybe the other side is still a bit cloudy at the moment, but theimportant thing is to have arrived here. And, honestly, I think I wouldn’thave made it to this point without all the people who have been by my sideduring these years.First, I would like to thank my supervisors Oscar Corcho and Yolanda Gilfor guiding me whenever I got stuck and for having the patience to answerall my questions. Furthermore, thanks to their help, together with AsunciónGómez Pérez’s advice, I was granted the FPU (Formación de ProfesoradoUniversitario) scholarship from the Ministerio de Ciencia e Innovación. Thisscholarship has funded the internships and the research described on thisdocument, and I am very grateful for having had the opportunity to enjoyit.I would also like to thank my family, specially my parents (Francisco Javierand Maŕıa Felisa) and my sister Elisa for all their support, advice andsuggestions during this period. Even from the distance!Next up are my lab mates, who have helped me with the figures (MaŕıaPoveda, I really think you could write a thesis just by doing cool figures),logos (Idafen Santana, also responsible for our soccer team), technical sup-port (Miguel Angel Garćıa and Raúl Alcázar), advice for the thesis (Andrés",
        "file_name": "20250512001353.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/tesis/PDFs/20250512001353.pdf",
        "pdf_link": "https://dgarijo.com/papers/tesis.pdf",
        "file_html": "html/20250512001353.html",
        "school": "ETSI\\_Informatica",
        "url": "https://dgarijo.com/papers/tesis.pdf",
        "year": "2015",
        "author": "Garijo Verdejo, Daniel",
        "ENTRYTYPE": "phdthesis",
        "ID": "garijo2015mining"
    },
    {
        "title": "{RMLdoc}: Documenting Mapping Rules for Knowledge Graph Construction",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/rmldoc",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/toledo2024rmldoc/PDFs/20250511232605.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Code repository: https://github.com/oeg-upm/rmldoc/Demo: https://w3id.org/rmldoc/example Keywords: Documentation · Knowledge Graph Construction · RML."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-78952-6_51",
        "abstract": "Abstract. In this demo we present RMLdoc, a Python package de-signed to generate documentation for RML mappings when constructingknowledge graphs from heterogeneous sources. Given an input mappingfile written in R2RML, RML, or YARRRML, RMLdoc will generate adetailed Markdown documentation explaining each mapping with corre-sponding diagrams, in a human readable manner. Thanks to RMLdoc,we aim to shed light in the knowledge graph construction process, makingmappings easier to maintain and understand by knowledge engineers.Code repository: https://github.com/oeg-upm/rmldoc/Demo: https://w3id.org/rmldoc/exampleKeywords: Documentation · Knowledge Graph Construction · RML.1 IntroductionKnowledge Graphs (KGs) are commonly constructed by transforming a set ofheterogeneous data sources (e.g., CSV, JSON files) into RDF graphs. Thesetransformations are performed by relating all input sources with the target on-tology terms, and can be described using declarative mapping languages such asthe W3C recommendation R2RML3 or its widely adopted extension RML [7]. In-stitutions such as the European Railway Agency4 or the European Commission(e.g., in the EU Public Procurement Data Space5) describe their transformationsusing these languages in some of their projects.Knowledge engineers are usually responsible for developing the mapping rulesneeded to construct KGs. In many cases, these engineers rely on graphical inter-faces (e.g, RMLEditor [5]) and human-friendly serializations like YARRRML [4]or Mapeathor [6] to aid them in the creation of mapping rules. However, themapping documents resultant from these efforts are, in many cases, complex andhard to interpret, which reduces their reusability by other engineers. Further-more, there is a lack of tools to generate a comprehensive and human-readable3 https://www.w3.org/TR/r2rml/4 https://data-interop.era.europa.eu/5 https://europa.eu/!qx9WxQhttps://orcid.org/0000-0002-2924-7272https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0003-0454-7145https://github.com/oeg-upm/rmldoc/https://w3id.org/rmldoc/examplehttps://www.w3.org/TR/r2rml/https://data-interop.era.europa.eu/https://europa.eu/!qx9WxQ2 Toledo et al.documentation of mapping rules. This situation delegates mappings as second-class resources in the KG development process, without documentation (scat-tered comments in the mapping document at most) or essential metadata (e.g.,version, creators, license).In this paper, we present RMLdoc [8],6 an open source Python package de-signed to create a human-readable documentation of the mapping rules usedto construct a Knowledge Graph. RMLdoc supports mapping rules describedin R2RML, RML, and YARRRML, helping practitioners better understand therelationships between the original data sources and the ontology terms. To thebest of our knowledge, this is the first approach that proposes the generation of",
        "publication_date": "2025-01-01",
        "authors": "Jhon Toledo, Ana Iglesias-Molina, David Chaves-Fraga, Daniel Garijo",
        "file_name": "20250511232605.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/toledo2024rmldoc/PDFs/20250511232605.pdf",
        "pdf_link": "https://dgarijo.com/papers/toledo2024rmldoc.pdf",
        "file_html": "html/20250511232605.html",
        "year": "2024",
        "booktitle": "To appear in the Extended Semantic Web Conference Poster and demo proceedings, 2024",
        "url": "https://dgarijo.com/papers/toledo2024rmldoc.pdf",
        "author": "Toledo, Jhon and Iglesias-Molina, Ana and Chaves-Fraga, David and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "toledormldoc"
    },
    {
        "title": "FAIROs: Towards FAIR Assessment in Research Objects",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/FAIR-Research-Object",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "README_BIBTEX",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "README_TEXT",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_README",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    },
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/TPDL2022_gonzalez/PDFs/20250511235137.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "FAIROs is open source and available on GitHub 8."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-16802-4_6",
        "abstract": "The FAIR principles have become a popular means to guide researchers when publishing their research outputs (i.e., data, software, etc.) in a Findable, Accessible, Interoperable and Reusable manner. In order to ease compliance with FAIR, different frameworks have been developed by the scientific community, offering guidance and suggestions to researchers. However, scientific outputs are rarely published in isolation. Research Objects have been proposed as a framework to capture the relationships and context of all constituents of an investigation. In this paper we present FAIROs, a framework for assessing the compliance of a Research Object (and its constituents) against the FAIR principles. FAIROs reuses existing FAIR validators for individual resources and proposes i) two scoring methods for assessing the fairness of Research Objects, ii) an initial implementation of the scoring methods in the FAIROs framework, and iii) an explanation-based approach designed to visualize the obtained scores. We validate FAIROs against 165 Research Objects, and discuss the advantages and limitations of different scoring systems.",
        "publication_date": "2022-01-01",
        "authors": "Esteban González, Alejandro Benítez, Daniel Garijo",
        "file_name": "20250511235137.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/TPDL2022_gonzalez/PDFs/20250511235137.pdf",
        "pdf_link": "https://dgarijo.com/papers/TPDL2022_gonzalez.pdf",
        "file_html": "html/20250511235137.html",
        "url": "https://dgarijo.com/papers/TPDL2022_gonzalez.pdf",
        "isbn": "978-3-031-16802-4",
        "pages": "68--80",
        "address": "Cham",
        "publisher": "Springer International Publishing",
        "year": "2022",
        "booktitle": "Linking Theory and Practice of Digital Libraries",
        "editor": "Silvello, Gianmaria and Corcho, Oscar and Manghi, Paolo and Di Nunzio, Giorgio Maria and Golub, Koraljka and Ferro, Nicola and Poggi, Antonella",
        "author": "Gonz{\\'a}lez, Esteban and Ben{\\'i}tez, Alejandro and Garijo, Daniel",
        "ENTRYTYPE": "inproceedings",
        "ID": "10.1007/978-3-031-16802-4_6"
    },
    {
        "title": "A Provenance-aware Linked Data Application for Trip Management and Organization",
        "implementation_urls": [],
        "doi": "10.1145/2063518.2063554",
        "abstract": "ABSTRACTWe present El Viajero, an application1 for exploiting, man-aging and organizing Linked Data in the domain of newsand blogs about travelling. El Viajero makes use of severalheterogeneous datasets to help users to plan future trips,and relies on the Open Provenance Model2 for modeling theprovenance information of the resources.Categories and Subject DescriptorsE.2 [Data storage representations]: Linked Representa-tionsGeneral TermsDesign, ExperimentationKeywordsLinked Data, provenance, news, blog1. INTRODUCTIONNews content providers rarely include provenance infor-mation about the resources that they generate and pub-lish, as already pointed out the W3C Provenance Incuba-tor Group3 in the News Aggregator Scenario Gap Analy-sis4. This provenance information is critical to determinewhether a resource can be trusted or not, since it allowsknowing about the source, references and process followedto create the resource.This paper describes how we exploit the provenance in-formation of guides, images, videos, trips and posts in orderto help other users to reuse and explore existing contentsto plan their own trips around the world. The application1http://webenemasuno.linkeddata.es/browser_en.html2http://openprovenance.org/3http://www.w3.org/2005/Incubator/prov/wiki/Main_Page4http://www.w3.org/2005/Incubator/prov/wiki/Analysis_of_News_Aggregator_Scenario#Gap_AnalysisPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee.I-SEMANTICS Triplification Challenge 2011 Graz, AustriaCopyright 2010 ACM 978-1-4503-0014-8/10/09 ...$10.00.manages the data retrieval transparently to the users, andorganizes the results in a map through a graphical interface.It relies on a dataset that we5 have recently published asLinked Data, containing the provenance information of theeditorial resources belonging to El Viajero6(which is part ofthe spanish newspaper El Páıs7). The dataset aggregates",
        "publication_date": "2011-09-07",
        "authors": "Daniel Garijo, Boris Villazón-Terrazas, Óscar Corcho",
        "file_name": "20250512002627.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/triplification/PDFs/20250512002627.pdf",
        "pdf_link": "https://dgarijo.com/papers/triplification.pdf",
        "file_html": "html/20250512002627.html",
        "keywords": "blog, linked data, news, provenance",
        "acmid": "2063554",
        "numpages": "3",
        "url": "https://dgarijo.com/papers/triplification.pdf",
        "isbn": "978-1-4503-0621-8",
        "pages": "224--226",
        "series": "I-Semantics '11",
        "address": "New York, NY, USA",
        "publisher": "ACM",
        "location": "Graz, Austria",
        "booktitle": "Proceedings of the 7th International Conference on Semantic Systems",
        "year": "2011",
        "author": "Garijo, Daniel and Villaz\\'{o}n-Terrazas, Boris and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "Garijo:2011:PLD:2063518.2063554"
    },
    {
        "title": "Workflow reuse in practice: a study of neuroimaging pipeline users",
        "implementation_urls": [],
        "doi": "10.1109/eScience.2014.33",
        "abstract": "Abstract—Workflow reuse is a major benefit of workflow systems and shared workflow repositories, but there are barely any studies that quantify the degree of reuse of workflows or the practical barriers that may stand in the way of successful reuse.  In our own work, we hypothesize that defining workflow fragments improves reuse, since end-to-end workflows may be very specific and only partially reusable by others.  This paper reports on a study of the current use of workflows and workflow fragments in labs that use the LONI Pipeline, a popular workflow system used mainly for neuroimaging research that enables users to define and reuse workflow fragments. We present an overview of the benefits of workflows and workflow fragments reported by users in informal discussions. We also report on a survey of researchers in a lab that has the LONI Pipeline installed, asking them about their experiences with reuse of workflow fragments and the actual benefits they perceive.  This leads to quantifiable indicators of the reuse of workflows and workflow fragments in practice. Finally, we discuss barriers to further adoption of workflow fragments and workflow reuse that motivate further work.  Keywords— scientific workflows; workflow fragments; workflow reuse; LONI Pipeline I.  INTRODUCTION Workflows have many benefits to scientists managing complex data analysis [8] [7] [9] [20]. They make it easier to reuse expert-grade methods and the software that implements them, helping newcomers understand complex multi-step data analysis methods, and can track provenance and facilitate reproducibility. Workflow reuse is often cited as a major benefit of workflows, and has been studied in repositories of workflows [19]. However, there are no studies on the level of reuse of workflows in practice in research laboratories.  We are also particularly interested in whether workflow fragments are more reusable than entire workflows [5]. This paper reports on a study on workflow reuse in labs that use a particular workflow system, the LONI Pipeline [3] [4].  The LONI Pipeline includes facilities for users to define subsets of workflows as “groupings” that may be reused by themselves and with others in new workflows. The community of the LONI Pipeline users provides a unique opportunity to study how workflow fragments are used in practice, whether they improve reuse, and the barriers that users find in reusing workflow fragments. The main contributions of this paper are twofold.  First, it articulates the benefits of workflows and workflow fragments reported by users in a neuroscience research lab.  Although many of these benefits such as reuse and time savings have been discussed in the context of workflows, others are not commonly highlighted, such as promoting standards, facilitating debugging, and teaching newcomers to the lab.  ",
        "publication_date": "2014-10-01",
        "authors": "Daniel Garijo, Óscar Corcho, Yolanda Gil, Meredith N. Braskie, Derrek P. Hibar, Xue Hua, Neda Jahanshad, Paul M. Thompson, Arthur W. Toga",
        "file_name": "20250512001206.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/usersurvey-escience14/PDFs/20250512001206.pdf",
        "pdf_link": "https://dgarijo.com/papers/usersurvey-escience14.pdf",
        "file_html": "html/20250512001206.html",
        "funding": "USNSF IIS-1344272",
        "organization": "IEEE",
        "url": "https://dgarijo.com/papers/usersurvey-escience14.pdf",
        "pages": "239--246",
        "volume": "1",
        "booktitle": "e-Science (e-Science), 2014 IEEE 10th International Conference on",
        "year": "2014",
        "author": "Garijo, Daniel and Corcho, Oscar and Gil, Yolanda and Braskie, Meredith N and Hibar, Derrek and Hua, Xue and Jahanshad, Neda and Thompson, Paul and Toga, Arthur W",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2014workflow"
    },
    {
        "title": "Automated Extraction of Research Software Installation Instructions from README files: An Initial Analysis",
        "implementation_urls": [
            {
                "identifier": "https://github.com/carlosug/READMEtoP-PLAN",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/utrilla_2024/PDFs/20250511232652.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "3 The code and corpus are publicly available at: https://github.com/carlosug/READMEtoP-PLAN/[44] 3.1 Classical Planning: Software Installation Instructions The central objective of planning tasks for an intelligent assistant is to au-tonomously detect the sequence of steps to execute in order to accomplish a 4 Carlos Utrilla Guerrero, Oscar Corcho, and Daniel Garijo Fig."
                    }
                ]
            }
        ],
        "doi": "https://doi.org/10.1007/978-3-031-65794-8_8",
        "abstract": "Abstract. Research Software code projects are typically described witha README files, which often contains the steps to set up, test andrun the code contained in them. Installation instructions are writtenin a human-readable manner and therefore are difficult to interpret byintelligent assistants designed to help other researchers setting up acode repository. In this paper we explore this gap by assessing whetherLarge Language Models (LLMs) are able to extract installation instruc-tion plans from README files. In particular, we define a methodol-ogy to extract alternate installation plans, an evaluation framework toassess the effectiveness of each result and an initial quantitative eval-uation based on state of the art LLM models (llama-2-70b-chat andMixtral-8x7b-Instruct-v0.1). Our results show that while LLMs are apromising approach for finding installation instructions, they present im-portant limitations when these instructions are not sequential or manda-tory.Keywords: Research/Scientific Knowledge Graphs · Natural ScientificLanguage Processing · Information Extraction1 IntroductionResearch Software [5] is becoming increasingly recognized as a means to supportthe results described in scientific publications. Researchers typically documenttheir software project in code repositories, using README files (i.e., readme.md)with instructions on how to install, setup and run their software tools. However,software documentation is usually described in natural language, which makesit challenging to automatically verify whether the installation steps required tomake the software project work are accurate or not. While seemingly arbitrary, itcan be challenging for researchers to follow instructions from different documentstandards and make sure they work harmonically and consistently.In this work we aim to address these issues by exploring and assessing theabilities of state of the art Large Language Models (LLMs) to extract installationmethods (Plans) and their corresponding instructions (Steps) from README2 Carlos Utrilla Guerrero, Oscar Corcho, and Daniel Garijofiles. LLMs such as GPT-4 [21] and MISTRAL [12] have been firmly establishedas state of the art approaches in various natural scientific language process-ing (NSLP) tasks related to knowledge extraction from human-like scientificsources such as software documentation from public sharing code hosting ser-vices. LLMs have also shown promise in following instructions [26] and learningto use tools [25]. However, existing research in the field is still quite novel.Our goal in this work is twofold: given a README file, we aim to 1) detect allthe available Plans (e.g., installation methods for different platforms or operativesystems) and, 2) for each Plan, detect what steps are required to install a softwareproject, as annotated by the authors. Our contributions3 include:1. PlanStep, a methodology to extract structured installation instructions fromREADME files;2. An evaluation framework to assess the ability of LLMs to capture installationinstructions, both in terms of Plans and Steps;3. An annotated corpus of 33 research software projects with their respectiveinstallation plans and steps.We implement our approach by following our methodology to evaluate twostate of the art LLMs (LLaMA-2 [31] and (MIXTRAL [12]) on both installationinstruction tasks with our corpus of annotated projects.",
        "publication_date": "2024-01-01",
        "authors": "Carlos Utrilla Guerrero, Óscar Corcho, Daniel Garijo",
        "file_name": "20250511232652.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/utrilla_2024/PDFs/20250511232652.pdf",
        "pdf_link": "https://dgarijo.com/papers/utrilla_2024.pdf",
        "file_html": "html/20250511232652.html",
        "isbn": "978-3-031-65794-8",
        "url": "https://dgarijo.com/papers/utrilla_2024.pdf",
        "editor": "Rehm, Georg and Dietze, Stefan and Schimmler, Sonja and Kr{\\\"u}ger, Frank",
        "pages": "114--133",
        "address": "Cham",
        "publisher": "Springer Nature Switzerland",
        "booktitle": "Natural Scientific Language Processing and Research Knowledge Graphs",
        "year": "2024",
        "author": "Utrilla Guerrero, Carlos and Corcho, Oscar and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "utrilla2024"
    },
    {
        "title": "An ontology for videogame interoperability",
        "implementation_urls": [],
        "doi": "10.1007/s11042-016-3552-6",
        "abstract": "Abstract During the last 20 years, video games have become very popular and widelyadopted in our society. However, despite the growth on video game industry, there is a lackof interoperability that allow developers to interchange their information freely and to formstronger partnerships. In this paper we present the Video Game Ontology (VGO), a model forenabling interoperability among video games and enhancing data analysis of gameplayinformation. We describe the creation process of the ontology, the ontology conceptualizationand its evaluation. In addition, we demonstrate the applicability of the Video Game Ontologyin action with three example games that take advantage of the created ontology. Also, wedemonstrate the use of the VGO in enabling interoperability among the example games.Keywords Video game ontology. Video game interoperability . Ontology evaluation1 IntroductionDuring the last decade, the video game industry has changed greatly due to growth of mobilegames and digital distribution channels [12], such as Steam1 and Google Play Store.2 Withintroduction of casual games like Angry Birds3 and Candy Crush Saga4 the video gameindustry has gained a significant growth boost even further [6]. Playing video games hasMultimed Tools ApplDOI 10.1007/s11042-016-3552-61http://store.steampowered.com/2https://play.google.com3https://www.angrybirds.com/4http://candycrushsaga.com/* Janne Parkkilajanne.parkkila@lut.fi1 School of Business and Management, Innovation and Software, Lappeenranta University ofTechnology, Lappeenranta, Finland2 Ontology Engineering Group Madrid, Universidad Politecnica de Madrid, Madrid, Spainhttp://crossmark.crossref.org/dialog/?doi=10.1007/s11042-016-3552-6&domain=pdfhttp://dx.doi.org/http://store.steampowered.com/http://dx.doi.org/http://store.steampowered.com/http://dx.doi.org/http://store.steampowered.com/http://dx.doi.org/http://store.steampowered.com/become an accepted form of entertainment and video games are played among all ages andtypes of people [4]. It is estimated that the total video game market will grow to be over 100billion US dollars by 2018 [8].This growth has also lead to a fierce competition between game companies, leading intoproblems such as visibility of new games in digital marketplaces [11] and grown budgetrequirements for marketing. Similar problems are familiar in other business fields and mar-keting professionals have recognized co-branding [1, 17] and cross-promotion [5, 14] aspossible solutions to address these issues. In co-branding, the goal is to form a partnershipbetween two or more companies and to market products together. In cross- promotion, closelyrelated products are marketed together, such as teleoperators offering their customers discountsfor music streaming services. Taking into account the perspective of co-operation among thegame developers, the possibility of connecting games together may enable new benefits for thegame companies [2], such as improving player engagement and boosting player discovery ofnew games based on their previously played games. However, this requires technologicalsolutions to enable interoperability among separate video game products. To this date,exchanging information between video games has been achieved in an ad-hoc manner amongsingle products, but there are no general solutions for interoperability between video games.Exchanging game-related information between two games may enable players to usecommon items that can be carried over from one game to another. In addition, completing",
        "publication_date": "2016-05-17",
        "authors": "Janne Parkkila, Filip Radulović, Daniel Garijo, María Poveda‐Villalón, Jouni Ikonen, Jari Porras, Asunción Gómez‐Pérez",
        "file_name": "20250512001414.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/vgo/PDFs/20250512001414.pdf",
        "pdf_link": "https://dgarijo.com/papers/vgo.pdf",
        "file_html": "html/20250512001414.html",
        "funding": "FPU FPU2012/04084",
        "url": "https://dgarijo.com/papers/vgo.pdf",
        "pages": "4981--5000",
        "number": "4",
        "volume": "76",
        "publisher": "Springer US",
        "journal": "Multimedia Tools and Applications",
        "year": "2016",
        "author": "Parkkila, Janne and Radulovic, Filip and Garijo, Daniel and Poveda-Villal{\\'o}n, Mar{\\'i}a and Ikonen, Jouni and Porras, Jari and G{\\'o}mez-P{\\'e}rez, Asunci{\\'o}n",
        "ENTRYTYPE": "article",
        "ID": "parkkila2017ontology"
    },
    {
        "title": "Towards workflow ecosystems through semantic and standard representations",
        "implementation_urls": [],
        "abstract": "ABSTRACT Workflows are increasingly used to manage and share scientific computations and methods. Workflow tools can be used to design, validate, execute and visualize scientific workflows and their execution results. Other tools manage workflow libraries or mine their contents. There has been a lot of recent work on workflow system integration as well as common workflow interlinguas, but the interoperability among workflow systems remains a challenge. Ideally, these tools would form a workflow ecosystem such that it should be possible to create a workflow with a tool, execute it with another, visualize it with another, and use yet another tool to mine a repository of such workflows or their executions.  In this paper, we describe our approach to create a workflow ecosystem through the use of standard models for provenance (OPM and W3C PROV) and extensions (P-PLAN and OPMW) to represent workflows.  We introduce WEST, a workflow ecosystem that integrates different workflow tools with diverse functions (workflow generation, execution, browsing, mining, and visualization) created by a variety of research groups. This is, to our knowledge, the first time that such a variety of functions and systems are integrated. Categories and Subject Descriptors C. Computer systems organization, D.2 Software engineering, D.2.10 Design.  General Terms Management, Documentation, Design, Reliability, Standardization. Keywords Scientific workflows, workflow ecosystems, interoperability, OPMW, PROV, P-Plan, WINGS.  1. INTRODUCTION The interoperability of workflow systems is an active area of research, including standard languages for representing workflow executions as provenance [24] [18], integration efforts that demonstrate the exchange of workflows across different systems [21] [17], and architectures with a modular design that can integrate alternative systems such as alternative execution engines [13].  As workflow technologies continue to mature, new tools are being developed with new functions that address different requirements for workflows, including design [26] [19] [11] [30] [25], validation [11], execution [5] [30] [25] [23], visualization [25] [11] [26], and mining [10] [14] [29].  The interoperability of all these different workflow tools remains an open research area. Ideally, given some requirements for an application it would be easy to assemble an end-to-end system out of the workflow tools available. Another scenario for interoperability is supporting users that have new requirements over time.  For example, a user could execute workflows with a particular a tool for many months and then later want to import all the workflow runs into another tool ",
        "publication_date": "2014-01-01",
        "authors": "Daniel Garijo Verdejo, Yolanda Gil, Óscar Corcho",
        "file_name": "20250512001235.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/west-works14/PDFs/20250512001235.pdf",
        "pdf_link": "https://dgarijo.com/papers/west-works14.pdf",
        "file_html": "html/20250512001235.html",
        "organization": "IEEE Press",
        "url": "https://dgarijo.com/papers/west-works14.pdf",
        "doi": "10.1109/WORKS.2014.13",
        "pages": "94--104",
        "booktitle": "Proceedings of the 9th Workshop on Workflows in Support of Large-Scale Science",
        "year": "2014",
        "author": "Garijo, Daniel and Gil, Yolanda and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2014towards"
    },
    {
        "title": "{WIDOCO}: a wizard for documenting ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/Widoco",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "FILE_CFF",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "README_BIBTEX",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_FILE",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    },
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEFCompleta/widoco-iswc2017/PDFs/20250512001927.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "WIDOCO is available in GitHub,12 where users can download it, open issues or ask for help."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.591294",
                "type": "zenodo",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_9",
        "abstract": "Abstract. In this paper we describe WIDOCO, a WIzard for DOCu-menting Ontologies that guides users through the documentation processof their vocabularies. Given an RDF vocabulary, WIDOCO detects miss-ing vocabulary metadata and creates a documentation with diagrams,human readable descriptions of the ontology terms and a summary ofchanges with respect to previous versions of the ontology. The docu-mentation consists on a set of linked enriched HTML pages that can befurther extended by end users. WIDOCO is open source and builds onwell established Semantic Web tools. So far, WIDOCO has been used todocument more than one hundred ontologies in different domains.Keywords: Ontology documentation, Ontology evolution, Ontology un-derstanding, OWL OntologiesResource Type: SoftwarePermanent URL: https://w3id.org/widocoSoftware DOI: https://doi.org/10.5281/zenodo.5912941 IntroductionOntology engineering methodologies acknowledge reuse of existing vocabulariesas a crucial step when developing a new ontology [11]. Therefore, ontology au-thors often provide a human-readable documentation of their vocabularies, inorder to facilitate their understanding and adoption by other researchers [9].There are three main aspects related to ontology documentation. The firstone is creating a human-readable representation of the content of the ontology:metadata, definition of classes and properties, visualization (e.g., diagrams relat-ing the different concepts) and versioning (explanation of the difference betweenversions of the ontologies). The second aspect is creating machine-readable an-notations of documentation metadata (e.g., provenance, snippets for facilitatingvocabulary discovery by search engines) and the third aspect is preparing thedocumentation files to be accessed as a web resource (doing content negotiation).Related work has been proposed to facilitate some of these aspects. For ex-ample, ontology editors like Protégé [8], have plugins for automatically creatingan HTML documentation with the definition of classes and properties.1 Simi-larly, approaches like LODE [9] or Parrot [12] provide drag-and-drop services to1 https://protegewiki.stanford.edu/wiki/OWLDocautomatically document ontology terms. However, most approaches are typicallydesigned for Semantic Web experts, presenting some of the following issues:1. Lack of guidelines and best practices for ontology documentation: users de-veloping ontologies may not know which are the common terms used todescribe the metadata of their ontologies. These metadata are important,because they are used by existing tools to create human readable descrip-tions of an ontology.2. Lack of ontology metadata completion: Current efforts do not indicate whichkey information may be missing when documenting an ontology.3. Lack of an ecosystem for ontology documentation and customization: mostexisting approaches focus on specific aspects of ontology documentation.On the one hand, approaches like LODE [9] generate a human readabledescription of the classes and properties of a given ontology, but neglect thegeneration of diagrams. On the other hand, tools like WebVowl [5] createdynamic visualizations of ontologies, but do not deal with the generation oftext. Integrating the outcome of these and other tools and customizing themaccording to user preferences takes time, especially to non programmers.",
        "publication_date": "2017-01-01",
        "authors": "Daniel Garijo",
        "file_name": "20250512001927.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/widoco-iswc2017/PDFs/20250512001927.pdf",
        "pdf_link": "https://dgarijo.com/papers/widoco-iswc2017.pdf",
        "file_html": "html/20250512001927.html",
        "funding": "USNSF ICER-1541029, NIH 1R01GM117097-01",
        "organization": "Springer, Cham",
        "url": "https://dgarijo.com/papers/widoco-iswc2017.pdf",
        "pages": "94--102",
        "volume": "10588",
        "booktitle": "d'Amato C. et al. (eds) The Semantic Web – ISWC 2017",
        "year": "2017",
        "author": "Garijo, Daniel",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2017widoco"
    },
    {
        "title": "A new approach for publishing workflows: abstractions, standards, and linked data",
        "implementation_urls": [],
        "abstract": "Abstractions, Standards, and Linked Data  Daniel Garijo OEG-DIA Facultad de Informática Universidad Politécnica de Madrid dgarijo@delicias.dia.fi.upm.es Yolanda Gil Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 gil@isi.edu  ABSTRACT In recent years, a variety of systems have been developed that export the workflows used to analyze data and make them part of published articles. We argue that the workflows that are published in current approaches are dependent on the specific codes used for execution, the specific workflow system used, and the specific workflow catalogs where they are published. In this paper, we describe a new approach that addresses these shortcomings and makes workflows more reusable through: 1) the use of abstract workflows to complement executable workflows to make them reusable when the execution environment is different, 2) the publication of both abstract and executable workflows using standards such as the Open Provenance Model that can be imported by other workflow systems, 3) the publication of workflows as Linked Data that results in open web accessible workflow repositories. We illustrate this approach using a complex workflow that we re-created from an influential publication that describes the generation of ‘drugomes’. Categories and Subject Descriptors C. Computer systems organization, D.2 Software engineering, D.2.10 Design. General Terms Documentation, Performance, Design, Standardization. Keywords Workflows, provenance, OPM, Wings, reproducibility. 1. INTRODUCTION Scientific workflows are products of research and should be treated as first-class citizens in cyberinfrastructure [9]. Workflows represent computations carried out to obtain scientific results, but these computations are only described in the narrative of published scientific articles and only at a very high level. Scientific articles describe computational methods informally, often requiring a significant effort from others to reproduce and to reuse. The reproducibility process can be so costly that it has been referred to as “forensic” research [1]. Studies have shown that reproducibility is not achievable from the article itself, even when datasets are published [3], [15]. Retractions of publications do ",
        "file_name": "20250512000825.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/works11/PDFs/20250512000825.pdf",
        "pdf_link": "https://dgarijo.com/papers/works11.pdf",
        "file_html": "html/20250512000825.html",
        "organization": "ACM",
        "funding": "USNSF CCF-0725332, AFOSR FA9550-11-1-0104",
        "url": "https://dgarijo.com/papers/works11.pdf",
        "doi": "10.1145/2110497.2110504",
        "pages": "47--56",
        "booktitle": "Proceedings of the 6th workshop on Workflows in support of large-scale science",
        "year": "2011",
        "author": "Garijo, Daniel and Gil, Yolanda",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2011new"
    },
    {
        "title": "On specifying and sharing scientific workflow optimization results using research objects",
        "implementation_urls": [],
        "doi": "10.1145/2534248.2534251",
        "abstract": "ABSTRACTReusing and repurposing scientific workflows for novel scien-tific experiments is nowadays facilitated by workflow repos-itories. Such repositories allow scientists to find existingworkflows and re-execute them. However, workflow inputparameters often need to be adjusted to the research problemat hand. Adapting these parameters may become a dauntingtask due to the infinite combinations of their values in a widerange of applications. Thus, a scientist may preferably usean automated optimization mechanism to adjust the work-flow set-up and improve the result. Currently, automatedoptimizations must be started from scratch as optimizationmeta-data are not stored together with workflow provenancedata. This important meta-data is lost and can neither bereused nor assessed by other researchers. In this paper wepresent a novel approach to capture optimization meta-databy extending the Research Object model and reusing theW3C standards. We validate our proposal through a real-world use case taken from the biodivertsity domain, anddiscuss the exploitation of our solution in the context ofexisting e-Science infrastructures.KeywordsScientific Workflows, Optimization, Research Object, Ontol-ogy, Taverna1. INTRODUCTIONIn recent years scientific workflows have emerged as analternative to script programming for performing in-silicoexperiments. Scientific workflows describe the set of tasksPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from Permissions@acm.org.WORKS13, November 17, 2013, Denver, CO, USACopyright is held by the owner/author(s).Publication rights licensed to ACM.ACM 978-1-4503-2502-8/13/11 ...$15.00http://dx.doi.org/10.1145/2534248.2534251.needed to carry out a computational experiment [5]. Sim-ilar to the ’mashup’ concept in web programming, manyscientific ideas can be phrased as different combinations ofexisting algorithmic building blocks (also known as compo-nents). Scientific workflows have been particularly attractiveto scientists aiming to expose, share and reuse their work [14],as they help specifying the methods used for each step ofthe experiment. Consequentially, e-Science environmentshave started to provide public repositories for collection andsharing of scientific workflows [20] thereby providing a sourcefor large amounts of material for assembling novel methods.",
        "publication_date": "2013-10-23",
        "authors": "Sonja Holl, Daniel Garijo, Khalid Belhajjame, Olav Zimmermann, Renato De Giovanni, Matthias Obst, Carole Goble",
        "file_name": "20250512001124.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/works13/PDFs/20250512001124.pdf",
        "pdf_link": "https://dgarijo.com/papers/works13.pdf",
        "file_html": "html/20250512001124.html",
        "organization": "ACM",
        "url": "https://dgarijo.com/papers/works13.pdf",
        "pages": "28--37",
        "booktitle": "Proceedings of the 8th Workshop on Workflows in Support of Large-Scale Science",
        "year": "2013",
        "author": "Holl, Sonja and Garijo, Daniel and Belhajjame, Khalid and Zimmermann, Olav and De Giovanni, Renato and Obst, Matthias and Goble, Carole",
        "ENTRYTYPE": "inproceedings",
        "ID": "holl2013specifying"
    },
    {
        "title": "Lightning Talk:\" I solemnly pledge\": A Manifesto for Personal Responsibility in the Engineering of Academic Software",
        "implementation_urls": [],
        "abstract": "Abstract— Software is fundamental to academic research work, both as part of the method and as the result of research. In June 2016 25 people gathered at Schloss Dagstuhl for a week-long Per-spectives Workshop and began to develop a manifesto which places emphasis on the scholarly value of academic software and on personal responsibility. Twenty pledges cover the recognition of academic software, the academic software process and the intellectual content of academic software. This is still work in progress. Through this lightning talk, we aim to get feedback and hone these further, as well as to inspire the WSSSPE audience to think about actions they can take themselves rather than actions they want others to take. We aim to publish a more fully devel-oped Dagstuhl Manifesto by December 2016. Index Terms—software, manifesto. I. INTRODUCTION Software is fundamental to academic research work, both as part of the method and as the result of research. With the ad-vent of artifact evaluation committees of conferences, journals that include source code and running systems as part of the published artifacts, as well as the increasing push to reproduci-bility, we foresee that software will only increase in importance as part of the academic process.  In June 2016, 25 people gathered at Schloss Dagstuhl for a weeklong Perspectives Workshop [1] to produce a roadmap towards future professional software engineering for software-based research instruments and other software produced and   This work is licensed under a CC-BY-4.0 license. used in an academic context (i.e., for research, not administra-tion). The group was carefully picked to be broad in its range of disciplines (including Astronomy, Social Sciences, Biology, Chemistry, Computer Science, Physics, and Humanities), roles (including computer science researchers, general and specialist research software engineers and systems administrators) and career stages (from PIs and institute heads to PhD students and postdocs). Despite its ambiguous title, “Engineering Academic Software,” the workshop was on the Engineering of Academic Software (not software for engineers). A Dagstuhl Perspectives Workshop results in a Manifesto. The open science and research software communities have been very active in creating manifestos: the Science Code Man-ifesto [2], Karlskrona Manifesto for Sustainability Design [3], the Reproducibility Manifesto [4], and Principles for Software Citation [5], FAIR data [6], and so on. Why do we need anoth-er Manifesto? First, our manifesto is to be less about what others should do, and more about what we, as individuals, should do. That is, it is more in the style of a personal responsibility pledge like those on open access [7] and open peer review [8]. It is easy for ",
        "file_name": "20250512001453.pdf",
        "file_path": "../Pruebas/SalidaRSEFCompleta/WSSSPE4_paper_15/PDFs/20250512001453.pdf",
        "pdf_link": "https://dgarijo.com/papers/WSSSPE4_paper_15.pdf",
        "file_html": "html/20250512001453.html",
        "url": "https://dgarijo.com/papers/WSSSPE4_paper_15.pdf",
        "publisher": "CEUR",
        "year": "2016",
        "author": "Allen, Alice and Aragon, Cecilia and Becker, Christoph and Carver, Jeffrey and Chis, Vasile-Andrei and Combemale, Benoit and Croucher, Mike and Crowston, Kevin and Garijo, Daniel and Gehani, Ashish and others",
        "ENTRYTYPE": "article",
        "ID": "allen2016lightning"
    }
]