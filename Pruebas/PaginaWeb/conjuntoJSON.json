[
    {
        "title": "Good practice versus reality: A landscape analysis of Research Software metadata adoption in European Open Science Clusters",
        "implementation_urls": [],
        "abstract": "Abstract—Research Software has become a key asset to sup-port the results described in academic publications, enablingeffective data analysis and reproducibility. In order to ensureadherence of Research Software to the Findable, Accessible,Interoperable, and Reusable (FAIR) principles, the scientificcommunity has proposed metadata guidelines and best practices.However, it is unclear how these practices have been adopted sofar. This paper examines how different scientific communitiesdescribe Research Software with metadata to support FAIR,how do they adopt existing good practices regarding citation,documentation or versioning, and what is the current adoptionof archival services for long-term preservation. We carry outour analysis in the software registries of five science clusters(in domains ranging from Physics to Environmental Sciences),together with a multi-domain collaborative software registry.Our results highlight the main gaps in metadata adoption inthe different communities, opening an opportunity for futurecontributions to aid researchers in adopting good FAIR and OpenScience practices.Index Terms—Research Software, Metadata, FAIR software,FAIR principles, Guidelines.I. INTRODUCTIONResearch Software, i.e., the code files, scripts, tools, orworkflows involved in or produced throughout the researchlifecycle [1] plays a pivotal role in modern scientific re-search, to support the research outputs described in scientificpublications. Understanding the role of Research Softwareis essential not only for supporting research efficiency butalso for ensuring reproducibility. In this regard, the scientificcommunity has promoted initiatives towards adopting OpenScience best practices, such as the Findable, Accessible,Interoperable and Reusable (FAIR) principles for data [2],which have also been extended for Research Software [3].The adoption of FAIR is expected to ease the reuse of dataand software while allowing automated systems to retrieve andinteroperate with (meta)data. In fact, compliance with FAIRis now part of the agenda of international organizations suchas the European Commission, through the European OpenScience Cloud initiative [4], or NASA [5].11https://science.nasa.gov/open-science/In order to ease the adoption of FAIR and Open Science,several efforts have developed guidelines, lessons, and goodpractices for researchers to improve the metadata available intheir code repositories (e.g., the Research Software MetaDataGuidelines (RSMD) [6], Software Carpentry2, Open SourceSecurity Foundation3, etc.). However, it is unclear how thesepractices are adopted by different scientific communities,making it challenging to assess their impact.In this paper, we address this issue by assessing the adoptionof Research Software metadata best practices (inspired by",
        "file_name": "20250510123912.pdf",
        "file_path": "../Pruebas/SalidaRSEF/El_Hounsri_MSR_2025_landscape_analysis_CR/PDFs/20250510123912.pdf",
        "pdf_link": "https://dgarijo.com/papers/El_Hounsri_MSR_2025_landscape_analysis_CR.pdf",
        "file_html": "html/20250510123912.html",
        "url": "https://dgarijo.com/papers/El_Hounsri_MSR_2025_landscape_analysis_CR.pdf",
        "series": "MSR '25",
        "publisher": "Association for Computing Machinery",
        "booktitle": "To appear in Proceedings of the Mining Software Repositories Conference, 2025)",
        "year": "2025",
        "author": "El Hounsri, Anas and Garijo, Daniel",
        "ENTRYTYPE": "inproceedings",
        "ID": "el_hounsri_25"
    },
    {
        "title": "Farm Explorer: A Tool for Calculating Transparent Greenhouse Gas Emissions",
        "implementation_urls": [],
        "abstract": "AbstractThis demo provides an overview of Farm Explorer , a tool designed to calculate greenhouse emissionson farms in a transparent manner. Farm Explorer uses semantic descriptions of emission calculationformulas by leveraging knowledge graphs containing static and dynamic information about the farmoperation and emission conversion factors. To enhance the transparency of emissions calculations, thetool records the provenance of the calculation process using standards like W3C PROV and W3C SOSA.Demo: https://w3id.org/tec-toolkit/ISWC-2024-demoSource: https://github.com/eats-project/farm-explorerKeywordsprovenance, carbon footprint, transparency, knowledge graph1. IntroductionThe agrifood systems, responsible for about a third of global anthropogenic greenhouse gasemissions [1], heavily rely on commercial carbon calculator tools [2]. Such tools aggregateresults calculated using various emission methodologies, data sources (e.g., GFLI for feeds [3])and carbon standards (e.g., GHG Protocol [4]) applied to specific aspects of agrifood activities(e.g., primary production on farms). Businesses provide data inputs that often need to belaboriously extracted from heterogeneous sources (e.g., sensors, manual records, machinerylogs, etc.). For example, to estimate emissions on a farm, a calculator may consider the electricityrequired to operate heavy machinery, use of fertilisers, amount of manure produced, andother logistics. Here, a typical emission calculation would estimate the amount of emissions-Posters, Demos, and Industry Tracks at ISWC 2024, November 13–15, 2024, Baltimore, USA∗Corresponding author.Envelope-Open milan.markovic@abdn.ac.uk (M. Markovic); stefano.germano@cs.ox.ac.uk (S. Germano); daniel.garijo@upm.es(D. Garijo); p.edwards@abdn.ac.uk (P. Edwards); a.li.21@abdn.ac.uk (A. Li); tewodrosalemu.ayall@abdn.ac.uk(T. A. Ayall); rachael.ramsey@sac.co.uk (R. Ramsey); georgios.leontidis@abdn.ac.uk (G. Leontidis)Orcid 0000-0002-5477-287X (M. Markovic); 0000-0001-6993-0618 (S. Germano); 0000-0003-0454-7145 (D. Garijo);0000-0002-0877-7063 (P. Edwards); 0009-0004-1103-8300 (A. Li); 0000-0001-9413-6459 (T. A. Ayall);0000-0001-9984-7241 (R. Ramsey); 0000-0001-6671-5568 (G. Leontidis)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).https://w3id.org/tec-toolkit/ISWC-2024-demohttps://github.com/eats-project/farm-explorermailto:milan.markovic@abdn.ac.ukmailto:stefano.germano@cs.ox.ac.ukmailto:daniel.garijo@upm.esmailto:p.edwards@abdn.ac.ukmailto:a.li.21@abdn.ac.ukmailto:tewodrosalemu.ayall@abdn.ac.ukmailto:rachael.ramsey@sac.co.ukmailto:georgios.leontidis@abdn.ac.ukhttps://orcid.org/0000-0002-5477-287Xhttps://orcid.org/0000-0001-6993-0618https://orcid.org/0000-0003-0454-7145https://orcid.org/0000-0002-0877-7063https://orcid.org/0009-0004-1103-8300https://orcid.org/0000-0001-9413-6459https://orcid.org/0000-0001-9984-7241https://orcid.org/0000-0001-6671-5568https://creativecommons.org/licenses/by/4.0generating resources (i.e., activity data) and multiply them by their corresponding emissionconversion factor. We argue that semantic web technologies are ideally positioned to provide",
        "file_name": "20250510123926.pdf",
        "file_path": "../Pruebas/SalidaRSEF/farm_explorer/PDFs/20250510123926.pdf",
        "pdf_link": "https://dgarijo.com/papers/farm_explorer.pdf",
        "file_html": "html/20250510123926.html",
        "year": "2024",
        "url": "https://dgarijo.com/papers/farm_explorer.pdf",
        "booktitle": "To appear in The 23rd International Semantic web Conference: ISWC 2024 Posters and Demos",
        "author": "Markovic, Milan and Germano, Stefano and Garijo, Daniel and Edwards, Pete and Li, Andy and Ayall, Tewodros Alemu and Ramsey, Rachael and Leontidis, Georgios",
        "ENTRYTYPE": "inproceedings",
        "ID": "markovic2024farm"
    },
    {
        "title": "Ontology Engineering and the FAIR principles: A Gap Analysis toward a FAIR-by-design methodology",
        "implementation_urls": [],
        "abstract": "AbstractOntologies and vocabularies play a key role when standardising, organizing and integrating data fromheterogeneous data sources into Knowledge Graphs. In order to develop ontologies, different engineeringmethodologies have been proposed throughout the years, whose application resulted in thousands ofsemantic artefacts (taxonomies, vocabularies and ontologies) in a wide range of domains. But how toensure that ontologies follow the Findable, Accessible, Interoperable and Reusable principles (FAIR) fromtheir inception? In this paper, we review: (i) existing guidelines to help make ontologies FAIR and (ii)published FAIRness assessment methodologies and map them to the ontology development lifecycleactivities. Our analysis outlines the current gaps, where no guidelines exist for ontologies to becomeFAIR-by-design.KeywordsOntology Engineering, FAIR principles, FAIRness assessment, FAIR-by-design, Semantic Artefacts,Ontologies, Vocabularies1. IntroductionOntologies and vocabularies play a key role in data integration by defining the structure, guidingthe construction, and validating Knowledge Graphs. Ontologies are widely used in multipledomains, ranging from Biomedicine [1] and Astrophysics [2] to Smart Cities [3] or Web contentannotation [4].Proceedings of the Joint Ontology Workshops (JOWO) - Episode X: The Tukker Zomer of Ontology, and satellite eventsco-located with the 14th International Conference on Formal Ontology in Information Systems (FOIS 2024), July 15-19,2024, Enschede, The Netherlands.*Corresponding author.†These authors contributed equally.$ m.poveda@upm.es (M. Poveda-Villalón); daniel.garijo@upm.es (D. Garijo);alejandra.gonzalez-beltran@stfc.ac.uk (A. N. Gonzalez-Beltran); clement.jonquet@inrae.fr (C. Jonquet);ylefranc@esciencefactory.com (Y. L. Franc)� https://agbeltran.github.io/ (A. N. Gonzalez-Beltran)� 0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0003-0454-7145 (D. Garijo); 0000-0003-3499-8262(A. N. Gonzalez-Beltran); 0000-0002-2404-1582 (C. Jonquet); 0000-0003-4631-418X (Y. L. Franc)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingsceur-ws.orgISSN 1613-0073mailto:m.poveda@upm.esmailto:daniel.garijo@upm.esmailto:alejandra.gonzalez-beltran@stfc.ac.ukmailto:clement.jonquet@inrae.frmailto:ylefranc@esciencefactory.comhttps://agbeltran.github.io/https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0003-0454-7145https://orcid.org/0000-0003-3499-8262https://orcid.org/0000-0002-2404-1582https://orcid.org/0000-0003-4631-418Xhttps://creativecommons.org/licenses/by/4.0A number of ontology engineering methodologies have been proposed by researchers throughthe years in order to build ontologies [5, 6, 7, 8, 9, 10, 11, 12] and finally [13] that we, in part,",
        "publication_date": "2024-07-15",
        "authors": "María Poveda‐Villalón, Daniel Garijo, Alejandra González-Beltrán, Clément Jonquet, Yann Le Franc",
        "file_name": "20250510124028.pdf",
        "file_path": "../Pruebas/SalidaRSEF/foam-4/PDFs/20250510124028.pdf",
        "pdf_link": "https://ceur-ws.org/Vol-3882/foam-4.pdf",
        "file_html": "html/20250510124028.html",
        "url": "https://ceur-ws.org/Vol-3882/foam-4.pdf",
        "volume": "3882",
        "series": "{CEUR} Workshop Proceedings",
        "publisher": "CEUR-WS.org",
        "booktitle": "FAIR principles for Ontologies and Metadata in Knowledge Management (FOAM 2024)",
        "year": "2024",
        "author": "Poveda-Villalon, Mar{\\'\\i}a and Garijo, Daniel and Gonzalez-Beltran, Alejandra and Jonquet, Clement and Le Franc, Yann",
        "ENTRYTYPE": "inproceedings",
        "ID": "poveda2024ontology"
    },
    {
        "title": "LLMs for Ontology Engineering: A landscape of Tasks and Benchmarking challenges",
        "implementation_urls": [],
        "abstract": "AbstractLarge Language Models (LLMs) have emerged as a powerful technology for text generation tasks, showingpromise in supporting the Ontology Engineering (OE) process. In this paper, we review current research onapplying LLMs to OE tasks, aiming to identify commonalities and gaps in the state of the art. We categorize theseefforts using the Linked Open Terms (LOT) methodology, characterizing them by their input and expected output.From this analysis, we highlight key challenges when creating benchmarks to evaluate LLM performance in OEtasks.KeywordsLarge Language Models, Ontology Engineering, Benchmark, Challenges1. IntroductionOntologies are a key component of Knowledge Engineering for integrating, validating and reasoningwith data in Knowledge Graphs [1]. However, developing ontologies is a challenging and time consumingtask. According to existing methodologies for ontology development [2], Knowledge Engineers shouldfollow an iterative process to 1) distill the knowledge of the target domain by interviewing expertsand understand their data-driven requirements, 2) implement a shared conceptualization by assessingexisting standard ontologies described in the domain and validating it against the requirements, 3) makethe ontology available on the web in both human and machine-readable manner, and 4) assess andmaintain the ontology by addressing any new requirements that may arise from its use. While differenttools have been developed by the scientific community to assist in the Ontology Engineerning process(e.g., for formalizing tests to assess requirements [3], creating human-readable documentation [4],ontology assessment [5, 6], etc.) a significant manual effort is still required from knowledge engineersfor conceptualizing, reusing and validating existing ontologies.In recent years, Large Language Models (LLMs) [7, 8, 9] have emerged as a disruptive AI technologyfor text generation tasks. On the one hand, LLMs have revolutionized the state of the art by providingimpressive results in challenging AI tasks such as code generation [10], question answering [11] ortext summarization [7], and becoming easy to adapt as chat bots such as ChatGPT.1 On the other hand,LLMs have limited reasoning skills [12], hallucination problems (i.e., producing inaccurate answers andinformation) [13], lack transparency when providing results [14] and present bias problems [15].A number of works have started using LLMs for aiding developers in ontology engineering tasks(e.g., proposing competency questions [16], learning ontologies from text [17], aligning concepts toexisting taxonomies [18], etc.). However, the tasks addressed in existing works are usually defined inan heterogeneous manner, with different scope, inputs and expected outputs. In this paper we providean overview of existing Ontology Engineering tasks addressed in the state of the art and map themISWC’24, Special Session on LLMs, November 11 - 15, 2024, Maryland, USAEnvelope-Open daniel.garijo@upm.es (D. Garijo); m.poveda@upm.es (M. Poveda-Villalón); elvira.amador@upm.es(E. Amador-Domínguez); ziyuan.wang@upm.es (Z. Wang); r.garcia@upm.es (R. García-Castro); oscar.corcho@upm.es(O. Corcho)Orcid 0000-0003-0454-7145 (D. Garijo); 0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0001-6838-1266 (E. Amador-Domínguez);0009-0000-6228-4713 (Z. Wang); 0000-0002-0421-452X (R. García-Castro); 0000-0002-9260-0753 (O. Corcho)© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).1https://chat.openai.com/mailto:daniel.garijo@upm.esmailto:m.poveda@upm.esmailto:elvira.amador@upm.esmailto:ziyuan.wang@upm.esmailto:r.garcia@upm.esmailto:oscar.corcho@upm.eshttps://orcid.org/0000-0003-0454-7145https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0001-6838-1266",
        "file_name": "20250510123942.pdf",
        "file_path": "../Pruebas/SalidaRSEF/iswc_llm/PDFs/20250510123942.pdf",
        "pdf_link": "https://dgarijo.com/papers/iswc_llm.pdf",
        "file_html": "html/20250510123942.html",
        "year": "2024",
        "url": "https://dgarijo.com/papers/iswc_llm.pdf",
        "booktitle": "To appear in the Proceedings of the 23rd International Semantic web Conference (Special session on LLMs)",
        "author": "Garijo, Daniel and Poveda-Villal{\\'o}n, Mar{\\'i}a and Amador-Dominguez, Elvira and Wang, Ziyuan and Garc{\\'i}a-Castro, Raul and Corcho, Oscar",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo_llm"
    },
    {
        "title": "Recording provenance of workflow runs with RO-Crate",
        "implementation_urls": [],
        "doi": "10.1371/journal.pone.0309210",
        "publication_date": "2024-09-10",
        "authors": "Simone Leo, Michael R. Crusoe, Laura Rodríguez‐Navas, Raúl Sirvent, Alexander Kanitz, Paul De Geest, Rudolf Wittner, Luca Pireddu, Daniel Garijo, José M. Fernández, Iacopo Colonnelli, Matej Gallo, Tazro Ohta, Hirotaka Suetake, Salvador Capella-Gutiérrez, Renske de Wit, Bruno P. Kinoshita, Stian Soiland‐Reyes",
        "file_name": "10!1371%journal!pone!0309210.pdf",
        "file_path": "../Pruebas/SalidaRSEF/journal.pone/PDFs/10!1371%journal!pone!0309210.pdf",
        "file_html": "html/10!1371%journal!pone!0309210.html",
        "number": "9",
        "abstract": "Recording the provenance of scientific computation results is key to the support of traceability, reproducibility and quality assessment of data products. Several data models have been explored to address this need, providing representations of workflow plans and their executions as well as means of packaging the resulting information for archiving and sharing. However, existing approaches tend to lack interoperable adoption across workflow management systems. In this work we present Workflow Run RO-Crate, an extension of RO-Crate (Research Object Crate) and Schema.org to capture the provenance of the execution of computational workflows at different levels of granularity and bundle together all their associated objects (inputs, outputs, code, etc.). The model is supported by a diverse, open community that runs regular meetings, discussing development, maintenance and adoption aspects. Workflow Run RO-Crate is already implemented by several workflow management systems, allowing interoperable comparisons between workflow runs from heterogeneous systems. We describe the model, its alignment to standards such as W3C PROV, and its implementation in six workflow systems. Finally, we illustrate the application of Workflow Run RO-Crate in two use cases of machine learning in the digital image analysis domain.",
        "pages": "1-35",
        "url": "https://doi.org/10.1371/journal.pone.0309210",
        "volume": "19",
        "month": "09",
        "year": "2024",
        "publisher": "Public Library of Science",
        "journal": "PLOS ONE",
        "author": "Leo, Simone AND Crusoe, Michael R. AND Rodríguez-Navas, Laura AND Sirvent, Raül AND Kanitz, Alexander AND De Geest, Paul AND Wittner, Rudolf AND Pireddu, Luca AND Garijo, Daniel AND Fernández, José M. AND Colonnelli, Iacopo AND Gallo, Matej AND Ohta, Tazro AND Suetake, Hirotaka AND Capella-Gutierrez, Salvador AND de Wit, Renske AND Kinoshita, Bruno P. AND Soiland-Reyes, Stian",
        "ENTRYTYPE": "article",
        "ID": "10.1371/journal.pone.0309210"
    },
    {
        "title": "RepoFromPaper: An Approach to Extract Software Code Implementations from Scientific Publications",
        "implementation_urls": [
            {
                "identifier": "https://github.com/StankovskiA/RepoFromPaper",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEF/stankovsi_2024/PDFs/20250510124049.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "In summary, the number of implementation links found only by the bi-directional approach was 4, the 14 RepoFromPaper is available at https://github.com/StankovskiA/RepoFromPaper 15 https://arxiv.org/list/cs.AI/recent 16 https://arxiv.org/list/cs.SE/recent 7 Discussion Our approach produces high evaluation results, presents several limitations."
                    }
                ]
            }
        ],
        "doi": "https://doi.org/10.1007/978-3-031-65794-8_7",
        "abstract": "Abstract. An increasing amount of scientists link to their research soft-ware code implementations in their academic publications in order tosupport the reusability of their results. However, research papers usuallycontain many code links (e.g., from reused tools or existing compet-ing efforts) making it challenging to automatically establish clear linksbetween papers and their corresponding implementations. This paperpresents RepoFromPaper, an approach for automatically extracting themain code implementation associated with a research paper, based on thecontext in which that link is mentioned. Our approach uses fine-tunedlanguage models to retrieve the top candidate sentences where a code im-plementation may be found, and uses custom heuristics to link candidatesentences back to their corresponding URL (footnote, reference or full-text mention). We evaluated RepoFromPaper on 150 research papers,obtaining an F1 score of 0.94. We also run our approach on nearly 1800papers from the CS.AI Arxiv category, discovering 604 paper-repositorylinks and making them available to the community.Keywords: Information extraction · Research Software · Software repos-itory · Open Science.1 IntroductionResearch Software, i.e., the source code files, algorithms, scripts, computationalworkflows and executables that were created during the research process [2] isbecoming recognized as a first class citizen in scientific curricula.1 In order tosupport the results described in academic publications, scientists often include alink to a code repository (e.g., GitHub, Gitlab) with their technical implemen-tations details.While efforts have been made by the scientific community to establish princi-ples[16] and formats for software citation [5], detecting the code repository linkassociated with a publication has two main challenges. First, authors often citeresearch software inconsistently, employing diverse formats and locations such as1 https://sfdora.org/read/2 A. Stankovski and D. Garijofull-text repository mentions (cases where the link is written in the paragraphs),footnotes, or references to refer to a software component [8]. Second, a publi-cation may contain several code repository links (from tools that are reused, orcompeting with the proposed approach) making it challenging to automaticallydetect the right code implementation.This paper introduces a methodology designed to address these challenges byautomatically extracting the software implementation repository link associatedwith a research paper, based on the context in which the link is mentioned. Thecore contributions of our work include:1. Training and validation datasets of labeled sentences designed to fine-tune and evaluate our approach [17]. The training dataset includes 61 re-search papers related to software engineering available on the PapersWith-Code2 platform. The validation dataset includes 150 software engineering re-search articles extracted from Arxiv. Both datasets encompass various typesof implementation mention sentences to cover the diverse ways authors ref-erence the implementation repository.2. RepoFromPaper3, a tool to automatically extract the code implementa-tion repository from a research paper, including PDF-to-Text conversion,sentence extraction, sentence classification and link search, as well as three",
        "publication_date": "2024-01-01",
        "authors": "Aleksandar Stankovski, Daniel Garijo",
        "file_name": "20250510124049.pdf",
        "file_path": "../Pruebas/SalidaRSEF/stankovsi_2024/PDFs/20250510124049.pdf",
        "pdf_link": "https://dgarijo.com/papers/stankovsi_2024.pdf",
        "file_html": "html/20250510124049.html",
        "isbn": "978-3-031-65794-7",
        "url": "https://dgarijo.com/papers/stankovsi_2024.pdf",
        "editor": "Rehm, Georg and Dietze, Stefan and Schimmler, Sonja and Kr{\\\"u}ger, Frank",
        "pages": "114--133",
        "address": "Cham",
        "publisher": "Springer Nature Switzerland",
        "booktitle": "Natural Scientific Language Processing and Research Knowledge Graphs",
        "year": "2024",
        "author": "Stankovski, Aleksandar and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "stankovski2024"
    },
    {
        "title": "Declarative generation of RDF-star graphs from heterogeneous data",
        "implementation_urls": [],
        "abstract": "Abstract. RDF-star has been proposed as an extension of RDF to annotate statements with triples. Libraries and graph storeshave started adopting RDF-star, but the generation of RDF-star data remains largely unexplored. To allow generating RDF-starfrom heterogeneous data, RML-star was proposed as an extension of RML. However, no implementation has been developedso far that implements the RML-star specification. In this work, we present Morph-KGCstar, which extends the Morph-KGCmaterialization engine to generate RDF-star datasets. We validate Morph-KGCstar by running test cases derived from the N-Triples-star syntax tests and we apply it to two real-world use cases from the biomedical and open science domains. We comparethe performance of our approach against other RDF-star generation methods (SPARQL-Anything), showing that Morph-KGCstarscales better for large input datasets, but it is slower when processing multiple smaller files.Keywords: Knowledge Graphs, RDF-star, RML-star, Data Integration1. IntroductionRDF-star [1] was proposed as an extension of RDF [2] to annotate statements and, thus, make statements aboutother statements (also known as reification [3]). RDF-star extends the RDF’s conceptual data model and concretesyntaxes by providing a compact alternative to other reification approaches, such as standard reification [4] orsingleton properties [5]. Following the uptake of the initial version of RDF-star, the W3C RDF-DEV CommunityGroup1 recently released a W3C Final Community Group Report [6] and the RDF-star Working Group2 has recentlybeen formed to extend related W3C Recommendations.Even though several libraries and graph stores have already adopted RDF-star3, the generation of RDF-star graphsremains largely unexplored. RDF graphs are often generated from heterogeneous semi-structured data, e.g., data inCSV, XML or JSON formats, etc. To generate RDF graphs, mapping languages are used to specify how RDF termsand triples can be generated from these data. The syntax of these mapping languages are either custom or repurposed.*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.**The authors contributed equally to this work.1https://www.w3.org/groups/cg/rdf-dev2https://www.w3.org/groups/wg/rdf-star3https://w3c.github.io/rdf-star/implementations1570-0844/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:julian.arenas.guerrero@upm.esmailto:ana.iglesiasm@upm.esmailto:daniel.garijo@upm.esmailto:oscar.corcho@upm.esmailto:david.chaves@kuleuven.bemailto:anastasia.dimou@kuleuven.bemailto:julian.arenas.guerrero@upm.eshttps://www.w3.org/groups/cg/rdf-devhttps://www.w3.org/groups/wg/rdf-starhttps://w3c.github.io/rdf-star/implementations2 J. Arenas-Guerrero et al. / Declarative generation of RDF-star graphs from heterogeneous data1 12 23 34 45 56 67 78 89 910 1011 1112 1213 13",
        "file_name": "20250510124644.pdf",
        "file_path": "../Pruebas/SalidaRSEF/swj_arenas_2024/PDFs/20250510124644.pdf",
        "pdf_link": "https://dgarijo.com/papers/swj_arenas_2024.pdf",
        "file_html": "html/20250510124644.html",
        "url": "https://dgarijo.com/papers/swj_arenas_2024.pdf",
        "year": "2025",
        "pages": "SW-243602",
        "number": "2",
        "volume": "16",
        "journal": "Semantic Web",
        "doi": "10.3233/SW-243602",
        "author": "Arenas-Guerrero, Juli{\\'a}n and Iglesias-Molina, Ana and Chaves-Fraga, David and Garijo, Daniel and Corcho, Oscar and Dimou, Anastasia",
        "ENTRYTYPE": "article",
        "ID": "arenas2024morph"
    },
    {
        "title": "{RMLdoc}: Documenting Mapping Rules for Knowledge Graph Construction",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/rmldoc",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEF/toledo2024rmldoc/PDFs/20250510124701.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Code repository: https://github.com/oeg-upm/rmldoc/Demo: https://w3id.org/rmldoc/example Keywords: Documentation · Knowledge Graph Construction · RML."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-78952-6_51",
        "abstract": "Abstract. In this demo we present RMLdoc, a Python package de-signed to generate documentation for RML mappings when constructingknowledge graphs from heterogeneous sources. Given an input mappingfile written in R2RML, RML, or YARRRML, RMLdoc will generate adetailed Markdown documentation explaining each mapping with corre-sponding diagrams, in a human readable manner. Thanks to RMLdoc,we aim to shed light in the knowledge graph construction process, makingmappings easier to maintain and understand by knowledge engineers.Code repository: https://github.com/oeg-upm/rmldoc/Demo: https://w3id.org/rmldoc/exampleKeywords: Documentation · Knowledge Graph Construction · RML.1 IntroductionKnowledge Graphs (KGs) are commonly constructed by transforming a set ofheterogeneous data sources (e.g., CSV, JSON files) into RDF graphs. Thesetransformations are performed by relating all input sources with the target on-tology terms, and can be described using declarative mapping languages such asthe W3C recommendation R2RML3 or its widely adopted extension RML [7]. In-stitutions such as the European Railway Agency4 or the European Commission(e.g., in the EU Public Procurement Data Space5) describe their transformationsusing these languages in some of their projects.Knowledge engineers are usually responsible for developing the mapping rulesneeded to construct KGs. In many cases, these engineers rely on graphical inter-faces (e.g, RMLEditor [5]) and human-friendly serializations like YARRRML [4]or Mapeathor [6] to aid them in the creation of mapping rules. However, themapping documents resultant from these efforts are, in many cases, complex andhard to interpret, which reduces their reusability by other engineers. Further-more, there is a lack of tools to generate a comprehensive and human-readable3 https://www.w3.org/TR/r2rml/4 https://data-interop.era.europa.eu/5 https://europa.eu/!qx9WxQhttps://orcid.org/0000-0002-2924-7272https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0003-0454-7145https://github.com/oeg-upm/rmldoc/https://w3id.org/rmldoc/examplehttps://www.w3.org/TR/r2rml/https://data-interop.era.europa.eu/https://europa.eu/!qx9WxQ2 Toledo et al.documentation of mapping rules. This situation delegates mappings as second-class resources in the KG development process, without documentation (scat-tered comments in the mapping document at most) or essential metadata (e.g.,version, creators, license).In this paper, we present RMLdoc [8],6 an open source Python package de-signed to create a human-readable documentation of the mapping rules usedto construct a Knowledge Graph. RMLdoc supports mapping rules describedin R2RML, RML, and YARRRML, helping practitioners better understand therelationships between the original data sources and the ontology terms. To thebest of our knowledge, this is the first approach that proposes the generation of",
        "publication_date": "2025-01-01",
        "authors": "Jhon Toledo, Ana Iglesias-Molina, David Chaves-Fraga, Daniel Garijo",
        "file_name": "20250510124701.pdf",
        "file_path": "../Pruebas/SalidaRSEF/toledo2024rmldoc/PDFs/20250510124701.pdf",
        "pdf_link": "https://dgarijo.com/papers/toledo2024rmldoc.pdf",
        "file_html": "html/20250510124701.html",
        "year": "2024",
        "booktitle": "To appear in the Extended Semantic Web Conference Poster and demo proceedings, 2024",
        "url": "https://dgarijo.com/papers/toledo2024rmldoc.pdf",
        "author": "Toledo, Jhon and Iglesias-Molina, Ana and Chaves-Fraga, David and Garijo, Daniel",
        "ENTRYTYPE": "article",
        "ID": "toledormldoc"
    },
    {
        "title": "{WIDOCO}: a wizard for documenting ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/Widoco",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "FILE_CFF",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "README_BIBTEX",
                        "location_type": "DOI",
                        "source": "SOMEF"
                    },
                    {
                        "type": "bidir",
                        "location": "CITATION_FILE",
                        "location_type": "TITLE",
                        "source": "SOMEF"
                    },
                    {
                        "type": "unidir",
                        "location": "../Pruebas/SalidaRSEF/widoco-iswc2017/PDFs/20250510124732.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "WIDOCO is available in GitHub,12 where users can download it, open issues or ask for help."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.591294",
                "type": "zenodo",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "RSEF"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_9",
        "abstract": "Abstract. In this paper we describe WIDOCO, a WIzard for DOCu-menting Ontologies that guides users through the documentation processof their vocabularies. Given an RDF vocabulary, WIDOCO detects miss-ing vocabulary metadata and creates a documentation with diagrams,human readable descriptions of the ontology terms and a summary ofchanges with respect to previous versions of the ontology. The docu-mentation consists on a set of linked enriched HTML pages that can befurther extended by end users. WIDOCO is open source and builds onwell established Semantic Web tools. So far, WIDOCO has been used todocument more than one hundred ontologies in different domains.Keywords: Ontology documentation, Ontology evolution, Ontology un-derstanding, OWL OntologiesResource Type: SoftwarePermanent URL: https://w3id.org/widocoSoftware DOI: https://doi.org/10.5281/zenodo.5912941 IntroductionOntology engineering methodologies acknowledge reuse of existing vocabulariesas a crucial step when developing a new ontology [11]. Therefore, ontology au-thors often provide a human-readable documentation of their vocabularies, inorder to facilitate their understanding and adoption by other researchers [9].There are three main aspects related to ontology documentation. The firstone is creating a human-readable representation of the content of the ontology:metadata, definition of classes and properties, visualization (e.g., diagrams relat-ing the different concepts) and versioning (explanation of the difference betweenversions of the ontologies). The second aspect is creating machine-readable an-notations of documentation metadata (e.g., provenance, snippets for facilitatingvocabulary discovery by search engines) and the third aspect is preparing thedocumentation files to be accessed as a web resource (doing content negotiation).Related work has been proposed to facilitate some of these aspects. For ex-ample, ontology editors like Protégé [8], have plugins for automatically creatingan HTML documentation with the definition of classes and properties.1 Simi-larly, approaches like LODE [9] or Parrot [12] provide drag-and-drop services to1 https://protegewiki.stanford.edu/wiki/OWLDocautomatically document ontology terms. However, most approaches are typicallydesigned for Semantic Web experts, presenting some of the following issues:1. Lack of guidelines and best practices for ontology documentation: users de-veloping ontologies may not know which are the common terms used todescribe the metadata of their ontologies. These metadata are important,because they are used by existing tools to create human readable descrip-tions of an ontology.2. Lack of ontology metadata completion: Current efforts do not indicate whichkey information may be missing when documenting an ontology.3. Lack of an ecosystem for ontology documentation and customization: mostexisting approaches focus on specific aspects of ontology documentation.On the one hand, approaches like LODE [9] generate a human readabledescription of the classes and properties of a given ontology, but neglect thegeneration of diagrams. On the other hand, tools like WebVowl [5] createdynamic visualizations of ontologies, but do not deal with the generation oftext. Integrating the outcome of these and other tools and customizing themaccording to user preferences takes time, especially to non programmers.",
        "publication_date": "2017-01-01",
        "authors": "Daniel Garijo",
        "file_name": "20250510124732.pdf",
        "file_path": "../Pruebas/SalidaRSEF/widoco-iswc2017/PDFs/20250510124732.pdf",
        "pdf_link": "https://dgarijo.com/papers/widoco-iswc2017.pdf",
        "file_html": "html/20250510124732.html",
        "funding": "USNSF ICER-1541029, NIH 1R01GM117097-01",
        "organization": "Springer, Cham",
        "url": "https://dgarijo.com/papers/widoco-iswc2017.pdf",
        "pages": "94--102",
        "volume": "10588",
        "booktitle": "d'Amato C. et al. (eds) The Semantic Web – ISWC 2017",
        "year": "2017",
        "author": "Garijo, Daniel",
        "ENTRYTYPE": "inproceedings",
        "ID": "garijo2017widoco"
    }
]